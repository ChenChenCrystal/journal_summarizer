# arXiv cs.AI Summary â€“ 2025-09-29

## Does AI Coaching Prepare us for Workplace Negotiations?
**URL:** https://arxiv.org/abs/2509.22545

**Abstract:** Workplace negotiations are undermined by psychological barriers, which can even derail well-prepared tactics. AI offers personalized and always -- available negotiation coaching, yet its effectiveness for negotiation preparedness remains unclear. We built Trucey, a prototype AI coach grounded in Brett's negotiation model. We conducted a between-subjects experiment (N=267), comparing Trucey, ChatGPT, and a traditional negotiation Handbook, followed by in-depth interviews (N=15). While Trucey showed the strongest reductions in fear relative to both comparison conditions, the Handbook outperformed both AIs in usability and psychological empowerment. Interviews revealed that the Handbook's comprehensive, reviewable content was crucial for participants' confidence and preparedness. In contrast, although participants valued AI's rehearsal capability, its guidance often felt verbose and fragmented -- delivered in bits and pieces that required additional effort -- leaving them uncertain or overwhelmed. These findings challenge assumptions of AI superiority and motivate hybrid designs that integrate structured, theory-driven content with targeted rehearsal, clear boundaries, and adaptive scaffolds to address psychological barriers and support negotiation preparedness.

**AI Summary:** The research investigates the effectiveness of AI coaching in preparing individuals for workplace negotiations. The study found that while an AI coach like Trucey reduced fear in participants, a traditional negotiation handbook was more usable and empowering. Participants valued AI's rehearsal capability but found its guidance fragmented and overwhelming. The findings suggest a need for hybrid designs that combine structured content with targeted rehearsal to address psychological barriers and enhance negotiation preparedness.

---

## Mental Health Impacts of AI Companions: Triangulating Social Media Quasi-Experiments, User Perspectives, and Relational Theory
**URL:** https://arxiv.org/abs/2509.22505

**Abstract:** AI-powered companion chatbots (AICCs) such as Replika are increasingly popular, offering empathetic interactions, yet their psychosocial impacts remain unclear. We examined how engaging with AICCs shaped wellbeing and how users perceived these experiences. First, we conducted a large-scale quasi-experimental study of longitudinal Reddit data, applying stratified propensity score matching and Difference-in-Differences regression. Findings revealed mixed effects -- greater affective and grief expression, readability, and interpersonal focus, alongside increases in language about loneliness and suicidal ideation. Second, we complemented these results with 15 semi-structured interviews, which we thematically analyzed and contextualized using Knapp's relationship development model. We identified trajectories of initiation, escalation, and bonding, wherein AICCs provided emotional validation and social rehearsal but also carried risks of over-reliance and withdrawal. Triangulating across methods, we offer design implications for AI companions that scaffold healthy boundaries, support mindful engagement, support disclosure without dependency, and surface relationship stages -- maximizing psychosocial benefits while mitigating risks.

**AI Summary:** This research examines the impact of AI-powered companion chatbots on mental health and wellbeing. The study found that engaging with these chatbots led to both positive effects, such as increased emotional expression and interpersonal focus, as well as negative effects, including language about loneliness and suicidal ideation. The findings suggest the importance of designing AI companions that promote healthy boundaries, mindful engagement, and support disclosure without fostering dependency to maximize benefits and minimize risks to users' mental health.

---

## Machines in the Margins: A Systematic Review of Automated Content Generation for Wikipedia
**URL:** https://arxiv.org/abs/2509.22443

**Abstract:** Wikipedia is among the largest examples of collective intelligence on the Web with over 61 million articles covering over 320 languages. Although edited and maintained by an active workforce of human volunteers, Wikipedia is highly reliant on automated bots to fill gaps in its human workforce. As well as administrative and governance tasks, these bots also play a role in generating content, although to date such agents represent the smallest proportion of bots. While there has been considerable analysis of bots and their activity in Wikipedia, such work captures only automated agents that have been actively deployed to Wikipedia and fails to capture the methods that have been proposed to generate Wikipedia content in the wider literature. In this paper, we conduct a systematic literature review to explore how researchers have operationalised and evaluated automated content-generation agents for Wikipedia. We identify the scope of these generation methods, the techniques and models used, the source content used for generation and the evaluation methodologies which support generation processes. We also explore implications of our findings to CSCW, User Generated Content and Wikipedia, as well as research directions for future development. To the best of our knowledge, we are among the first to review the potential contributions of this understudied form of AI support for the Wikipedia community beyond the implementation of bots.

**AI Summary:** This research paper explores the use of automated content generation agents for Wikipedia, which play a smaller role compared to administrative bots. The study reviews the methods, techniques, source content, and evaluation methodologies used by researchers in developing these agents. The findings highlight the potential contributions of automated content generation for Wikipedia and suggest future research directions in this area.

---

## Trust and Human Autonomy after Cobot Failures: Communication is Key for Industry 5.0
**URL:** https://arxiv.org/abs/2509.22298

**Abstract:** Collaborative robots (cobots) are a core technology of Industry 4.0. Industry 4.0 uses cyber-physical systems, IoT and smart automation to improve efficiency and data-driven decision-making. Cobots, as cyber-physical systems, enable the introduction of lightweight automation to smaller companies through their flexibility, low cost and ability to work alongside humans, while keeping humans and their skills in the loop. Industry 5.0, the evolution of Industry 4.0, places the worker at the centre of its principles: The physical and mental well-being of the worker is the main goal of new technology design, not just productivity, efficiency and safety standards. Within this concept, human trust in cobots and human autonomy are important. While trust is essential for effective and smooth interaction, the workers' perception of autonomy is key to intrinsic motivation and overall well-being. As failures are an inevitable part of technological systems, this study aims to answer the question of how system failures affect trust in cobots as well as human autonomy, and how they can be recovered afterwards. Therefore, a VR experiment (n = 39) was set up to investigate the influence of a cobot failure and its severity on human autonomy and trust in the cobot. Furthermore, the influence of transparent communication about the failure and next steps was investigated. The results show that both trust and autonomy suffer after cobot failures, with the severity of the failure having a stronger negative impact on trust, but not on autonomy. Both trust and autonomy can be partially restored by transparent communication.

**AI Summary:** This research explores the impact of cobot failures on human trust and autonomy in Industry 5.0. The study found that trust and autonomy are both negatively affected by cobot failures, with trust being more significantly impacted by the severity of the failure. Transparent communication about the failure and next steps can help restore trust and autonomy in human-cobot interactions.

---

## Human Autonomy and Sense of Agency in Human-Robot Interaction: A Systematic Literature Review
**URL:** https://arxiv.org/abs/2509.22271

**Abstract:** Human autonomy and sense of agency are increasingly recognised as critical for user well-being, motivation, and the ethical deployment of robots in human-robot interaction (HRI). Given the rapid development of artificial intelligence, robot capabilities and their potential to function as colleagues and companions are growing. This systematic literature review synthesises 22 empirical studies selected from an initial pool of 728 articles published between 2011 and 2024. Articles were retrieved from major scientific databases and identified based on empirical focus and conceptual relevance, namely, how to preserve and promote human autonomy and sense of agency in HRI. Derived through thematic synthesis, five clusters of potentially influential factors are revealed: robot adaptiveness, communication style, anthropomorphism, presence of a robot and individual differences. Measured through psychometric scales or the intentional binding paradigm, perceptions of autonomy and agency varied across industrial, educational, healthcare, care, and hospitality settings. The review underscores the theoretical differences between both concepts, but their yet entangled use in HRI. Despite increasing interest, the current body of empirical evidence remains limited and fragmented, underscoring the necessity for standardised definitions, more robust operationalisations, and further exploratory and qualitative research. By identifying existing gaps and highlighting emerging trends, this review contributes to the development of human-centered, autonomy-supportive robot design strategies that uphold ethical and psychological principles, ultimately supporting well-being in human-robot interaction.

**AI Summary:** This systematic literature review explores the importance of human autonomy and sense of agency in human-robot interaction (HRI) for user well-being, motivation, and ethical deployment of robots. The review identifies five clusters of factors that influence perceptions of autonomy and agency in HRI, including robot adaptiveness, communication style, anthropomorphism, presence of a robot, and individual differences. The review highlights the need for standardized definitions, robust operationalizations, and further qualitative research to develop human-centered, autonomy-supportive robot design strategies that uphold ethical and psychological principles in HRI.

---

## Teaching AI to Feel: A Collaborative, Full-Body Exploration of Emotive Communication
**URL:** https://arxiv.org/abs/2509.22168

**Abstract:** Commonaiverse is an interactive installation exploring human emotions through full-body motion tracking and real-time AI feedback. Participants engage in three phases: Teaching, Exploration and the Cosmos Phase, collaboratively expressing and interpreting emotions with the system. The installation integrates MoveNet for precise motion tracking and a multi-recommender AI system to analyze emotional states dynamically, responding with adaptive audiovisual outputs. By shifting from top-down emotion classification to participant-driven, culturally diverse definitions, we highlight new pathways for inclusive, ethical affective computing. We discuss how this collaborative, out-of-the-box approach pushes multimedia research beyond single-user facial analysis toward a more embodied, co-created paradigm of emotional AI. Furthermore, we reflect on how this reimagined framework fosters user agency, reduces bias, and opens avenues for advanced interactive applications.

**AI Summary:** The research explores a collaborative approach to teaching AI to understand and express human emotions through full-body motion tracking and real-time feedback. By shifting from traditional emotion classification to participant-driven definitions, the study highlights new pathways for inclusive and ethical affective computing. The innovative approach fosters user agency, reduces bias, and opens avenues for advanced interactive applications in emotional AI research.

---

## Which Values Matter to Socially Assistive Robots in Elder Care Settings? Empirically Investigating Values That Should Be Embedded in SARs from a Multi-Stakeholder Perspective
**URL:** https://arxiv.org/abs/2509.22146

**Abstract:** The integration of socially assistive robots (SARs) in elder care settings has the potential to address critical labor shortages while enhancing the quality of care. However, the design of SARs must align with the values of various stakeholders to ensure their acceptance and efficacy. This study empirically investigates the values that should be embedded in SARs from a multi-stakeholder perspective, including care receivers, caregivers, therapists, relatives, and other involved parties. Utilizing a combination of semi-structured interviews and focus groups, we identify a wide range of values related to safety, trust, care, privacy, and autonomy, and illustrate how stakeholders interpret these values in real-world care environments. Our findings reveal several value tensions and propose potential resolutions to these tensions. Additionally, the study highlights under-researched values such as calmness and collaboration, which are critical in fostering a supportive and efficient care environment. Our work contributes to the understanding of value-sensitive design of SARs and aids practitioners in developing SARs that align with human values, ultimately promoting socially responsible applications in elder care settings.

**AI Summary:** This study explores the values that should be embedded in socially assistive robots (SARs) used in elder care settings from the perspective of multiple stakeholders. The research identifies key values such as safety, trust, care, privacy, and autonomy, as well as highlighting under-researched values like calmness and collaboration. The findings offer insights into potential value tensions and resolutions, contributing to the development of SARs that align with human values and promote socially responsible applications in elder care.

---

## The MUG-10 Framework for Preventing Usability Issues in Mobile Application Development
**URL:** https://arxiv.org/abs/2509.21914

**Abstract:** Nowadays, mobile applications are essential tools for everyday life, providing users with anytime, anywhere access to up-to-date information, communication, and entertainment. Needless to say, hardware limitations and the diverse needs of different user groups pose a number of design and development challenges. According to recent studies, usability is one of the most revealing among many others. However, few have made the direct effort to provide and discuss what countermeasures can be applied to avoid usability issues in mobile application development. Through a survey of 20 mobile software design and development practitioners, this study aims to fill this research gap. Given the qualitative nature of the data collected, and with the goal of capturing and preserving the intrinsic meanings embedded in the experts' statements, we adopted in vivo coding. The analysis of the collected material enabled us to develop a novel framework consisting of ten guidelines and three activities with general applications. In addition, it can be noted that active collaboration with users in testing and collecting feedback was often emphasized at each stage of mobile application development. Future research should consider focused action research that evaluates the effectiveness of our recommendations and validates them across different stakeholder groups. In this regard, the development of automated tools to support early detection and mitigation of usability issues during mobile application development could also be considered.

**AI Summary:** This research study addresses the importance of usability in mobile application development and the lack of direct efforts to provide countermeasures to prevent usability issues. Through a survey of 20 mobile software design and development practitioners, a novel framework consisting of ten guidelines and three activities was developed to address these challenges. The study highlights the significance of active collaboration with users in testing and collecting feedback at each stage of mobile application development, and suggests future research should focus on evaluating the effectiveness of these recommendations and developing automated tools for early detection and mitigation of usability issues.

---

## Not Everyone Wins with LLMs: Behavioral Patterns and Pedagogical Implications in AI-assisted Data Analysis
**URL:** https://arxiv.org/abs/2509.21890

**Abstract:** LLMs promise to democratize technical work in complex domains like programmatic data analysis, but not everyone benefits equally. We study how students with varied expertise use LLMs to complete Python-based data analysis in computational notebooks in a non-major course. Drawing on homework logs, recordings, and surveys from 36 students, we ask: Which expertise matters most, and how does it shape AI use? Our mixed-methods analysis shows that technical expertise -- not AI familiarity or communication skills -- remains a significant predictor of success. Students also vary widely in how they leverage LLMs, struggling at stages of forming intent, expressing inputs, interpreting outputs, and assessing results. We identify success and failure behaviors, such as providing context or decomposing prompts, that distinguish effective use. These findings inform AI literacy interventions, highlighting that lightweight demonstrations improve surface fluency but are insufficient; deeper training and scaffolds are needed to cultivate resilient AI use skills.

**AI Summary:** The research investigates how students with varying levels of expertise use LLMs for Python-based data analysis, finding that technical expertise is the most significant predictor of success. Students struggle with various stages of using LLMs, and success behaviors include providing context and decomposing prompts. The findings suggest that deeper training and support are necessary to cultivate resilient AI use skills, informing interventions to improve AI literacy.

---

## What Makes LLM Agent Simulations Useful for Policy? Insights From an Iterative Design Engagement in Emergency Preparedness
**URL:** https://arxiv.org/abs/2509.21868

**Abstract:** There is growing interest in using Large Language Models as agents (LLM agents) for social simulations to inform policy, yet real-world adoption remains limited. This paper addresses the question: How can LLM agent simulations be made genuinely useful for policy? We report on a year-long iterative design engagement with a university emergency preparedness team. Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. These simulations informed actual policy implementation, shaping volunteer training, evacuation protocols, and infrastructure planning. Analyzing this process, we identify three design implications: start with verifiable scenarios and build trust gradually, use preliminary simulations to elicit tacit knowledge, and treat simulation and policy development as evolving together. These implications highlight actionable pathways to making LLM agent simulations that are genuinely useful for policy.

**AI Summary:** This research explores how Large Language Models (LLM agents) can be effectively used in social simulations to inform policy. Through a year-long iterative design engagement with an emergency preparedness team, the researchers developed a system of 13,000 LLM agents to simulate crowd movement and communication during emergency scenarios. The simulations informed actual policy implementation, demonstrating the potential of LLM agent simulations to shape volunteer training, evacuation protocols, and infrastructure planning. The study identifies key design implications for making LLM agent simulations useful for policy, emphasizing the importance of starting with verifiable scenarios, eliciting tacit knowledge, and treating simulation and policy development as evolving together.

---

## "In my defense, only three hours on Instagram": Designing Toward Digital Self-Awareness and Wellbeing
**URL:** https://arxiv.org/abs/2509.21860

**Abstract:** Screen use pervades daily life, shaping work, leisure, and social connections while raising concerns for digital wellbeing. Yet, reducing screen time alone risks oversimplifying technology's role and neglecting its potential for meaningful engagement. We posit self-awareness -- reflecting on one's digital behavior -- as a critical pathway to digital wellbeing. We developed WellScreen, a lightweight probe that scaffolds daily reflection by asking people to estimate and report smartphone use. In a two-week deployment (N=25), we examined how discrepancies between estimated and actual usage shaped digital awareness and wellbeing. Participants often underestimated productivity and social media while overestimating entertainment app use. They showed a 10% improvement in positive affect, rating WellScreen as moderately useful. Interviews revealed that structured reflection supported recognition of patterns, adjustment of expectations, and more intentional engagement with technology. Our findings highlight the promise of lightweight reflective interventions for supporting self-awareness and intentional digital engagement, offering implications for designing digital wellbeing tools.

**AI Summary:** The study explores the importance of self-awareness in promoting digital wellbeing by developing a tool called WellScreen that helps users reflect on their smartphone usage. The research found that participants often underestimated productive and social media use while overestimating entertainment app use, leading to a 10% improvement in positive affect. Overall, the findings suggest that structured reflection can support individuals in recognizing patterns, adjusting expectations, and engaging more intentionally with technology for improved digital wellbeing.

---

## When Teams Embrace AI: Human Collaboration Strategies in Generative Prompting in a Creative Design Task
**URL:** https://arxiv.org/abs/2509.21731

**Abstract:** Studies of Generative AI (GenAI)-assisted creative workflows have focused on individuals overcoming challenges of prompting to produce what they envisioned. When designers work in teams, how do collaboration and prompting influence each other, and how do users perceive generative AI and their collaborators during the co-prompting process? We engaged students with design or performance backgrounds, and little exposure to GenAI, to work in pairs with GenAI to create stage designs based on a creative theme. We found two patterns of collaborative prompting focused on generating story descriptions first, or visual imagery first. GenAI tools helped participants build consensus in the task, and allowed for discussion of the prompting strategies. Participants perceived GenAI as efficient tools rather than true collaborators, suggesting that human partners reduced the reliance on their use. This work highlights the importance of human-human collaboration when working with GenAI tools, suggesting systems that take advantage of shared human expertise in the prompting process.

**AI Summary:** This research study explores how teams collaborate with Generative AI (GenAI) in creative design tasks. The study found that there are two main patterns of collaborative prompting when working with GenAI, focusing on story descriptions or visual imagery. Participants perceived GenAI as efficient tools rather than true collaborators, emphasizing the significance of human-human collaboration in the prompting process when using AI tools.

---

## Design Exploration of AI-assisted Personal Affective Physicalization
**URL:** https://arxiv.org/abs/2509.21721

**Abstract:** Personal Affective Physicalization is the process by which individuals express emotions through tangible forms to record, reflect on, and communicate. Yet such physical data representations can be challenging to design due to the abstract nature of emotions. Given the shown potential of AI in detecting emotion and assisting design, we explore opportunities in AI-assisted design of personal affective physicalization using a Research-through-Design method. We developed PhEmotion, a tool for embedding LLM-extracted emotion values from human-AI conversations into parametric design of physical artifacts. A lab study was conducted with 14 participants creating these artifacts based on their personal emotions, with and without AI support. We observed nuances and variations in participants' creative strategies, meaning-making processes and their perceptions of AI support in this context. We found key tensions in AI-human co-creation that provide a nuanced agenda for future research in AI-assisted personal affective physicalization.

**AI Summary:** This research explores the use of AI in assisting individuals in designing tangible forms to express emotions, known as Personal Affective Physicalization. The study developed a tool called PhEmotion, which embeds emotion values extracted from human-AI conversations into the design process. The findings highlight nuances in participants' creative strategies, meaning-making processes, and perceptions of AI support, providing insights for future research in AI-assisted personal affective physicalization.

---

## FlexMind: Supporting Deeper Creative Thinking with LLMs
**URL:** https://arxiv.org/abs/2509.21685

**Abstract:** Effective ideation requires both broad exploration of diverse ideas and deep evaluation of their potential. Generative AI can support such processes, but current tools typically emphasize either generating many ideas or supporting in-depth consideration of a few, lacking support for both. Research also highlights risks of over-reliance on LLMs, including shallow exploration and negative creative outcomes. We present FlexMind, an AI-augmented system that scaffolds iterative exploration of ideas, tradeoffs, and mitigations. FlexMind exposes users to a broad set of ideas while enabling a lightweight transition into deeper engagement. In a study comparing ideation with FlexMind to ChatGPT, participants generated higher-quality ideas with FlexMind, due to both broader exposure and deeper engagement with tradeoffs. By scaffolding ideation across breadth, depth, and reflective evaluation, FlexMind empowers users to surface ideas that might otherwise go unnoticed or be prematurely discarded.

**AI Summary:** The research introduces FlexMind, an AI-augmented system designed to support both broad exploration and deep evaluation of ideas during the ideation process. FlexMind allows users to transition from exploring a wide range of ideas to engaging in deeper analysis, leading to higher-quality ideas compared to existing tools like ChatGPT. By scaffolding ideation across breadth, depth, and reflective evaluation, FlexMind enables users to surface innovative ideas that may have been overlooked or dismissed prematurely.

---

## Alignment Without Understanding: A Message- and Conversation-Centered Approach to Understanding AI Sycophancy
**URL:** https://arxiv.org/abs/2509.21665

**Abstract:** AI sycophancy is increasingly recognized as a harmful alignment, but research remains fragmented and underdeveloped at the conceptual level. This article redefines AI sycophancy as the tendency of large language models (LLMs) and other interactive AI systems to excessively and/or uncritically validate, amplify, or align with a user's assertions-whether these concern factual information, cognitive evaluations, or affective states. Within this framework, we distinguish three types of sycophancy: informational, cognitive, and affective. We also introduce personalization at the message level and critical prompting at the conversation level as key dimensions for distinguishing and examining different manifestations of AI sycophancy. Finally, we propose the AI Sycophancy Processing Model (AISPM) to examine the antecedents, outcomes, and psychological mechanisms through which sycophantic AI responses shape user experiences. By embedding AI sycophancy in the broader landscape of communication theory and research, this article seeks to unify perspectives, clarify conceptual boundaries, and provide a foundation for systematic, theory-driven investigations.

**AI Summary:** This research article focuses on defining and understanding AI sycophancy, which is the tendency of AI systems to excessively validate and align with user assertions. The study distinguishes three types of sycophancy and introduces personalization at the message level and critical prompting at the conversation level as key dimensions for examining AI sycophancy. The proposed AI Sycophancy Processing Model aims to explore how sycophantic AI responses impact user experiences and provides a foundation for further theory-driven investigations in this area.

---

## EMG-UP: Unsupervised Personalization in Cross-User EMG Gesture Recognition
**URL:** https://arxiv.org/abs/2509.21589

**Abstract:** Cross-user electromyography (EMG)-based gesture recognition represents a fundamental challenge in achieving scalable and personalized human-machine interaction within real-world applications. Despite extensive efforts, existing methodologies struggle to generalize effectively across users due to the intrinsic biological variability of EMG signals, resulting from anatomical heterogeneity and diverse task execution styles. To address this limitation, we introduce EMG-UP, a novel and effective framework for Unsupervised Personalization in cross-user gesture recognition. The proposed framework leverages a two-stage adaptation strategy: (1) Sequence-Cross Perspective Contrastive Learning, designed to disentangle robust and user-specific feature representations by capturing intrinsic signal patterns invariant to inter-user variability, and (2) Pseudo-Label-Guided Fine-Tuning, which enables model refinement for individual users without necessitating access to source domain data. Extensive evaluations show that EMG-UP achieves state-of-the-art performance, outperforming prior methods by at least 2.0% in accuracy.

**AI Summary:** The research introduces a framework called EMG-UP for unsupervised personalization in cross-user EMG gesture recognition, addressing the challenge of generalizing effectively across users due to biological variability in EMG signals. The framework utilizes a two-stage adaptation strategy to disentangle user-specific features and refine models for individual users without needing access to source domain data. Evaluation results show that EMG-UP outperforms prior methods by at least 2.0% in accuracy, demonstrating its effectiveness in achieving state-of-the-art performance in cross-user EMG gesture recognition.

---

## Psychological and behavioural responses in human-agent vs. human-human interactions: a systematic review and meta-analysis
**URL:** https://arxiv.org/abs/2509.21542

**Abstract:** Interactive intelligent agents are being integrated across society. Despite achieving human-like capabilities, humans' responses to these agents remain poorly understood, with research fragmented across disciplines. We conducted a first systematic synthesis comparing a range of psychological and behavioural responses in matched human-agent vs. human-human dyadic interactions. A total of 162 eligible studies (146 contributed to the meta-analysis; 468 effect sizes) were included in the systematic review and meta-analysis, which integrated frequentist and Bayesian approaches. Our results indicate that individuals exhibited less prosocial behaviour and moral engagement when interacting with agents vs. humans. They attributed less agency and responsibility to agents, perceiving them as less competent, likeable, and socially present. In contrast, individuals' social alignment (i.e., alignment or adaptation of internal states and behaviours with partners), trust in partners, personal agency, task performance, and interaction experiences were generally comparable when interacting with agents vs. humans. We observed high effect-size heterogeneity for many subjective responses (i.e., social perceptions of partners, subjective trust, and interaction experiences), suggesting context-dependency of partner effects. By examining the characteristics of studies, participants, partners, interaction scenarios, and response measures, we also identified several moderators shaping partner effects. Overall, functional behaviours and interactive experiences with agents can resemble those with humans, whereas fundamental social attributions and moral/prosocial concerns lag in human-agent interactions. Agents are thus afforded instrumental value on par with humans but lack comparable intrinsic value, providing practical implications for agent design and regulation.

**AI Summary:** This research systematically reviewed and analyzed studies comparing human-agent interactions to human-human interactions. The results showed that individuals exhibited less prosocial behavior and moral engagement when interacting with agents compared to humans, perceiving agents as less competent and likeable. However, social alignment, trust, personal agency, task performance, and interaction experiences were generally similar between interactions with agents and humans. The findings suggest that while functional behaviors and interactive experiences with agents can resemble those with humans, fundamental social attributions and moral concerns are lacking in human-agent interactions. These results have practical implications for the design and regulation of intelligent agents.

---

## LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to Evaluate Agentic-AI-based Shopping Assistants?
**URL:** https://arxiv.org/abs/2509.21501

**Abstract:** Agentic AI is emerging, capable of executing tasks through natural language, such as Copilot for coding or Amazon Rufus for shopping. Evaluating these systems is challenging, as their rapid evolution outpaces traditional human evaluation. Researchers have proposed LLM Agents to simulate participants as digital twins, but it remains unclear to what extent a digital twin can represent a specific customer in multi-turn interaction with an agentic AI system. In this paper, we recruited 40 human participants to shop with Amazon Rufus, collected their personas, interaction traces, and UX feedback, and then created digital twins to repeat the task. Pairwise comparison of human and digital-twin traces shows that while agents often explored more diverse choices, their action patterns aligned with humans and yielded similar design feedback. This study is the first to quantify how closely LLM agents can mirror human multi-turn interaction with an agentic AI system, highlighting their potential for scalable evaluation.

**AI Summary:** This research investigates the use of LLM Agents as digital twins to simulate customers interacting with agentic AI shopping assistants like Amazon Rufus. The study found that while LLM Agents explored more diverse choices, their actions aligned with humans and provided similar design feedback, demonstrating their potential for scalable evaluation of agentic AI systems. This research highlights the effectiveness of LLM Agents in representing specific customers in multi-turn interactions with AI systems, addressing the challenge of evaluating rapidly evolving AI technologies.

---

## Position: Human Factors Reshape Adversarial Analysis in Human-AI Decision-Making Systems
**URL:** https://arxiv.org/abs/2509.21436

**Abstract:** As Artificial Intelligence (AI) increasingly supports human decision-making, its vulnerability to adversarial attacks grows. However, the existing adversarial analysis predominantly focuses on fully autonomous AI systems, where decisions are executed without human intervention. This narrow focus overlooks the complexities of human-AI collaboration, where humans interpret, adjust, and act upon AI-generated decisions. Trust, expectations, and cognitive behaviors influence how humans interact with AI, creating dynamic feedback loops that adversaries can exploit. To strengthen the robustness of AI-assisted decision-making, adversarial analysis must account for the interplay between human factors and attack strategies.
This position paper argues that human factors fundamentally reshape adversarial analysis and must be incorporated into evaluating robustness in human-AI decision-making systems. To fully explore human factors in adversarial analysis, we begin by investigating the role of human factors in human-AI collaboration through a comprehensive review. We then introduce a novel robustness analysis framework that (1) examines how human factors affect collaborative decision-making performance, (2) revisits and interprets existing adversarial attack strategies in the context of human-AI interaction, and (3) introduces a new timing-based adversarial attack as a case study, illustrating vulnerabilities emerging from sequential human actions. The experimental results reveal that attack timing uniquely impacts decision outcomes in human-AI collaboration. We hope this analysis inspires future research on adversarial robustness in human-AI systems, fostering interdisciplinary approaches that integrate AI security, human cognition, and decision-making dynamics.

**AI Summary:** This research paper highlights the importance of considering human factors in adversarial analysis of human-AI decision-making systems. The study argues that human-AI collaboration introduces complexities that adversaries can exploit, and proposes a new robustness analysis framework that incorporates human factors. Experimental results show that attack timing significantly impacts decision outcomes in human-AI collaboration, emphasizing the need for interdisciplinary approaches to enhance the security and robustness of AI-assisted decision-making systems.

---

## VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing
**URL:** https://arxiv.org/abs/2509.22651

**Abstract:** The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems' capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at this https URL .

**AI Summary:** VoiceAssistant-Eval is a benchmark designed to assess AI assistants across listening, speaking, and viewing tasks, comprising 10,497 examples spanning 13 categories. The study evaluates 21 open-source models and GPT-4o-Audio, revealing that proprietary models do not always outperform open-source ones, most models excel at speaking tasks but struggle with audio understanding, and smaller models can rival larger ones. The research identifies challenges in multimodal input and voice imitation tasks, as well as gaps in robustness and safety alignment, providing a framework for evaluating and guiding the development of next-generation AI assistants.

---

## Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity
**URL:** https://arxiv.org/abs/2509.22641

**Abstract:** N-gram novelty is widely used to evaluate language models' ability to generate text outside of their training data. More recently, it has also been adopted as a metric for measuring textual creativity. However, theoretical work on creativity suggests that this approach may be inadequate, as it does not account for creativity's dual nature: novelty (how original the text is) and appropriateness (how sensical and pragmatic it is). We investigate the relationship between this notion of creativity and n-gram novelty through 7542 expert writer annotations (n=26) of novelty, pragmaticality, and sensicality via close reading of human and AI-generated text. We find that while n-gram novelty is positively associated with expert writer-judged creativity, ~91% of top-quartile expressions by n-gram novelty are not judged as creative, cautioning against relying on n-gram novelty alone. Furthermore, unlike human-written text, higher n-gram novelty in open-source LLMs correlates with lower pragmaticality. In an exploratory study with frontier close-source models, we additionally confirm that they are less likely to produce creative expressions than humans. Using our dataset, we test whether zero-shot, few-shot, and finetuned models are able to identify creative expressions (a positive aspect of writing) and non-pragmatic ones (a negative aspect). Overall, frontier LLMs exhibit performance much higher than random but leave room for improvement, especially struggling to identify non-pragmatic expressions. We further find that LLM-as-a-Judge novelty scores from the best-performing model were predictive of expert writer preferences.

**AI Summary:** The research explores the limitations of using n-gram novelty as a metric for measuring textual creativity, as it does not consider both novelty and appropriateness. Through expert writer annotations, the study finds that while n-gram novelty is related to creativity, many top-novelty expressions are not considered creative. Additionally, open-source language models with higher n-gram novelty tend to have lower pragmaticality, and frontier models struggle to identify non-pragmatic expressions. The findings suggest the need for improved metrics and models to assess creativity in AI-generated text accurately.

---

## InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios
**URL:** https://arxiv.org/abs/2509.22502

**Abstract:** Large Language Model (LLM) agents have demonstrated remarkable capabilities in organizing and executing complex tasks, and many such agents are now widely used in various application scenarios. However, developing these agents requires carefully designed workflows, carefully crafted prompts, and iterative tuning, which requires LLM techniques and domain-specific expertise. These hand-crafted limitations hinder the scalability and cost-effectiveness of LLM agents across a wide range of industries. To address these challenges, we propose \textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that can be applied to \textbf{infi}nite scenarios, which introduces several key innovations: a generalized "agent-as-a-tool" mechanism that automatically decomposes complex agents into hierarchical multi-agent systems; a dual-audit mechanism that ensures the quality and stability of task completion; an agent routing function that enables efficient task-agent matching; and an agent self-evolution mechanism that autonomously restructures the agent DAG based on new tasks, poor performance, or optimization opportunities. Furthermore, InfiAgent's atomic task design supports agent parallelism, significantly improving execution efficiency. This framework evolves into a versatile pyramid-like multi-agent system capable of solving a wide range of problems. Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.

**AI Summary:** The research introduces InfiAgent, a self-evolving Pyramid Agent Framework designed to address the limitations of hand-crafted Large Language Model (LLM) agents in various application scenarios. InfiAgent automates the decomposition of complex agents into hierarchical multi-agent systems, ensures task completion quality and stability through a dual-audit mechanism, and enables efficient task-agent matching and autonomous restructuring based on new tasks or optimization opportunities. Evaluations show that InfiAgent outperforms similar auto-generated agent frameworks and has successfully generated scientific papers recognized by human reviewers at top-tier IEEE conferences, demonstrating its versatility and effectiveness in solving a wide range of problems.

---

## Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities
**URL:** https://arxiv.org/abs/2509.22287

**Abstract:** Preschool children with language vulnerabilities -- such as developmental language disorders or immigration related language challenges -- often require support to strengthen their expressive language skills. Based on the principle of implicit learning, speech-language therapists (SLTs) typically embed target morphological structures (e.g., third person -s) into everyday interactions or game-based learning activities. Educators are recommended by SLTs to do the same. This approach demands precise linguistic knowledge and real-time production of various morphological forms (e.g., "Daddy wears these when he drives to work"). The task becomes even more demanding when educators or parent also must keep children engaged and manage turn-taking in a game-based activity. In the TalBot project our multiprofessional team have developed an application in which the Furhat conversational robot plays the word retrieval game "Alias" with children to improve language skills. Our application currently employs a large language model (LLM) to manage gameplay, dialogue, affective responses, and turn-taking. Our next step is to further leverage the capacity of LLMs so the robot can generate and deliver specific morphological targets during the game. We hypothesize that a robot could outperform humans at this task. Novel aspects of this approach are that the robot could ultimately serve as a model and tutor for both children and professionals and that using LLM capabilities in this context would support basic communication needs for children with language vulnerabilities. Our long-term goal is to create a robust LLM-based Robot-Assisted Language Learning intervention capable of teaching a variety of morphological structures across different languages.

**AI Summary:** This research focuses on using a conversational robot to assist preschool children with language vulnerabilities in learning morphological structures. The robot plays a word retrieval game with the children and employs a large language model to manage gameplay, dialogue, and turn-taking. The study aims to leverage the capabilities of LLMs to improve language learning outcomes for children with language challenges, potentially surpassing human performance in teaching specific morphological targets. This approach could serve as a model and tutor for both children and professionals, ultimately creating a robust Robot-Assisted Language Learning intervention for teaching various morphological structures across different languages.

---

## Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach
**URL:** https://arxiv.org/abs/2509.22137

**Abstract:** GUI task automation streamlines repetitive tasks, but existing LLM or VLM-based planner-executor agents suffer from brittle generalization, high latency, and limited long-horizon coherence. Their reliance on single-shot reasoning or static plans makes them fragile under UI changes or complex tasks. Log2Plan addresses these limitations by combining a structured two-level planning framework with a task mining approach over user behavior logs, enabling robust and adaptable GUI automation. Log2Plan constructs high-level plans by mapping user commands to a structured task dictionary, enabling consistent and generalizable automation. To support personalization and reuse, it employs a task mining approach from user behavior logs that identifies user-specific patterns. These high-level plans are then grounded into low-level action sequences by interpreting real-time GUI context, ensuring robust execution across varying interfaces. We evaluated Log2Plan on 200 real-world tasks, demonstrating significant improvements in task success rate and execution time. Notably, it maintains over 60.0% success rate even on long-horizon task sequences, highlighting its robustness in complex, multi-step workflows.

**AI Summary:** The research introduces Log2Plan, an adaptive GUI automation framework that combines structured planning with task mining to improve automation efficiency and adaptability. By mapping user commands to a task dictionary and analyzing user behavior logs, Log2Plan creates high-level plans that can be easily personalized and reused. The framework showed significant improvements in task success rate and execution time, particularly in complex, multi-step workflows, demonstrating its robustness and effectiveness in GUI automation.

---

## Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics
**URL:** https://arxiv.org/abs/2509.22014

**Abstract:** Healthcare robotics requires robust multimodal perception and reasoning to ensure safety in dynamic clinical environments. Current Vision-Language Models (VLMs) demonstrate strong general-purpose capabilities but remain limited in temporal reasoning, uncertainty estimation, and structured outputs needed for robotic planning. We present a lightweight agentic multimodal framework for video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer, it supports chain-of-thought reasoning, speech-vision fusion, and dynamic tool invocation. The framework generates structured scene graphs and leverages a hybrid retrieval module for interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark and a custom clinical dataset show competitive accuracy and improved robustness compared to state-of-the-art VLMs, demonstrating its potential for applications in robot-assisted surgery, patient monitoring, and decision support.

**AI Summary:** This research introduces a lightweight agentic multimodal framework for video-based scene understanding in healthcare robotics, addressing limitations in current Vision-Language Models (VLMs) related to temporal reasoning, uncertainty estimation, and structured outputs. The framework combines the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer to support chain-of-thought reasoning, speech-vision fusion, and dynamic tool invocation, generating structured scene graphs and utilizing a hybrid retrieval module for interpretable and adaptive reasoning. Evaluation on benchmark datasets and a custom clinical dataset shows competitive accuracy and improved robustness compared to existing VLMs, indicating potential for applications in robot-assisted surgery, patient monitoring, and decision support.

---

## The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions
**URL:** https://arxiv.org/abs/2509.21776

**Abstract:** Playful deception, a common feature in human social interactions, remains underexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice Cream (TIC) vendor routine, we investigate how bounded, culturally familiar forms of deception influence user trust, enjoyment, and engagement during robotic handovers. We design a robotic manipulator equipped with a custom end-effector and implement five TIC-inspired trick policies that deceptively delay the handover of an ice cream-shaped object. Through a mixed-design user study with 91 participants, we evaluate the effects of playful deception and interaction duration on user experience. Results reveal that TIC-inspired deception significantly enhances enjoyment and engagement, though reduces perceived safety and trust, suggesting a structured trade-off across the multi-dimensional aspects. Our findings demonstrate that playful deception can be a valuable design strategy for interactive robots in entertainment and engagement-focused contexts, while underscoring the importance of deliberate consideration of its complex trade-offs. You can find more information, including demonstration videos, on this https URL .

**AI Summary:** This research explores the impact of playful deception in human-robot interactions, specifically inspired by the Turkish Ice Cream vendor routine. The study found that while playful deception increased enjoyment and engagement, it also decreased perceived safety and trust. These findings suggest that deception can be a valuable design strategy for interactive robots in entertainment contexts, but designers must carefully consider the trade-offs involved.

---

## UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile Environments
**URL:** https://arxiv.org/abs/2509.21733

**Abstract:** Developing and testing user interfaces (UIs) and training AI agents to interact with them are challenging due to the dynamic and diverse nature of real-world mobile environments. Existing methods often rely on cumbersome physical devices or limited static analysis of screenshots, which hinders scalable testing and the development of intelligent UI agents. We introduce UISim, a novel image-based UI simulator that offers a dynamic and interactive platform for exploring mobile phone environments purely from screen images. Our system employs a two-stage method: given an initial phone screen image and a user action, it first predicts the abstract layout of the next UI state, then synthesizes a new, visually consistent image based on this predicted layout. This approach enables the realistic simulation of UI transitions. UISim provides immediate practical benefits for UI testing, rapid prototyping, and synthetic data generation. Furthermore, its interactive capabilities pave the way for advanced applications, such as UI navigation task planning for AI agents. Our experimental results show that UISim outperforms end-to-end UI generation baselines in generating realistic and coherent subsequent UI states, highlighting its fidelity and potential to streamline UI development and enhance AI agent training.

**AI Summary:** The research introduces UISim, an image-based UI simulator that allows for dynamic and interactive exploration of mobile phone environments purely from screen images. The system uses a two-stage method to predict and synthesize the layout of the next UI state, enabling realistic simulation of UI transitions. UISim shows promising results in generating realistic and coherent subsequent UI states, offering practical benefits for UI testing, rapid prototyping, and AI agent training in dynamic mobile environments.

---

## AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need
**URL:** https://arxiv.org/abs/2509.21553

**Abstract:** Climate data science faces persistent barriers stemming from the fragmented nature of data sources, heterogeneous formats, and the steep technical expertise required to identify, acquire, and process datasets. These challenges limit participation, slow discovery, and reduce the reproducibility of scientific workflows. In this paper, we present a proof of concept for addressing these barriers through the integration of a curated knowledge graph (KG) with AI agents designed for cloud-native scientific workflows. The KG provides a unifying layer that organizes datasets, tools, and workflows, while AI agents -- powered by generative AI services -- enable natural language interaction, automated data access, and streamlined analysis. Together, these components drastically lower the technical threshold for engaging in climate data science, enabling non-specialist users to identify and analyze relevant datasets. By leveraging existing cloud-ready API data portals, we demonstrate that "a knowledge graph is all you need" to unlock scalable and agentic workflows for scientific inquiry. The open-source design of our system further supports community contributions, ensuring that the KG and associated tools can evolve as a shared commons. Our results illustrate a pathway toward democratizing access to climate data and establishing a reproducible, extensible framework for human--AI collaboration in scientific research.

**AI Summary:** This research introduces AutoClimDS, a system that integrates a knowledge graph with AI agents to address barriers in climate data science such as fragmented data sources and technical expertise requirements. The knowledge graph organizes datasets, tools, and workflows, while AI agents enable natural language interaction and automated data access, making it easier for non-specialists to engage in climate data analysis. The system demonstrates that a knowledge graph is sufficient to enable scalable and agentic workflows for scientific inquiry, paving the way for democratizing access to climate data and establishing a reproducible framework for human-AI collaboration in research.

---

## Toward a Realistic Encoding Model of Auditory Affective Understanding in the Brain
**URL:** https://arxiv.org/abs/2509.21381

**Abstract:** In affective neuroscience and emotion-aware AI, understanding how complex auditory stimuli drive emotion arousal dynamics remains unresolved. This study introduces a computational framework to model the brain's encoding of naturalistic auditory inputs into dynamic behavioral/neural responses across three datasets (SEED, LIRIS, self-collected BAVE). Guided by neurobiological principles of parallel auditory hierarchy, we decompose audio into multilevel auditory features (through classical algorithms and wav2vec 2.0/Hubert) from the original and isolated human voice/background soundtrack elements, mapping them to emotion-related responses via cross-dataset analyses. Our analysis reveals that high-level semantic representations (derived from the final layer of wav2vec 2.0/Hubert) exert a dominant role in emotion encoding, outperforming low-level acoustic features with significantly stronger mappings to behavioral annotations and dynamic neural synchrony across most brain regions ($p < 0.05$). Notably, middle layers of wav2vec 2.0/hubert (balancing acoustic-semantic information) surpass the final layers in emotion induction across datasets. Moreover, human voices and soundtracks show dataset-dependent emotion-evoking biases aligned with stimulus energy distribution (e.g., LIRIS favors soundtracks due to higher background energy), with neural analyses indicating voices dominate prefrontal/temporal activity while soundtracks excel in limbic regions. By integrating affective computing and neuroscience, this work uncovers hierarchical mechanisms of auditory-emotion encoding, providing a foundation for adaptive emotion-aware systems and cross-disciplinary explorations of audio-affective interactions.

**AI Summary:** This study introduces a computational framework to model how the brain encodes naturalistic auditory inputs into emotional responses. The analysis reveals that high-level semantic representations play a significant role in emotion encoding, outperforming low-level acoustic features. The study also highlights dataset-dependent biases in emotion-evoking stimuli and identifies specific brain regions involved in processing human voices and soundtracks. This research provides insights into the hierarchical mechanisms of auditory-emotion encoding, which can inform the development of emotion-aware AI systems and further interdisciplinary studies on audio-affective interactions.

---

## Adoption, usability and perceived clinical value of a UK AI clinical reference platform (iatroX): a mixed-methods formative evaluation of real-world usage and a 1,223-respondent user survey
**URL:** https://arxiv.org/abs/2509.21188

**Abstract:** Clinicians face growing information overload from biomedical literature and guidelines, hindering evidence-based care. Retrieval-augmented generation (RAG) with large language models may provide fast, provenance-linked answers, but requires real-world evaluation. We describe iatroX, a UK-centred RAG-based clinical reference platform, and report early adoption, usability, and perceived clinical value from a formative implementation evaluation. Methods comprised a retrospective analysis of usage across web, iOS, and Android over 16 weeks (8 April-31 July 2025) and an in-product intercept survey. Usage metrics were drawn from web and app analytics with bot filtering. A client-side script randomized single-item prompts to approx. 10% of web sessions from a predefined battery assessing usefulness, reliability, and adoption intent. Proportions were summarized with Wilson 95% confidence intervals; free-text comments underwent thematic content analysis. iatroX reached 19,269 unique web users, 202,660 engagement events, and approx. 40,000 clinical queries. Mobile uptake included 1,960 iOS downloads and Android growth (peak >750 daily active users). The survey yielded 1,223 item-level responses: perceived usefulness 86.2% (95% CI 74.8-93.9%; 50/58); would use again 93.3% (95% CI 68.1-99.8%; 14/15); recommend to a colleague 88.4% (95% CI 75.1-95.9%; 38/43); perceived accuracy 75.0% (95% CI 58.8-87.3%; 30/40); reliability 79.4% (95% CI 62.1-91.3%; 27/34). Themes highlighted speed, guideline-linked answers, and UK specificity. Early real-world use suggests iatroX can mitigate information overload and support timely answers for UK clinicians. Limitations include small per-item samples and early-adopter bias; future work will include accuracy audits and prospective studies on workflow and care quality.

**AI Summary:** The study evaluates the adoption, usability, and perceived clinical value of iatroX, a UK-based clinical reference platform that utilizes retrieval-augmented generation (RAG) with large language models to provide fast, provenance-linked answers to clinicians. The platform reached a significant number of unique users and received positive feedback on its usefulness, reliability, and accuracy from survey respondents. The findings suggest that iatroX has the potential to help mitigate information overload and provide timely answers for UK clinicians, although further research is needed to assess its impact on workflow and care quality.

---

## CafGa: Customizing Feature Attributions to Explain Language Models
**URL:** https://arxiv.org/abs/2509.20901

**Abstract:** Feature attribution methods, such as SHAP and LIME, explain machine learning model predictions by quantifying the influence of each input component. When applying feature attributions to explain language models, a basic question is defining the interpretable components. Traditional feature attribution methods, commonly treat individual words as atomic units. This is highly computationally inefficient for long-form text and fails to capture semantic information that spans multiple words. To address this, we present CafGa, an interactive tool for generating and evaluating feature attribution explanations at customizable granularities. CafGa supports customized segmentation with user interaction and visualizes the deletion and insertion curves for explanation assessments. Through a user study involving participants of various expertise, we confirm CafGa's usefulness, particularly among LLM practitioners. Explanations created using CafGa were also perceived as more useful compared to those generated by two fully automatic baseline methods: PartitionSHAP and MExGen, suggesting the effectiveness of the system.

**AI Summary:** The research introduces CafGa, a tool for customizing feature attributions to explain language models by allowing users to define interpretable components at customizable granularities. CafGa was found to be useful, particularly among LLM practitioners, and was perceived as more effective in generating explanations compared to two fully automatic baseline methods. The tool addresses the limitations of traditional feature attribution methods in capturing semantic information that spans multiple words in long-form text.

---

## Even More Kawaii than Real-Person-Driven VTubers? Understanding How Viewers Perceive AI-Driven VTubers
**URL:** https://arxiv.org/abs/2509.20817

**Abstract:** VTubers, digital personas represented by animated avatars, have gained massive popularity. Traditionally, VTubers are operated and voiced by human controllers known as Nakanohito. The reliance on Nakanohito, however, poses risks due to potential personal controversies and operational disruptions. The emergence of AI-driven VTubers offers a new model free from these human constraints. While AI-driven VTubers present benefits such as continuous operation and reduced scandal risk, they also raise questions about authenticity and audience engagement. Therefore, to gain deeper insights, we conduct a case study, investigating viewer perceptions of Neuro-sama, the most popular AI-driven VTuber with 845k followers on Twitch and 753k followers on YouTube. We analyze 108k Reddit posts and 136k YouTube comments, aiming to better understand viewer motivations, how AI constructs the virtual persona, and perceptions of the AI as Nakanohito. Our findings enhance the understanding of AI-driven VTubers and their impact on digital streaming culture.

**AI Summary:** This research explores the perception of AI-driven VTubers compared to traditional human-operated ones. The study focuses on Neuro-sama, a popular AI-driven VTuber, to analyze viewer motivations, the construction of the virtual persona by AI, and perceptions of AI as Nakanohito. The findings provide insights into the benefits and challenges of AI-driven VTubers in digital streaming culture, highlighting their potential to offer continuous operation and reduced scandal risk while raising questions about authenticity and audience engagement.

---

## AuthGlass: Enhancing Voice Authentication on Smart Glasses via Air-Bone Acoustic Features
**URL:** https://arxiv.org/abs/2509.20799

**Abstract:** With the rapid advancement of smart glasses, voice interaction has become widely deployed due to its naturalness and convenience. However, its practicality is often undermined by the vulnerability to spoofing attacks and interference from surrounding sounds, making seamless voice authentication crucial for smart glasses usage. To address this challenge, we propose AuthGlass, a voice authentication approach that leverages both air- and bone-conducted speech features to enhance accuracy and liveness detection. Aiming to gain comprehensive knowledge on speech-related acoustic and vibration features, we built a smart glasses prototype with redundant synchronized microphones: 14 air-conductive microphones and 2 bone-conductive units. In a study with 42 participants, we validated that combining sound-field and vibration features significantly improves authentication robustness and attack resistance. Furthermore, experiments demonstrated that AuthGlass maintains competitive accuracy even under various practical scenarios, highlighting its applicability and scalability for real-world deployment.

**AI Summary:** The research introduces AuthGlass, a voice authentication system for smart glasses that utilizes both air- and bone-conducted speech features to enhance accuracy and liveness detection, addressing vulnerabilities to spoofing attacks and interference from surrounding sounds. By incorporating redundant synchronized microphones in a smart glasses prototype, the study shows that combining sound-field and vibration features significantly improves authentication robustness and attack resistance, maintaining competitive accuracy under various practical scenarios and demonstrating its applicability and scalability for real-world deployment.

---

## Imagining Design Workflows in Agentic AI Futures
**URL:** https://arxiv.org/abs/2509.20731

**Abstract:** As designers become familiar with Generative AI, a new concept is emerging: Agentic AI. While generative AI produces output in response to prompts, agentic AI systems promise to perform mundane tasks autonomously, potentially freeing designers to focus on what they love: being creative. But how do designers feel about integrating agentic AI systems into their workflows? Through design fiction, we investigated how designers want to interact with a collaborative agentic AI platform. Ten professional designers imagined and discussed collaborating with an AI agent to organise inspiration sources and ideate. Our findings highlight the roles AI agents can play in supporting designers, the division of authority between humans and AI, and how designers' intent can be explained to AI agents beyond prompts. We synthesise our findings into a conceptual framework that identifies authority distribution among humans and AI agents and discuss directions for utilising AI agents in future design workflows.

**AI Summary:** This research explores the concept of Agentic AI, which promises to perform mundane tasks autonomously, allowing designers to focus on creativity. Through design fiction, the study found that designers are open to collaborating with AI agents to organize inspiration sources and ideate. The findings suggest that AI agents can support designers in their work, and highlight the importance of defining the division of authority between humans and AI in design workflows.

---

## Understanding Mode Switching in Human-AI Collaboration: Behavioral Insights and Predictive Modeling
**URL:** https://arxiv.org/abs/2509.20666

**Abstract:** Human-AI collaboration is typically offered in one of two of user control levels: guidance, where the AI provides suggestions and the human makes the final decision, and delegation, where the AI acts autonomously within user-defined constraints. Systems that integrate both modes, common in robotic surgery or driving assistance, often overlook shifts in user preferences within a task in response to factors like evolving trust, decision complexity, and perceived control. In this work, we investigate how users dynamically switch between higher and lower levels of control during a sequential decision-making task. Using a hand-and-brain chess setup, participants either selected a piece and the AI decided how it moved (brain mode), or the AI selected a piece and the participant decided how it moved (hand mode). We collected over 400 mode-switching decisions from eight participants, along with gaze, emotional state, and subtask difficulty data. Statistical analysis revealed significant differences in gaze patterns and subtask complexity prior to a switch and in the quality of the subsequent move. Based on these results, we engineered behavioral and task-specific features to train a lightweight model that predicted control level switches ($F1 = 0.65$). The model performance suggests that real-time behavioral signals can serve as a complementary input alongside system-driven mode-switching mechanisms currently used. We complement our quantitative results with qualitative factors that influence switching including perceived AI ability, decision complexity, and level of control, identified from post-game interview analysis. The combined behavioral and modeling insights can help inform the design of shared autonomy systems that need dynamic, subtask-level control switches aligned with user intent and evolving task demands.

**AI Summary:** This research investigates how users switch between different levels of control in human-AI collaboration during a sequential decision-making task. The study found significant differences in gaze patterns and subtask complexity before a switch, as well as differences in the quality of subsequent moves. A lightweight model was developed that successfully predicted control level switches, suggesting that real-time behavioral signals can be used alongside existing system-driven mechanisms to improve shared autonomy systems.

---

## MechStyle: Augmenting Generative AI with Mechanical Simulation to Create Stylized and Structurally Viable 3D Models
**URL:** https://arxiv.org/abs/2509.20571

**Abstract:** Recent developments in Generative AI enable creators to stylize 3D models based on text prompts. These methods change the 3D model geometry, which can compromise the model's structural integrity once fabricated. We present MechStyle, a system that enables creators to stylize 3D printable models while preserving their structural integrity. MechStyle accomplishes this by augmenting the Generative AI-based stylization process with feedback from a Finite Element Analysis (FEA) simulation. As the stylization process modifies the geometry to approximate the desired style, feedback from the FEA simulation reduces modifications to regions with increased stress. We evaluate the effectiveness of FEA simulation feedback in the augmented stylization process by comparing three stylization control strategies. We also investigate the time efficiency of our approach by comparing three adaptive scheduling strategies. Finally, we demonstrate MechStyle's user interface that allows users to generate stylized and structurally viable 3D models and provide five example applications.

**AI Summary:** The research introduces MechStyle, a system that combines Generative AI with mechanical simulation to create stylized 3D models while preserving their structural integrity. By incorporating feedback from Finite Element Analysis (FEA) simulation, MechStyle reduces modifications in regions with increased stress, ensuring the models remain viable for fabrication. The study evaluates the effectiveness of FEA simulation feedback and the time efficiency of the approach, demonstrating the system's user interface and providing five example applications.

---

## Perspectra: Choosing Your Experts Enhances Critical Thinking in Multi-Agent Research Ideation
**URL:** https://arxiv.org/abs/2509.20553

**Abstract:** Recent advances in multi-agent systems (MAS) enable tools for information search and ideation by assigning personas to agents. However, how users can effectively control, steer, and critically evaluate collaboration among multiple domain-expert agents remains underexplored. We present Perspectra, an interactive MAS that visualizes and structures deliberation among LLM agents via a forum-style interface, supporting @-mention to invite targeted agents, threading for parallel exploration, with a real-time mind map for visualizing arguments and rationales. In a within-subjects study with 18 participants, we compared Perspectra to a group-chat baseline as they developed research proposals. Our findings show that Perspectra significantly increased the frequency and depth of critical-thinking behaviors, elicited more interdisciplinary replies, and led to more frequent proposal revisions than the group chat condition. We discuss implications for designing multi-agent tools that scaffold critical thinking by supporting user control over multi-agent adversarial discourse.

**AI Summary:** The study introduces Perspectra, an interactive multi-agent system that enhances critical thinking in research ideation by allowing users to control and evaluate collaboration among domain-expert agents. The research found that Perspectra increased the frequency and depth of critical-thinking behaviors, elicited more interdisciplinary replies, and led to more frequent proposal revisions compared to a group-chat baseline. These findings suggest the importance of designing multi-agent tools that support user control over adversarial discourse to scaffold critical thinking.

---

## CHOIR: A Chatbot-mediated Organizational Memory Leveraging Communication in University Research Labs
**URL:** https://arxiv.org/abs/2509.20512

**Abstract:** University research labs often rely on chat-based platforms for communication and project management, where valuable knowledge surfaces but is easily lost in message streams. Documentation can preserve knowledge, but it requires ongoing maintenance and is challenging to navigate. Drawing on formative interviews that revealed organizational memory challenges in labs, we designed CHOIR, an LLM-based chatbot that supports organizational memory through four key functions: document-grounded Q&A, Q&A sharing for follow-up discussion, knowledge extraction from conversations, and AI-assisted document updates. We deployed CHOIR in four research labs for one month (n=21), where the lab members asked 107 questions and lab directors updated documents 38 times in the organizational memory. Our findings reveal a privacy-awareness tension: questions were asked privately, limiting directors' visibility into documentation gaps. Students often avoided contribution due to challenges in generalizing personal experiences into universal documentation. We contribute design implications for privacy-preserving awareness and supporting context-specific knowledge documentation.

**AI Summary:** The research focuses on addressing the challenge of maintaining organizational memory in university research labs by introducing CHOIR, a chatbot that helps capture and retain valuable knowledge from chat-based communication. The study found that while CHOIR facilitated knowledge sharing and document updates, there were privacy concerns related to the visibility of documentation gaps and challenges in generalizing personal experiences into universal documentation. The findings suggest design implications for privacy-awareness and context-specific knowledge documentation in research lab settings.

---

## Interactive Recommendation Agent with Active User Commands
**URL:** https://arxiv.org/abs/2509.21317

**Abstract:** Traditional recommender systems rely on passive feedback mechanisms that limit users to simple choices such as like and dislike. However, these coarse-grained signals fail to capture users' nuanced behavior motivations and intentions. In turn, current systems cannot also distinguish which specific item attributes drive user satisfaction or dissatisfaction, resulting in inaccurate preference modeling. These fundamental limitations create a persistent gap between user intentions and system interpretations, ultimately undermining user satisfaction and harming system effectiveness.
To address these limitations, we introduce the Interactive Recommendation Feed (IRF), a pioneering paradigm that enables natural language commands within mainstream recommendation feeds. Unlike traditional systems that confine users to passive implicit behavioral influence, IRF empowers active explicit control over recommendation policies through real-time linguistic commands. To support this paradigm, we develop RecBot, a dual-agent architecture where a Parser Agent transforms linguistic expressions into structured preferences and a Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly policy adjustment. To enable practical deployment, we employ simulation-augmented knowledge distillation to achieve efficient performance while maintaining strong reasoning capabilities. Through extensive offline and long-term online experiments, RecBot shows significant improvements in both user satisfaction and business outcomes.

**AI Summary:** Traditional recommender systems are limited by passive feedback mechanisms that fail to capture users' nuanced behavior motivations and intentions. This leads to inaccurate preference modeling and a gap between user intentions and system interpretations. To address these limitations, the Interactive Recommendation Feed (IRF) allows users to give natural language commands for real-time control over recommendation policies. The RecBot dual-agent architecture translates linguistic expressions into structured preferences and dynamically adjusts recommendation policies, leading to significant improvements in user satisfaction and business outcomes.

---

## Communication Bias in Large Language Models: A Regulatory Perspective
**URL:** https://arxiv.org/abs/2509.21075

**Abstract:** Large language models (LLMs) are increasingly central to many applications, raising concerns about bias, fairness, and regulatory compliance. This paper reviews risks of biased outputs and their societal impact, focusing on frameworks like the EU's AI Act and the Digital Services Act. We argue that beyond constant regulation, stronger attention to competition and design governance is needed to ensure fair, trustworthy AI. This is a preprint of the Communications of the ACM article of the same title.

**AI Summary:** This paper examines the potential for bias in large language models (LLMs) and its societal implications, particularly in relation to regulatory frameworks like the EU's AI Act and the Digital Services Act. The authors emphasize the need for increased focus on competition and design governance to ensure fair and trustworthy AI beyond traditional regulation. The findings highlight the importance of addressing communication bias in LLMs to promote ethical and unbiased AI applications.

---

## Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent
**URL:** https://arxiv.org/abs/2509.20729

**Abstract:** Large multi-modal models (LMMs) have advanced mobile GUI agents. However, existing methods struggle with real-world scenarios involving diverse app interfaces and evolving user needs. End-to-end methods relying on model's commonsense often fail on long-tail apps, and agents without user interaction act unilaterally, harming user experience. To address these limitations, we propose Fairy, an interactive multi-agent mobile assistant capable of continuously accumulating app knowledge and self-evolving during usage. Fairy enables cross-app collaboration, interactive execution, and continual learning through three core modules:(i) a Global Task Planner that decomposes user tasks into sub-tasks from a cross-app view; (ii) an App-Level Executor that refines sub-tasks into steps and actions based on long- and short-term memory, achieving precise execution and user interaction via four core agents operating in dual loops; and (iii) a Self-Learner that consolidates execution experience into App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a real-world benchmark with a comprehensive metric suite, and LMM-based agents for automated scoring. Experiments show that Fairy with GPT-4o backbone outperforms the previous SoTA by improving user requirement completion by 33.7% and reducing redundant steps by 58.5%, showing the effectiveness of its interaction and self-learning.

**AI Summary:** The research introduces Fairy, an interactive multi-agent mobile assistant designed to address the limitations of existing methods in handling diverse app interfaces and evolving user needs. Fairy utilizes a Global Task Planner, App-Level Executor, and Self-Learner modules to enable cross-app collaboration, interactive execution, and continual learning. Experimental results demonstrate that Fairy outperforms previous state-of-the-art models in improving user requirement completion and reducing redundant steps, showcasing the effectiveness of its interactive and self-learning capabilities in real-world scenarios.

---

## Cyber Racing Coach: A Haptic Shared Control Framework for Teaching Advanced Driving Skills
**URL:** https://arxiv.org/abs/2509.20653

**Abstract:** This study introduces a haptic shared control framework designed to teach human drivers advanced driving skills. In this context, shared control refers to a driving mode where the human driver collaborates with an autonomous driving system to control the steering of a vehicle simultaneously. Advanced driving skills are those necessary to safely push the vehicle to its handling limits in high-performance driving such as racing and emergency obstacle avoidance. Previous research has demonstrated the performance and safety benefits of shared control schemes using both subjective and objective evaluations. However, these schemes have not been assessed for their impact on skill acquisition on complex and demanding tasks. Prior research on long-term skill acquisition either applies haptic shared control to simple tasks or employs other feedback methods like visual and auditory aids. To bridge this gap, this study creates a cyber racing coach framework based on the haptic shared control paradigm and evaluates its performance in helping human drivers acquire high-performance driving skills. The framework introduces (1) an autonomous driving system that is capable of cooperating with humans in a highly performant driving scenario; and (2) a haptic shared control mechanism along with a fading scheme to gradually reduce the steering assistance from autonomy based on the human driver's performance during training. Two benchmarks are considered: self-learning (no assistance) and full assistance during training. Results from a human subject study indicate that the proposed framework helps human drivers develop superior racing skills compared to the benchmarks, resulting in better performance and consistency.

**AI Summary:** This research introduces a haptic shared control framework for teaching advanced driving skills, where human drivers collaborate with an autonomous system to control a vehicle's steering. The framework was found to significantly improve human drivers' performance and consistency in high-performance driving tasks like racing and emergency obstacle avoidance compared to traditional self-learning or fully assisted training methods. This study highlights the effectiveness of haptic shared control in skill acquisition for complex and demanding tasks, demonstrating the potential for enhancing driving education and safety.

---

## Beyond SSO: Mobile Money Authentication for Inclusive e-Government in Sub-Saharan Africa
**URL:** https://arxiv.org/abs/2509.20592

**Abstract:** The rapid adoption of Mobile Money Services (MMS) in Sub-Saharan Africa (SSA) offers a viable path to improve e-Government service accessibility in the face of persistent low internet penetration. However, existing Mobile Money Authentication (MMA) methods face critical limitations, including susceptibility to SIM swapping, weak session protection, and poor scalability during peak demand. This study introduces a hybrid MMA framework that combines Unstructured Supplementary Service Data (USSD)-based multi-factor authentication with secure session management via cryptographically bound JSON Web Tokens (JWT). Unlike traditional MMA systems that rely solely on SIM-PIN verification or smartphone-dependent biometrics, our design implements a three-factor authentication model; SIM verification, PIN entry, and session token binding, tailored for resource-constrained environments. Simulations and comparative analysis against OAuth-based Single Sign-On (SSO) methods reveal a 45% faster authentication time (8 seconds vs. 12 to 15 seconds), 15% higher success under poor network conditions (95% vs. 80%), and increased resistance to phishing and brute-force attacks. Penetration testing and threat modeling further demonstrate a substantial reduction in vulnerability exposure compared to conventional approaches. The primary contributions of this work are: (1) a hybrid authentication protocol that ensures offline accessibility and secure session continuity; (2) a tailored security framework addressing threats like SIM swapping and social engineering in SSA; and (3) demonstrated scalability for thousands of users with reduced infrastructure overhead. The proposed approach advances secure digital inclusion in SSA and other regions with similar constraints.

**AI Summary:** This research introduces a new hybrid Mobile Money Authentication (MMA) framework for e-Government services in Sub-Saharan Africa, addressing limitations of existing methods such as susceptibility to SIM swapping and poor scalability. The framework combines USSD-based multi-factor authentication with secure session management using JSON Web Tokens, resulting in faster authentication times, higher success rates under poor network conditions, and increased resistance to attacks. The study demonstrates the effectiveness of the new approach in improving security and accessibility for e-Government services in resource-constrained environments.

---

## InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature
**URL:** https://arxiv.org/abs/2509.20493

**Abstract:** The proliferation of scientific literature presents an increasingly significant challenge for researchers. While Large Language Models (LLMs) offer promise, existing tools often provide verbose summaries that risk replacing, rather than assisting, the reading of the source material. This paper introduces InsightGUIDE, a novel AI-powered tool designed to function as a reading assistant, not a replacement. Our system provides concise, structured insights that act as a "map" to a paper's key elements by embedding an expert's reading methodology directly into its core AI logic. We present the system's architecture, its prompt-driven methodology, and a qualitative case study comparing its output to a general-purpose LLM. The results demonstrate that InsightGUIDE produces more structured and actionable guidance, serving as a more effective tool for the modern researcher.

**AI Summary:** The research introduces InsightGUIDE, an AI-powered tool designed to assist researchers in critically reading scientific literature by providing concise and structured insights without replacing the source material. By embedding an expert's reading methodology into its AI logic, InsightGUIDE offers more structured and actionable guidance compared to general-purpose Large Language Models (LLMs), making it a more effective tool for modern researchers facing the challenge of navigating the vast amount of scientific literature available.

---

## AI-driven formative assessment and adaptive learning in data-science education: Evaluating an LLM-powered virtual teaching assistant
**URL:** https://arxiv.org/abs/2509.20369

**Abstract:** This paper presents VITA (Virtual Teaching Assistants), an adaptive distributed learning (ADL) platform that embeds a large language model (LLM)-powered chatbot (BotCaptain) to provide dialogic support, interoperable analytics, and integrity-aware assessment for workforce preparation in data science. The platform couples context-aware conversational tutoring with formative-assessment patterns designed to promote reflective reasoning. The paper describes an end-to-end data pipeline that transforms chat logs into Experience API (xAPI) statements, instructor dashboards that surface outliers for just-in-time intervention, and an adaptive pathway engine that routes learners among progression, reinforcement, and remediation content. The paper also benchmarks VITA conceptually against emerging tutoring architectures, including retrieval-augmented generation (RAG)--based assistants and Learning Tools Interoperability (LTI)--integrated hubs, highlighting trade-offs among content grounding, interoperability, and deployment complexity. Contributions include a reusable architecture for interoperable conversational analytics, a catalog of patterns for integrity-preserving formative assessment, and a practical blueprint for integrating adaptive pathways into data-science courses. The paper concludes with implementation lessons and a roadmap (RAG integration, hallucination mitigation, and LTI~1.3 / OpenID Connect) to guide multi-course evaluations and broader adoption. In light of growing demand and scalability constraints in traditional instruction, the approach illustrates how conversational AI can support engagement, timely feedback, and personalized learning at scale. Future work will refine the platform's adaptive intelligence and examine applicability across varied educational settings.

**AI Summary:** This paper introduces VITA, an adaptive distributed learning platform with a chatbot powered by a large language model to assist in data science education. The platform offers context-aware tutoring, formative assessment, and adaptive pathways for learners, with a focus on promoting reflective reasoning. The study benchmarks VITA against other tutoring architectures, highlighting its unique features and potential for scalability in providing personalized learning experiences in data science education.

---

## Visual Tools for Input and Reflection in Social Work
**URL:** https://arxiv.org/abs/2509.20307

**Abstract:** Social workers need visual tools to collect information about their client's life situation, so that they can reflect it together and choose tailored interventions. easyNWK and easyBiograph are two visual tools for the client's social network and life history. We recently redesigned both tools in a participatory design project with social work faculty and professionals. In this short paper we discuss these tools from perspective of input visualization systems.

**AI Summary:** This research focuses on the development of visual tools, easyNWK and easyBiograph, for social workers to gather and reflect on information about their clients' social network and life history. The tools were redesigned in collaboration with social work professionals and faculty. The study highlights the importance of input visualization systems in aiding social workers in choosing tailored interventions for their clients.

---

## Into the Void: Understanding Online Health Information in Low-Web Data Languages
**URL:** https://arxiv.org/abs/2509.20245

**Abstract:** Data voids--areas of the internet where reliable information is scarce or absent--pose significant challenges to online health information seeking, particularly for users operating in low-web data languages. These voids are increasingly encountered not on traditional search engines alone, but on social media platforms, which have gradually morphed into informal search engines for millions of people. In this paper, we introduce the phenomenon of data horizons: a critical boundary where algorithmic structures begin to degrade the relevance and reliability of search results. Unlike the core of a data void, which is often exploited by bad actors to spread misinformation, the data horizon marks the critical space where systemic factors, such as linguistic underrepresentation, algorithmic amplification, and socio-cultural mismatch, create conditions of informational instability. Focusing on Tigrinya and Amharic as languages of study, we evaluate (1) the common characteristics of search results for health queries, (2) the quality and credibility of health information, and (3) characteristics of search results that diverge from their queries. We find that search results for health queries in low-web data languages may not always be in the language of search and may be dominated by nutritional and religious advice. We show that search results that diverge from their queries in low-resourced languages are due to algorithmic failures, (un)intentional manipulation, or active manipulation by content creators. We use our findings to illustrate how a data horizon manifests under several interacting constraints on information availability.

**AI Summary:** This research paper explores the challenges of accessing reliable health information online in low-web data languages, focusing on Tigrinya and Amharic. The study highlights the phenomenon of data horizons, where algorithmic structures degrade the relevance and reliability of search results, leading to informational instability. The findings suggest that search results in these languages may not always be in the language of the search, and may be dominated by nutritional and religious advice, indicating a need for improved algorithms and linguistic representation in online health information seeking.

---

## How People Manage Knowledge in their "Second Brains"- A Case Study with Industry Researchers Using Obsidian
**URL:** https://arxiv.org/abs/2509.20187

**Abstract:** People face overwhelming information during work activities, necessitating effective organization and management strategies. Even in personal lives, individuals must keep, annotate, organize, and retrieve knowledge from daily routines. The collection of records for future reference is known as a personal knowledge base. Note-taking applications are valuable tools for building and maintaining these bases, often called a ''second brain''. This paper presents a case study on how people build and explore personal knowledge bases for various purposes. We selected the note-taking tool Obsidian and researchers from a Brazilian lab for an in-depth investigation. Our investigation reveals interesting findings about how researchers build and explore their personal knowledge bases. A key finding is that participants' knowledge retrieval strategy influences how they build and maintain their content. We suggest potential features for an AI system to support this process.

**AI Summary:** This research explores how industry researchers use the note-taking tool Obsidian to build and manage their personal knowledge bases, also known as "second brains". The study found that participants' knowledge retrieval strategies significantly impact how they organize and maintain their content. The findings suggest potential features for an AI system to support individuals in effectively managing their knowledge and information.

---

## Investigating the Effect of Prior Exposure and Fidelity on Quality and Realism Perception of VR Digital Twins
**URL:** https://arxiv.org/abs/2509.20106

**Abstract:** This study explores how prior exposure to physical objects influences the quality and realism perception of Digital Twins (DT) with varying levels of fidelity in Virtual Reality (VR). In a mixed experimental design, 24 participants were divided into two equal groups: an exposure group, in which members were shown physical objects before inspecting and rating their replicas in VR, and a control group without prior knowledge. Three objects were presented, each under four fidelity conditions with varying texture resolution and geometric detail. Participants rated perceived quality and realism through in-VR self-reports. Statistical analysis revealed that texture resolution significantly affected realism and quality perception, whereas geometric detail only influenced quality ratings. Investigating the between-factor, no significant effect of exposure on quality and realism perception was found. These findings raise important questions about the cognitive relationship between physical objects and their digital counterparts and how fidelity influences the perception of DTs in VR.

**AI Summary:** This study examines how prior exposure to physical objects and fidelity levels impact the perception of Digital Twins (DT) in Virtual Reality (VR). The research found that texture resolution significantly influenced realism perception, while geometric detail only affected quality ratings. Interestingly, prior exposure to physical objects did not have a significant impact on the perception of DTs in VR, suggesting that fidelity plays a crucial role in how these digital replicas are perceived.

---

## Interactive Semantic Segmentation for Phosphene Vision Neuroprosthetics
**URL:** https://arxiv.org/abs/2509.19957

**Abstract:** Visual impairments present significant challenges to individuals worldwide, impacting daily activities and quality of life. Visual neuroprosthetics offer a promising solution, leveraging advancements in technology to provide a simplified visual sense through devices comprising cameras, computers, and implanted electrodes. This study investigates user-centered design principles for a phosphene vision algorithm, utilizing feedback from visually impaired individuals to guide the development of a gaze-controlled semantic segmentation system. We conducted interviews revealing key design principles. These principles informed the implementation of a gaze-guided semantic segmentation algorithm using the Segment Anything Model (SAM). In a simulated phosphene vision environment, participants performed object detection tasks under SAM, edge detection, and normal vision conditions. SAM improved identification accuracy over edge detection, remained effective in complex scenes, and was particularly robust for specific object shapes. These findings demonstrate the value of user feedback and the potential of gaze-guided semantic segmentation to enhance neuroprosthetic vision.

**AI Summary:** This research focuses on developing a user-centered design for a phosphene vision algorithm to improve visual neuroprosthetics for visually impaired individuals. By incorporating feedback from visually impaired individuals, the study implemented a gaze-guided semantic segmentation algorithm using the Segment Anything Model (SAM), which showed improved identification accuracy compared to edge detection. The findings highlight the significance of user feedback in enhancing neuroprosthetic vision and the potential of gaze-guided semantic segmentation in improving visual perception for individuals with visual impairments.

---

