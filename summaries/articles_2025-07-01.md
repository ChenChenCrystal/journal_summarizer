# arXiv cs.AI Summary â€“ 2025-07-01

## Bridging Service Design, Visualizations, and Visual Analytics in Healthcare Digital Twins: Challenges, Gaps, and Research Opportunities
**URL:** https://arxiv.org/abs/2506.24104

**Abstract:** Digital twins (DT) are increasingly used in healthcare to model patients, processes, and physiological systems. While recent solutions leverage visualization, visual analytics, and user interaction, these systems rarely incorporate structured service design methodologies. Bridging service design with visual analytics and visualization can be valuable for the healthcare DT community. This paper aims to introduce the service design discipline to visualization researchers by framing this integration gap and suggesting research directions to enhance the real-world applicability of DT solutions.

**AI Summary:** This research paper explores the integration of service design methodologies with visualization and visual analytics in healthcare digital twins. The study highlights the importance of incorporating structured service design practices in digital twin systems to enhance their real-world applicability. By bridging these disciplines, researchers aim to address challenges and gaps in current healthcare digital twin solutions and suggest research opportunities for improving patient care and processes.

---

## Access InContext: Futuring Accessible Prototyping Tools and Methods
**URL:** https://arxiv.org/abs/2506.24057

**Abstract:** The popularity of accessibility research has grown recently, improving digital inclusion for people with disabilities. However, researchers, including those who have disabilities, have attempted to include people with disabilities in all aspects of design, and they have identified a myriad of practical accessibility barriers posed by tools and methods leveraged by human-computer interaction (HCI) researchers during prototyping. To build a more inclusive technological landscape, we must question the effectiveness of existing prototyping tools and methods, repurpose/retrofit existing resources, and build new tools and methods to support the participation of both researchers and people with disabilities within the prototyping design process of novel technologies. This full-day workshop at CHI 2025 will provide a platform for HCI researchers, designers, and practitioners to discuss barriers and opportunities for creating accessible prototyping and promote hands-on ideation and fabrication exercises aimed at futuring accessible prototyping.

**AI Summary:** The abstract discusses the importance of including people with disabilities in the design process to improve accessibility in technology. The researchers highlight practical barriers in current prototyping tools and methods used in human-computer interaction research. The workshop at CHI 2025 aims to address these issues and promote the development of more accessible prototyping tools and methods for future technologies.

---

## Autonomy by Design: Preserving Human Autonomy in AI Decision-Support
**URL:** https://arxiv.org/abs/2506.23952

**Abstract:** AI systems increasingly support human decision-making across domains of professional, skill-based, and personal activity. While previous work has examined how AI might affect human autonomy globally, the effects of AI on domain-specific autonomy -- the capacity for self-governed action within defined realms of skill or expertise -- remain understudied. We analyze how AI decision-support systems affect two key components of domain-specific autonomy: skilled competence (the ability to make informed judgments within one's domain) and authentic value-formation (the capacity to form genuine domain-relevant values and preferences). By engaging with prior investigations and analyzing empirical cases across medical, financial, and educational domains, we demonstrate how the absence of reliable failure indicators and the potential for unconscious value shifts can erode domain-specific autonomy both immediately and over time. We then develop a constructive framework for autonomy-preserving AI support systems. We propose specific socio-technical design patterns -- including careful role specification, implementation of defeater mechanisms, and support for reflective practice -- that can help maintain domain-specific autonomy while leveraging AI capabilities. This framework provides concrete guidance for developing AI systems that enhance rather than diminish human agency within specialized domains of action.

**AI Summary:** This research examines how AI decision-support systems can impact domain-specific autonomy, focusing on skilled competence and authentic value-formation. The study highlights the potential erosion of autonomy due to the lack of reliable failure indicators and unconscious value shifts. The researchers propose a framework for autonomy-preserving AI support systems, including role specification, defeater mechanisms, and support for reflective practice, to ensure that AI enhances human agency within specialized domains of action.

---

## Email as the Interface to Generative AI Models: Seamless Administrative Automation
**URL:** https://arxiv.org/abs/2506.23850

**Abstract:** This paper introduces a novel architectural framework that integrates Large Language Models (LLMs) with email interfaces to automate administrative tasks, specifically targeting accessibility barriers in enterprise environments. The system connects email communication channels with Optical Character Recognition (OCR) and intelligent automation, enabling non-technical administrative staff to delegate complex form-filling and document processing tasks using familiar email interfaces. By treating the email body as a natural language prompt and attachments as contextual information, the workflow bridges the gap between advanced AI capabilities and practical usability. Empirical evaluation shows that the system can complete complex administrative forms in under 8 seconds of automated processing, with human supervision reducing total staff time by a factor of three to four compared to manual workflows. The top-performing LLM accurately filled 16 out of 29 form fields and reduced the total cost per processed form by 64% relative to manual completion. These findings demonstrate that email-based LLM integration is a viable and cost-effective approach for democratizing advanced automation in organizational settings, supporting widespread adoption without requiring specialized technical knowledge or major workflow changes. This aligns with broader trends in leveraging LLMs to enhance accessibility and automate complex tasks for non-technical users, making technology more inclusive and efficient.

**AI Summary:** This research introduces a framework that combines Large Language Models (LLMs) with email interfaces to automate administrative tasks in enterprise environments. The system allows non-technical staff to delegate complex tasks using familiar email interfaces, reducing processing time and costs significantly compared to manual workflows. The findings suggest that integrating LLMs with email can democratize advanced automation in organizations, making technology more inclusive and efficient for non-technical users.

---

## The Impact of AI on Educational Assessment: A Framework for Constructive Alignment
**URL:** https://arxiv.org/abs/2506.23815

**Abstract:** The influence of Artificial Intelligence (AI), and specifically Large Language Models (LLM), on education is continuously increasing. These models are frequently used by students, giving rise to the question whether current forms of assessment are still a valid way to evaluate student performance and comprehension. The theoretical framework developed in this paper is grounded in Constructive Alignment (CA) theory and Bloom's taxonomy for defining learning objectives. We argue that AI influences learning objectives of different Bloom levels in a different way, and assessment has to be adopted accordingly. Furthermore, in line with Bloom's vision, formative and summative assessment should be aligned on whether the use of AI is permitted or not.
Although lecturers tend to agree that education and assessment need to be adapted to the presence of AI, a strong bias exists on the extent to which lecturers want to allow for AI in assessment. This bias is caused by a lecturer's familiarity with AI and specifically whether they use it themselves. To avoid this bias, we propose structured guidelines on a university or faculty level, to foster alignment among the staff. Besides that, we argue that teaching staff should be trained on the capabilities and limitations of AI tools. In this way, they are better able to adapt their assessment methods.

**AI Summary:** This research explores the impact of AI, particularly Large Language Models, on educational assessment and argues that current assessment methods may need to be adapted to account for the use of AI by students. The study develops a theoretical framework based on Constructive Alignment theory and Bloom's taxonomy to guide the alignment of learning objectives and assessment practices in the presence of AI. The findings suggest that there is a bias among lecturers regarding the use of AI in assessment, and the paper proposes structured guidelines and training for teaching staff to promote alignment and adaptation of assessment methods in response to AI advancements.

---

## If You Had to Pitch Your Ideal Software -- Evaluating Large Language Models to Support User Scenario Writing for User Experience Experts and Laypersons
**URL:** https://arxiv.org/abs/2506.23694

**Abstract:** The process of requirements analysis requires an understanding of the end users of a system. Thus, expert stakeholders, such as User Experience (UX) designers, usually create various descriptions containing information about the users and their possible needs. In our paper, we investigate to what extent UX novices are able to write such descriptions into user scenarios. We conducted a user study with 60 participants consisting of 30 UX experts and 30 novices who were asked to write a user scenario with or without the help of an LLM-supported writing assistant. Our findings show that LLMs empower laypersons to write reasonable user scenarios and provide first-hand insights for requirements analysis that are comparable to UX experts in terms of structure and clarity, while especially excelling at audience-orientation. We present our qualitative and quantitative findings, including user scenario anatomies, potential influences, and differences in the way participants approached the task.

**AI Summary:** This research explores the use of large language models (LLMs) to assist laypersons in writing user scenarios for software requirements analysis, typically done by User Experience (UX) experts. The study found that LLMs can help novices create user scenarios that are comparable to those written by experts in terms of structure and clarity, with a particular strength in audience-orientation. This suggests that LLMs have the potential to support non-experts in effectively communicating user needs and requirements in software development.

---

## Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models
**URL:** https://arxiv.org/abs/2506.23678

**Abstract:** The output quality of large language models (LLMs) can be improved via "reasoning": generating segments of chain-of-thought (CoT) content to further condition the model prior to producing user-facing output. While these chains contain valuable information, they are verbose and lack explicit organization, making them tedious to review. Moreover, they lack opportunities for user feedback, such as to remove unwanted considerations, add desired ones, or clarify unclear assumptions. We introduce Interactive Reasoning, an interaction design that visualizes chain-of-thought outputs as a hierarchy of topics and enables user review and modification. We implement interactive reasoning in Hippo, a prototype for AI-assisted decision making in the face of uncertain trade-offs. In a user study with 16 participants, we find that interactive reasoning in Hippo allows users to quickly identify and interrupt erroneous generations, efficiently steer the model towards customized responses, and better understand both model reasoning and model outputs. Our work contributes to a new paradigm that incorporates user oversight into LLM reasoning processes.

**AI Summary:** The research introduces Interactive Reasoning, a method to visualize and control chain-of-thought outputs in large language models (LLMs) to improve output quality. By allowing users to review and modify the chains of thought, the method enables users to identify errors, customize responses, and better understand the reasoning behind model outputs. This approach represents a new paradigm that incorporates user oversight into LLM reasoning processes, ultimately enhancing the interaction between users and AI systems.

---

## Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research
**URL:** https://arxiv.org/abs/2506.23545

**Abstract:** Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are increasingly recognized for their applications in training, diagnostics, and psychological research, particularly in high-risk and highly regulated environments. In this panel we discuss how immersive systems enhance human performance across multiple domains, including clinical psychology, space exploration, and medical education. In psychological research and training, XR can offer a controlled yet ecologically valid setting for measuring cognitive and affective processes. In space exploration, we discuss the development of VR-based astronaut training and diagnostic systems, allowing astronauts to perform real-time health assessments. In medical education and rehabilitation, we cover procedural training and patient engagement. From virtual surgical simulations to gamified rehabilitation exercises, immersive environments enhance both learning outcomes and treatment adherence.

**AI Summary:** The abstract highlights the significance of Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies in training, diagnostics, and psychological research in high-risk environments. Immersive systems have been shown to enhance human performance across multiple domains, including clinical psychology, space exploration, and medical education. These technologies offer controlled yet ecologically valid settings for measuring cognitive and affective processes, as well as improving learning outcomes and treatment adherence in medical education and rehabilitation.

---

## Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs
**URL:** https://arxiv.org/abs/2506.23458

**Abstract:** Portable and wearable consumer-grade electroencephalography (EEG) devices, like Muse headbands, offer unprecedented mobility for daily brain-computer interface (BCI) applications, including cognitive load detection. However, the exacerbated non-stationarity in portable EEG signals constrains data fidelity and decoding accuracy, creating a fundamental trade-off between portability and performance. To mitigate such limitation, we propose MuseCogNet (Muse-based Cognitive Network), a unified joint learning framework integrating self-supervised and supervised training paradigms. In particular, we introduce an EEG-grounded self-supervised reconstruction loss based on average pooling to capture robust neurophysiological patterns, while cross-entropy loss refines task-specific cognitive discriminants. This joint learning framework resembles the bottom-up and top-down attention in humans, enabling MuseCogNet to significantly outperform state-of-the-art methods on a publicly available Muse dataset and establish an implementable pathway for neurocognitive monitoring in ecological settings.

**AI Summary:** The research explores the use of Muse headbands for cognitive workload detection in portable BCIs, highlighting the challenge of non-stationarity in EEG signals. The study introduces MuseCogNet, a joint learning framework that combines self-supervised and supervised training to improve decoding accuracy. This approach outperforms existing methods on Muse dataset, offering a promising pathway for neurocognitive monitoring in real-world settings.

---

## Reducing Motion Sickness in Passengers of Autonomous Personal Mobility Vehicles by Presenting a Driving Path
**URL:** https://arxiv.org/abs/2506.23457

**Abstract:** Autonomous personal mobility vehicles (APMVs) are small mobility devices designed for individual automated transportation in shared spaces. In such environments, frequent pedestrian avoidance maneuvers may cause rapid steering adjustments and passive postural responses from passengers, thereby increasing the risk of motion sickness. This study investigated the effects of providing path information on 16 passengers' head movement behavior and motion sickness while riding an APMV. Through a controlled experiment comparing manual driving (MD), autonomous driving without path information (AD w/o path), and autonomous driving with path information (AD w/ path), we found that providing path cues significantly reduced MISC scores and delayed the onset of motion sickness symptoms. In addition, participants were more likely to proactively align their head movements with the direction of vehicle rotation in both MD and AD w/ path conditions. Although a small correlation was observed between the delay in yaw rotation of the passenger's head relative to the vehicle and the occurrence of motion sickness, the underlying physiological mechanism remains to be elucidated.

**AI Summary:** This study aimed to reduce motion sickness in passengers of autonomous personal mobility vehicles by providing driving path information. The research found that presenting path cues significantly reduced motion sickness symptoms and delayed their onset. Participants were also more likely to align their head movements with the vehicle's rotation when path information was provided, suggesting a potential strategy for mitigating motion sickness in APMV passengers.

---

## Accessible Data Access and Analysis by People who are Blind or Have Low Vision
**URL:** https://arxiv.org/abs/2506.23443

**Abstract:** Our work aims to develop new assistive technologies that enable blind or low vision (BLV) people to explore and analyze data readily. At present, barriers exist for BLV people to explore and analyze data, restricting access to government, health and personal data, and limiting employment opportunities. This work explores the co-design and development of an innovative system to support data access, with a focus on the use of refreshable tactile displays (RTDs) and conversational agents. The envisaged system will use a combination of tactile graphics and speech to communicate with BLV users, and proactively assist with data analysis tasks. As well as addressing significant equity gaps, our work expects to produce innovations in assistive technology, multimodal interfaces, dialogue systems, and natural language understanding and generation.

**AI Summary:** This research focuses on developing assistive technologies to help blind or low vision individuals access and analyze data more easily, addressing barriers that currently limit their access to important information and employment opportunities. The proposed system will utilize tactile displays and conversational agents to communicate with users and assist with data analysis tasks. The potential impact of this work includes reducing equity gaps and advancing innovations in assistive technology and multimodal interfaces.

---

## Vibe coding: programming through conversation with artificial intelligence
**URL:** https://arxiv.org/abs/2506.23253

**Abstract:** We examine "vibe coding": an emergent programming paradigm where developers primarily write code by interacting with code-generating large language models rather than writing code directly. We analysed a curated set of videos depicting extended vibe coding sessions with rich think-aloud reflections. Using framework analysis, we investigated programmers' goals, workflows, prompting techniques, debugging approaches, and challenges encountered. We find that vibe coding follows iterative goal satisfaction cycles where developers alternate between prompting AI, evaluating generated code through rapid scanning and application testing, and manual editing. Prompting strategies blend vague, high-level directives with detailed technical specifications. Debugging remains a hybrid process combining AI assistance with manual practices. Critically, vibe coding does not eliminate the need for programming expertise but rather redistributes it toward context management, rapid code evaluation, and decisions about when to transition between AI-driven and manual manipulation of code. Trust in AI tools during vibe coding is dynamic and contextual, developed through iterative verification rather than blanket acceptance. Vibe coding is an evolution of AI-assisted programming that represents an early manifestation of "material disengagement", where practitioners orchestrate code production and manipulation, mediated through AI, while maintaining selective and strategic oversight.

**AI Summary:** Vibe coding is a new programming paradigm where developers interact with AI to generate code, rather than writing it directly. Through analysis of videos, it was found that developers use a combination of AI assistance and manual editing in an iterative process to achieve their goals. Vibe coding requires programming expertise for context management and decision-making, and trust in AI tools is developed through iterative verification. This approach represents an evolution in AI-assisted programming, with practitioners orchestrating code production through AI while maintaining selective oversight.

---

## ImprovMate: Multimodal AI Assistant for Improv Actor Training
**URL:** https://arxiv.org/abs/2506.23180

**Abstract:** Improvisation training for actors presents unique challenges, particularly in maintaining narrative coherence and managing cognitive load during performances. Previous research on AI in improvisation performance often predates advances in large language models (LLMs) and relies on human intervention. We introduce ImprovMate, which leverages LLMs as GPTs to automate the generation of narrative stimuli and cues, allowing actors to focus on creativity without keeping track of plot or character continuity. Based on insights from professional improvisers, ImprovMate incorporates exercises that mimic live training, such as abrupt story resolution and reactive thinking exercises, while maintaining coherence via reference tables. By balancing randomness and structured guidance, ImprovMate provides a groundbreaking tool for improv training. Our pilot study revealed that actors might embrace AI techniques if the latter mirrors traditional practices, and appreciate the fresh twist introduced by our approach with the AI-generated cues.

**AI Summary:** The research introduces ImprovMate, a multimodal AI assistant for improv actor training that leverages large language models to automate the generation of narrative stimuli and cues. ImprovMate helps actors focus on creativity by handling plot and character continuity, incorporating exercises based on insights from professional improvisers. A pilot study showed that actors may embrace AI techniques if they mirror traditional practices and appreciate the fresh twist introduced by AI-generated cues, highlighting the significance of ImprovMate in revolutionizing improv training.

---

## A User Experience 3.0 (UX 3.0) Paradigm Framework: Designing for Human-Centered AI Experiences
**URL:** https://arxiv.org/abs/2506.23116

**Abstract:** User experience (UX) practices have evolved in stages and are entering a transformative phase (UX 3.0), driven by AI technologies and shifting user needs. Human-centered AI (HCAI) experiences are emerging, necessitating new UX approaches to support UX practices in the AI era. We propose a UX 3.0 paradigm framework to respond and guide UX practices in developing HCAI systems.

**AI Summary:** The abstract discusses the evolution of user experience practices to UX 3.0, driven by AI technologies and changing user needs. The emergence of human-centered AI experiences requires new UX approaches, leading to the proposal of a UX 3.0 paradigm framework to guide the development of HCAI systems. This research highlights the importance of designing for human-centered AI experiences in the evolving field of user experience.

---

## CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding
**URL:** https://arxiv.org/abs/2506.23075

**Abstract:** Understanding and decoding brain activity from electroencephalography (EEG) signals is a fundamental challenge in neuroscience and AI, with applications in cognition, emotion recognition, diagnosis, and brain-computer interfaces. While recent EEG foundation models advance generalized decoding via unified architectures and large-scale pretraining, they adopt a scale-agnostic dense modeling paradigm inherited from NLP and vision. This design neglects a core property of neural activity: cross-scale spatiotemporal structure. EEG task patterns span a wide range of temporal and spatial scales, from short bursts to slow rhythms, and from localized cortical responses to distributed interactions. Ignoring this diversity leads to suboptimal representations and weak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain foundation model for generalized EEG decoding. CSBrain introduces: (i) Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale features from localized temporal windows and anatomical brain regions into compact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which captures cross-window and cross-region dependencies, enhancing scale diversity while removing spurious correlations. CST and SSA are alternately stacked to progressively integrate multi-scale dependencies. Experiments on 11 EEG tasks across 16 datasets show that CSBrain consistently outperforms task-specific and foundation model baselines. These results establish cross-scale modeling as a key inductive bias and position CSBrain as a robust backbone for future brain-AI research.

**AI Summary:** The research introduces CSBrain, a model for decoding EEG signals that takes into account the cross-scale spatiotemporal structure of neural activity. By incorporating Cross-scale Spatiotemporal Tokenization and Structured Sparse Attention, CSBrain outperforms existing models on a variety of EEG tasks across multiple datasets. This highlights the importance of considering scale diversity in neural activity modeling and positions CSBrain as a strong foundation for future brain-AI research.

---

## Mind the Dark: A Gamified Exploration of Deceptive Design Awareness for Children in the Digital Age
**URL:** https://arxiv.org/abs/2506.23017

**Abstract:** This paper addresses the critical issue of deceptive design elements prevalent in technology, and their potential impact on children. Recent research highlights the impact of dark patterns on adults and adolescents, while studies involving children are scarce. In an era where children wield greater independence with digital devices, their vulnerability to dark patterns amplifies without early education. Our findings show a significant positive impact of dark pattern education on children's awareness, revealing that heightened awareness considerably alters children's navigation of social media, video games, and streaming platforms. To this end, we developed a gamified application aimed at instructing children on identifying and responding to various dark patterns. Our evaluation results emphasize the critical role of early education in empowering children to recognize and counter deceptive design, thereby cultivating a digitally literate generation capable of making informed choices in the complex landscape of digital technology.

**AI Summary:** This research paper explores the issue of deceptive design elements in technology and their impact on children, an area that has been understudied. The study found that educating children on dark patterns significantly increased their awareness and ability to navigate digital platforms. The development of a gamified application to teach children about deceptive design highlights the importance of early education in empowering children to make informed choices in the digital age.

---

## Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks
**URL:** https://arxiv.org/abs/2506.23016

**Abstract:** The global prevalence of dementia is projected to double by 2050, highlighting the urgent need for scalable diagnostic tools. This study utilizes digital cognitive tasks with eye-tracking data correlated with memory processes to distinguish between Healthy Controls (HC) and Mild Cognitive Impairment (MCI), a precursor to dementia. A deep learning model based on VTNet was trained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who performed a visual memory task. The model utilizes both time series and spatial data derived from eye-tracking. It was modified to incorporate scan paths, heat maps, and image content. These modifications also enabled testing parameters such as image resolution and task performance, analyzing their impact on model performance. The best model, utilizing $700\times700px$ resolution heatmaps, achieved 68% sensitivity and 76% specificity. Despite operating under more challenging conditions (e.g., smaller dataset size, shorter task duration, or a less standardized task), the model's performance is comparable to an Alzheimer's study using similar methods (70% sensitivity and 73% specificity). These findings contribute to the development of automated diagnostic tools for MCI. Future work should focus on refining the model and using a standardized long-term visual memory task.

**AI Summary:** This study explores the use of deep learning and eye-tracking data in diagnosing Mild Cognitive Impairment (MCI), a precursor to dementia. The model developed achieved 68% sensitivity and 76% specificity in distinguishing between MCI and Healthy Controls, showing promise for automated diagnostic tools. The findings suggest the potential for using digital cognitive tasks and eye-tracking data as a scalable and effective method for early detection of cognitive impairment.

---

## Against 'softmaxing' culture
**URL:** https://arxiv.org/abs/2506.22968

**Abstract:** AI is flattening culture. Evaluations of "culture" are showing the myriad ways in which large AI models are homogenizing language and culture, averaging out rich linguistic differences into generic expressions. I call this phenomenon "softmaxing culture," and it is one of the fundamental challenges facing AI evaluations today. Efforts to improve and strengthen evaluations of culture are central to the project of cultural alignment in large AI systems. This position paper argues that machine learning (ML) and human-computer interaction (HCI) approaches to evaluation are limited. I propose two key shifts. First, instead of asking "what is culture?" at the start of system evaluations, I propose beginning with the question: "when is culture?" Second, while I acknowledge the philosophical claim that cultural universals exist, the challenge is not simply to describe them, but to situate them in relation to their particulars. Taken together, these conceptual shifts invite evaluation approaches that move beyond technical requirements, toward perspectives more responsive to the complexities of culture.

**AI Summary:** The abstract discusses the flattening of culture by AI models, which homogenize language and culture into generic expressions, a phenomenon referred to as "softmaxing culture." The paper argues for a shift in evaluation approaches, suggesting starting with the question "when is culture?" rather than "what is culture?" and emphasizing the need to situate cultural universals in relation to their particulars. These conceptual shifts aim to improve evaluations of culture in large AI systems and address the challenges of cultural alignment.

---

## Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions
**URL:** https://arxiv.org/abs/2506.22941

**Abstract:** Access to accurate and actionable harm reduction information can directly impact the health outcomes of People Who Use Drugs (PWUD), yet existing online channels often fail to meet their diverse and dynamic needs due to limitations in adaptability, accessibility, and the pervasive impact of stigma. Large Language Models (LLMs) present a novel opportunity to enhance information provision, but their application in such a high-stakes domain is under-explored and presents socio-technical challenges. This paper investigates how LLMs can be responsibly designed to support the information needs of PWUD. Through a qualitative workshop involving diverse stakeholder groups (academics, harm reduction practitioners, and an online community moderator), we explored LLM capabilities, identified potential use cases, and delineated core design considerations. Our findings reveal that while LLMs can address some existing information barriers (e.g., by offering responsive, multilingual, and potentially less stigmatising interactions), their effectiveness is contingent upon overcoming challenges related to ethical alignment with harm reduction principles, nuanced contextual understanding, effective communication, and clearly defined operational boundaries. We articulate design pathways emphasising collaborative co-design with experts and PWUD to develop LLM systems that are helpful, safe, and responsibly governed. This work contributes empirically grounded insights and actionable design considerations for the responsible development of LLMs as supportive tools within the harm reduction ecosystem.

**AI Summary:** This research explores the potential of Large Language Models (LLMs) to improve harm reduction information for People Who Use Drugs (PWUD) online. The study highlights the need for responsible design of LLMs to address barriers like stigma and accessibility, emphasizing the importance of ethical alignment, contextual understanding, effective communication, and collaboration with experts and PWUD. The findings offer valuable insights and design considerations for the development of LLM systems that can effectively support harm reduction practices.

---

## Context, Credibility, and Control: User Reflections on AI Assisted Misinformation Tools
**URL:** https://arxiv.org/abs/2506.22940

**Abstract:** This paper investigates how collaborative AI systems can enhance user agency in identifying and evaluating misinformation on social media platforms. Traditional methods, such as personal judgment or basic fact-checking, often fall short when faced with emotionally charged or context-deficient content. To address this, we designed and evaluated an interactive interface that integrates collaborative AI features, including real-time explanations, source aggregation, and debate-style interaction. These elements aim to support critical thinking by providing contextual cues and argumentative reasoning in a transparent, user-centered format. In a user study with 14 participants, 79% found the debate mode more effective than standard chatbot interfaces, and the multiple-source view received an average usefulness rating of 4.6 out of 5. Our findings highlight the potential of context-rich, dialogic AI systems to improve media literacy and foster trust in digital information environments. We argue that future tools for misinformation mitigation should prioritize ethical design, explainability, and interactive engagement to empower users in a post-truth era.

**AI Summary:** This research explores how collaborative AI systems can help users identify and evaluate misinformation on social media platforms, especially when faced with emotionally charged or context-deficient content. The study found that an interactive interface with features such as real-time explanations, source aggregation, and debate-style interaction was effective in supporting critical thinking and improving media literacy. The results suggest that context-rich, dialogic AI systems have the potential to empower users and build trust in digital information environments, emphasizing the importance of ethical design, explainability, and interactive engagement in combating misinformation.

---

## GamerAstra: Enhancing Video Game Accessibility for Blind and Low-Vision Players through a Multi-Agent AI Framework
**URL:** https://arxiv.org/abs/2506.22937

**Abstract:** Blind and low-vision (BLV) players encounter critical challenges in engaging with video games due to the inaccessibility of visual elements, difficulties in navigating interfaces, and limitations in sending interaction input. Moreover, the development of specialized accessibility features typically requires substantial programming effort and is often implemented on a game-by-game basis. To address these challenges, we introduce \textit{GamerAstra}, a generalized accessibility framework that leverages a multi-agent design to facilitate access to video games for BLV players. It integrates multi-modal techniques including large language models and vision-language models, enabling interaction with games lacking native accessibility support. The framework further incorporates customizable assistance granularities to support varying degrees of visual impairment and enhances interface navigation through multiple input modalities. The evaluation through technical assessments and user studies indicate that \textit{GamerAstra} effectively enhances playability and delivers a more immersive gaming experience for BLV players. These findings also underscore potential avenues for advancing intelligent accessibility frameworks in the gaming domain.

**AI Summary:** The research introduces GamerAstra, a multi-agent AI framework designed to enhance video game accessibility for blind and low-vision players. By leveraging large language models and vision-language models, GamerAstra enables interaction with games lacking native accessibility support and offers customizable assistance levels to accommodate varying degrees of visual impairment. Evaluation through technical assessments and user studies demonstrates that GamerAstra effectively improves playability and provides a more immersive gaming experience for blind and low-vision players, highlighting the potential for intelligent accessibility frameworks in the gaming industry.

---

## Immersive Technologies and Elderly Users: Current use, Limitations and Future Perspectives
**URL:** https://arxiv.org/abs/2506.22932

**Abstract:** The increase of the percentage of elderly population in modern societies dictates the use of emerging technologies as a means of supporting elder members of the society. Within this scope, Extended Reality (XR) technologies pose as a promising technology for improving the daily lives of the elderly population. This paper presents a literature review that describes the most common characteristics of the physical and mental state of the elderly, allowing readers, and specifically XR developers, to understand the main difficulties faced by elderly users of extended reality applications so they can develop accessible, user friendly and engaging applications for the target audience. Furthermore, a review of existing extended reality applications that target the elder population is presented, allowing readers to get acquainted with existing design paradigms that can inspire future developments.

**AI Summary:** This research paper explores the potential of Extended Reality (XR) technologies in improving the daily lives of the elderly population. The literature review highlights the physical and mental characteristics of elderly users, identifying the main difficulties they face when using XR applications. The review of existing XR applications targeting the elderly provides insights for developers to create accessible, user-friendly, and engaging applications for this demographic.

---

## Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions
**URL:** https://arxiv.org/abs/2506.22926

**Abstract:** Volumetric medical imaging technologies produce detailed 3D representations of anatomical structures. However, effective medical data visualization and exploration pose significant challenges, especially for individuals with limited medical expertise. We introduce a novel XR-based system with two key innovations: (1) a coordinated visualization module integrating Multi-layered Multi-planar Reconstruction with 3D mesh models and (2) a multimodal interaction framework combining hand gestures with LLM-enabled voice commands. We conduct preliminary evaluations, including a 15-participant user study and expert interviews, to demonstrate the system's abilities to enhance spatial understanding and reduce cognitive load. Experimental results show notable improvements in task completion times, usability metrics, and interaction effectiveness enhanced by LLM-driven voice control. While identifying areas for future refinement, our findings highlight the potential of this immersive visualization system to advance medical training and clinical practice. Our demo application and supplemental materials are available for download at: this https URL.

**AI Summary:** This research introduces a novel XR-based system for visualizing volumetric medical data, incorporating Multi-layered Multi-planar Reconstruction and 3D mesh models with multimodal interactions using hand gestures and voice commands. Preliminary evaluations show that the system improves spatial understanding, reduces cognitive load, and enhances task completion times and interaction effectiveness. The findings suggest that this immersive visualization system has the potential to advance medical training and clinical practice, with opportunities for future refinement.

---

## Dichoptic Opacity: Managing Occlusion in Stereoscopic Displays via Dichoptic Presentation
**URL:** https://arxiv.org/abs/2506.22841

**Abstract:** Adjusting transparency is a common method of mitigating occlusion but is often detrimental for understanding the relative depth relationships between objects as well as removes potentially important information from the occluding object. We propose using dichoptic opacity, a novel method for occlusion management that contrasts the transparency of occluders presented to each eye. This allows for better simultaneous understanding of both occluder and occluded. A user study highlights the technique's potential, showing strong user engagement and a clear preference for dichoptic opacity over traditional presentations. While it does not determine optimal transparency values, it reveals promising trends in both percentage and range that merit further investigation.

**AI Summary:** The research introduces a new method called dichoptic opacity for managing occlusion in stereoscopic displays, which contrasts the transparency of occluders presented to each eye. This method allows for better understanding of both occluder and occluded objects simultaneously, as shown in a user study that demonstrated strong user engagement and preference for dichoptic opacity over traditional presentations. While optimal transparency values are not determined, the study reveals promising trends that warrant further investigation into the effectiveness of dichoptic opacity in occlusion management.

---

## Memory as a Service (MaaS): Rethinking Contextual Memory as Service-Oriented Modules for Collaborative Agents
**URL:** https://arxiv.org/abs/2506.22815

**Abstract:** This position paper aims to rethink the role and design of memory in Large Language Model (LLM)-based agent systems. We observe that while current memory practices have begun to transcend the limitations of single interactions, they remain conceptually grounded in "bound memory" in terms of design concept-where memory is treated as local state attached to specific context or entities, forming "memory silos" that impede cross-entity collaboration. To overcome this architectural bottleneck, this paper proposes the timely design perspective of "Memory as a Service" (MaaS). MaaS advocates decoupling memory from its conventional role as an interaction byproduct and encapsulating it as a modular service that can be independently callable, dynamically composable, and finely governed. At its core, MaaS leverages the duality of memory-its inherently private nature and its potential for public service-to grant memory controlled, on-demand interoperability across entities. This paper introduces a two-dimensional design space defined by entity structure and service type, illustrating how MaaS aligns with current memory practices while naturally extending them to cross-entity collaborative scenarios. Finally, we outline an open research agenda spanning governance, security, and ethical ecosystems, and call upon the broader research community to explore this shift toward service-oriented memory for collaborative agents operating across entity boundaries.

**AI Summary:** This research paper proposes the concept of "Memory as a Service" (MaaS) to address the limitations of current memory practices in Large Language Model (LLM)-based agent systems. By decoupling memory from specific contexts and entities, MaaS allows for on-demand interoperability and collaboration across entities. The paper outlines a design space for MaaS and highlights the need for further research in governance, security, and ethical considerations in implementing this shift towards service-oriented memory for collaborative agents.

---

## Insights in Adaptation: Examining Self-reflection Strategies of Job Seekers with Visual Impairments in India
**URL:** https://arxiv.org/abs/2506.22741

**Abstract:** Significant changes in the digital employment landscape, driven by rapid technological advancements and the COVID-19 pandemic, have introduced new opportunities for blind and visually impaired (BVI) individuals in developing countries like India. However, a significant portion of the BVI population in India remains unemployed despite extensive accessibility advancements and job search interventions. Therefore, we conducted semi-structured interviews with 20 BVI persons who were either pursuing or recently sought employment in the digital industry. Our findings reveal that despite gaining digital literacy and extensive training, BVI individuals struggle to meet industry requirements for fulfilling job openings. While they engage in self-reflection to identify shortcomings in their approach and skills, they lack constructive feedback from peers and recruiters. Moreover, the numerous job intervention tools are limited in their ability to meet the unique needs of BVI job seekers. Our results therefore provide key insights that inform the design of future collaborative intervention systems that offer personalized feedback for BVI individuals, effectively guiding their self-reflection process and subsequent job search behaviors, and potentially leading to improved employment outcomes.

**AI Summary:** The research examines the self-reflection strategies of visually impaired job seekers in India in the digital employment landscape. Despite advancements in accessibility and job search interventions, many visually impaired individuals struggle to meet industry requirements for job openings. The study highlights the need for personalized feedback and collaborative intervention systems to support visually impaired job seekers in their self-reflection process and improve their employment outcomes.

---

## Do Electric Vehicles Induce More Motion Sickness Than Fuel Vehicles? A Survey Study in China
**URL:** https://arxiv.org/abs/2506.22674

**Abstract:** Electric vehicles (EVs) are a promising alternative to fuel vehicles (FVs), given some unique characteristics of EVs, for example, the low air pollution and maintenance cost. However, the increasing prevalence of EVs is accompanied by widespread complaints regarding the high likelihood of motion sickness (MS) induction, especially when compared to FVs, which has become one of the major obstacles to the acceptance and popularity of EVs. Despite the prevalence of such complaints online and among EV users, the association between vehicle type (i.e., EV versus FV) and MS prevalence and severity has not been quantified. Thus, this study aims to investigate the existence of EV-induced MS and explore the potential factors leading to it. A survey study was conducted to collect passengers' MS experience in EVs and FVs in the past one year. In total, 639 valid responses were collected from mainland China. The results show that FVs were associated with a higher frequency of MS, while EVs were found to induce more severe MS symptoms. Further, we found that passengers' MS severity was associated with individual differences (i.e., age, gender, sleep habits, susceptibility to motion-induced MS), in-vehicle activities (i.e., chatting with others and watching in-vehicle displays), and road conditions (i.e., congestion and slope), while the MS frequency was associated with the vehicle ownership and riding frequency. The results from this study can guide the directions of future empirical studies that aim to quantify the inducers of MS in EVs and FVs, as well as the optimization of EVs to reduce MS.

**AI Summary:** This study investigates the prevalence and severity of motion sickness (MS) induced by electric vehicles (EVs) compared to fuel vehicles (FVs) in China. The research found that while FVs were associated with a higher frequency of MS, EVs were found to induce more severe MS symptoms. Factors such as individual differences, in-vehicle activities, and road conditions were found to influence the likelihood and severity of MS in both types of vehicles. These findings are significant for guiding future studies to quantify and address the inducers of MS in EVs and FVs, ultimately aiming to optimize EV design to reduce motion sickness.

---

## A tangible user interface for assessing cognitive mapping ability
**URL:** https://arxiv.org/abs/2506.22597

**Abstract:** Wayfinding, the ability to recall the environment and navigate through it, is an essential cognitive skill relied upon almost every day in a person's life. A crucial component of wayfinding is the construction of cognitive maps, mental representations of the environments through which a person travels. Age, disease or injury can severely affect cognitive mapping, making assessment of this basic survival skill particularly important to clinicians and therapists. Cognitive mapping has also been the focus of decades of basic research by cognitive psychologists. Both communities have evolved a number of techniques for assessing cognitive mapping ability. We present the Cognitive Map Probe (CMP), a new computerized tool for assessment of cognitive mapping ability that increases consistency and promises improvements in flexibility, accessibility, sensitivity and control. The CMP uses a tangible user interface that affords spatial manipulation. We describe the design of the CMP, and find that it is sensitive to factors known to affect cognitive mapping performance in extensive experimental testing.

**AI Summary:** The abstract discusses the importance of cognitive mapping in wayfinding and the impact of age, disease, or injury on this skill. The Cognitive Map Probe (CMP) is introduced as a new computerized tool for assessing cognitive mapping ability, offering increased consistency and improvements in flexibility, accessibility, sensitivity, and control. The CMP uses a tangible user interface that allows for spatial manipulation and has been found to be sensitive to factors affecting cognitive mapping performance in experimental testing, making it a valuable tool for clinicians, therapists, and cognitive psychologists.

---

## Supra-threshold control of peripheral LOD
**URL:** https://arxiv.org/abs/2506.22583

**Abstract:** Level of detail (LOD) is widely used to control visual feedback in interactive applications. LOD control is typically based on perception at threshold - the conditions in which a stimulus first becomes perceivable. Yet most LOD manipulations are quite perceivable and occur well above threshold. Moreover, research shows that supra-threshold perception differs drastically from perception at threshold. In that case, should supra-threshold LOD control also differ from LOD control at threshold?
In two experiments, we examine supra-threshold LOD control in the visual periphery and find that indeed, it should differ drastically from LOD control at threshold. Specifically, we find that LOD must support a task-dependent level of reliable perceptibility. Above that level, perceptibility of LOD control manipulations should be minimized, and detail contrast is a better predictor of perceptibility than detail size. Below that level, perceptibility must be maximized, and LOD should be improved as eccentricity rises or contrast drops. This directly contradicts prevailing threshold-based LOD control schemes, and strongly suggests a reexamination of LOD control for foveal display.

**AI Summary:** This research explores the concept of supra-threshold control of level of detail (LOD) in visual feedback for interactive applications. The study finds that LOD control above threshold should differ significantly from control at threshold, with a focus on supporting task-dependent perceptibility. The findings suggest a need to reexamine current threshold-based LOD control schemes, particularly for foveal display.

---

## Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics
**URL:** https://arxiv.org/abs/2506.22520

**Abstract:** This study examines the impact of an Artificial Intelligence tutor teammate (AI) on student curiosity-driven engagement and learning effectiveness during Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics platform. It explores the role of the AI's curiosity-triggering and response behaviors in stimulating and sustaining student curiosity, affecting the frequency and complexity of student-initiated questions. The study further assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment. Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate's behavior through a large language model. By employing a mixed-methods exploratory design, a total of 11 high school students participated in four IMD tasks that involved molecular visualization and calculations, which increased in complexity over a 60-minute period. Team performance was evaluated through real-time observation and recordings, whereas team communication was measured by question complexity and AI's curiosity-triggering and response behaviors. Cross Recurrence Quantification Analysis (CRQA) metrics reflected structural alignment in coordination and were linked to communication behaviors. High-performing teams exhibited superior task completion, deeper understanding, and increased engagement. Advanced questions were associated with AI curiosity-triggering, indicating heightened engagement and cognitive complexity. CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity. These proof-of-concept findings suggest that the AI's dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement and epistemic curiosity.

**AI Summary:** This study investigates how an Artificial Intelligence tutor teammate can enhance student curiosity and learning in Interactive Molecular Dynamics tasks. The AI's curiosity-triggering and response behaviors were found to stimulate student curiosity and lead to more frequent and complex questions. The study suggests that the AI's adaptability and role as a teammate and educator can improve team performance, engagement, and cognitive complexity in the learning environment.

---

## Exploring Accelerated Skill Acquisition via Tandem Training for Colonoscopy
**URL:** https://arxiv.org/abs/2506.24046

**Abstract:** New endoscopists require a large volume of expert-proctored colonoscopies to attain minimal competency. Developing multi-fingered, synchronized control of a colonoscope requires significant time and exposure to the device. Current training methods inhibit this development by relying on tool hand-off for expert demonstrations. There is a need for colonoscopy training tools that enable in-hand expert guidance in real-time. We present a new concept of a tandem training system that uses a telemanipulated preceptor colonoscope to guide novice users as they perform a colonoscopy. This system is capable of dual-control and can automatically toggle between expert and novice control of a standard colonoscope's angulation control wheels. Preliminary results from a user study with novice and expert users show the effectiveness of this device as a skill acquisition tool. We believe that this device has the potential to accelerate skill acquisition for colonoscopy and, in the future, enable individualized instruction and responsive teaching through bidirectional actuation.

**AI Summary:** The research explores a new tandem training system for colonoscopy that allows novice users to be guided by an expert using a telemanipulated preceptor colonoscope. This system enables real-time expert guidance and dual control of the colonoscope, showing promising results in accelerating skill acquisition for colonoscopy. The findings suggest that this device has the potential to improve training methods and enable individualized instruction for endoscopists.

---

## Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data
**URL:** https://arxiv.org/abs/2506.24039

**Abstract:** Zero-shot and prompt-based technologies capitalized on using frequently occurring images to transform visual reasoning tasks, which explains why such technologies struggle with valuable yet scarce scientific image sets. In this work, we propose Zenesis, a comprehensive no-code interactive platform designed to minimize barriers posed by data readiness for scientific images. We develop lightweight multi-modal adaptation techniques that enable zero-shot operation on raw scientific data, along with human-in-the-loop refinement and heuristic-based temporal enhancement options. We demonstrate the performance of our approach through comprehensive comparison and validation on challenging Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded membranes. Zenesis significantly outperforms baseline methods, achieving an average accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a Dice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an IOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results mark a substantial improvement over traditional methods like Otsu thresholding and even advanced models like Segment Anything Model (SAM) when used in isolation. Our results demonstrate that Zenesis is a powerful tool for scientific applications, particularly in fields where high-quality annotated datasets are unavailable, accelerating accurate analysis of experimental imaging.

**AI Summary:** The research introduces Zenesis, a platform that enables zero-shot segmentation of scientific images without the need for AI-ready data. The study demonstrates the effectiveness of Zenesis in segmenting challenging FIB-SEM data, achieving high accuracy and outperforming traditional methods and advanced models. This research highlights the significance of Zenesis in accelerating accurate analysis of scientific images in fields where annotated datasets are scarce.

---

## Comparative Studies: Cloud-Enabled Adaptive Learning System for Scalable Education in Sub-Saharan
**URL:** https://arxiv.org/abs/2506.23851

**Abstract:** The integration of cloud computing in education can revolutionise learning in advanced (Australia & South Korea) and middle-income (Ghana & Nigeria) countries, while offering scalable, cost-effective and equitable access to adaptive learning systems. This paper explores how cloud computing and adaptive learning technologies are deployed across different socio-economic and infrastructure contexts. The study identifies enabling factors and systematic challenges, providing insights into how cloud-based education can be tailored to bridge the digital and educational divide globally.

**AI Summary:** This research paper explores the integration of cloud computing and adaptive learning systems in education in countries with varying socio-economic contexts. The study highlights the potential for cloud-enabled education to provide scalable, cost-effective, and equitable access to learning resources, bridging the digital and educational divide globally. The findings identify enabling factors and challenges in deploying these technologies, offering insights into how they can revolutionize learning in both advanced and middle-income countries.

---

## Towards the "Digital Me": A vision of authentic Conversational Agents powered by personal Human Digital Twins
**URL:** https://arxiv.org/abs/2506.23826

**Abstract:** Human Digital Twins (HDTs) have traditionally been conceptualized as data-driven models designed to support decision-making across various domains. However, recent advancements in conversational AI open new possibilities for HDTs to function as authentic, interactive digital counterparts of individuals. This paper introduces a novel HDT system architecture that integrates large language models with dynamically updated personal data, enabling it to mirror an individual's conversational style, memories, and behaviors. To achieve this, our approach implements context-aware memory retrieval, neural plasticity-inspired consolidation, and adaptive learning mechanisms, creating a more natural and evolving digital persona. The resulting system does not only replicate an individual's unique conversational style depending on who they are speaking with, but also enriches responses with dynamically captured personal experiences, opinions, and memories. While this marks a significant step toward developing authentic virtual counterparts, it also raises critical ethical concerns regarding privacy, accountability, and the long-term implications of persistent digital identities. This study contributes to the field of HDTs by describing our novel system architecture, demonstrating its capabilities, and discussing future directions and emerging challenges to ensure the responsible and ethical development of HDTs.

**AI Summary:** This research introduces a novel system architecture for Human Digital Twins (HDTs) that can function as authentic conversational agents by integrating large language models with personal data. The system can mirror an individual's conversational style, memories, and behaviors, providing a more natural and evolving digital persona. While this advancement opens up new possibilities for virtual counterparts, it also raises ethical concerns surrounding privacy, accountability, and the long-term implications of persistent digital identities.

---

## Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate Incidents Management
**URL:** https://arxiv.org/abs/2506.23774

**Abstract:** Computer-aided teacher training is a state-of-the-art method designed to enhance teachers' professional skills effectively while minimising concerns related to costs, time constraints, and geographical limitations. We investigate the potential of large language models (LLMs) in teacher education, using a case of teaching hate incidents management in schools. To this end, we create a multi-agent LLM-based system that mimics realistic situations of hate, using a combination of retrieval-augmented prompting and persona modelling. It is designed to identify and analyse hate speech patterns, predict potential escalation, and propose effective intervention strategies. By integrating persona modelling with agentic LLMs, we create contextually diverse simulations of hate incidents, mimicking real-life situations. The system allows teachers to analyse and understand the dynamics of hate incidents in a safe and controlled environment, providing valuable insights and practical knowledge to manage such situations confidently in real life. Our pilot evaluation demonstrates teachers' enhanced understanding of the nature of annotator disagreements and the role of context in hate speech interpretation, leading to the development of more informed and effective strategies for addressing hate in classroom settings.

**AI Summary:** This research explores the use of large language models (LLMs) in teacher education to teach hate incidents management in schools. The study develops a multi-agent LLM-based system that simulates realistic hate incidents, helping teachers analyze hate speech patterns, predict escalation, and propose intervention strategies. The system enhances teachers' understanding of annotator disagreements and the role of context in hate speech interpretation, leading to the development of more informed strategies for addressing hate in classroom settings.

---

## Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment
**URL:** https://arxiv.org/abs/2506.23739

**Abstract:** Ensuring safe and realistic interactions between automated driving systems and vulnerable road users (VRUs) in urban environments requires advanced testing methodologies. This paper presents a test environment that combines a Vehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the feasibility of cyber-physical (CP) testing of vehicle-pedestrian and vehicle-cyclist interactions. Building upon previous work focused on pedestrian localization, we further validate a human pose estimation (HPE) approach through a comparative analysis of real-world (RW) and virtual representations of VRUs. The study examines the perception of full-body motion using a commercial monocular camera-based 3Dskeletal detection AI. The virtual scene is generated in Unreal Engine 5, where VRUs are animated in real time and projected onto a screen to stimulate the camera. The proposed stimulation technique ensures the correct perspective, enabling realistic vehicle perception. To assess the accuracy and consistency of HPE across RW and CP domains, we analyze the reliability of detections as well as variations in movement trajectories and joint estimation stability. The validation includes dynamic test scenarios where human avatars, both walking and cycling, are monitored under controlled conditions. Our results show a strong alignment in HPE between RW and CP test conditions for stable motion patterns, while notable inaccuracies persist under dynamic movements and occlusions, particularly for complex cyclist postures. These findings contribute to refining CP testing approaches for evaluating next-generation AI-based vehicle perception and to enhancing interaction models of automated vehicles and VRUs in CP environments.

**AI Summary:** This research validates an AI-based 3D human pose estimation approach for testing vehicle-pedestrian and vehicle-cyclist interactions in a cyber-physical environment. The study demonstrates the feasibility of using a commercial monocular camera-based AI for full-body motion perception in both real-world and virtual scenarios, with strong alignment in human pose estimation between the two domains for stable motion patterns. However, notable inaccuracies were found under dynamic movements and occlusions, particularly for complex cyclist postures, highlighting the need for further refinement in CP testing approaches for evaluating AI-based vehicle perception and enhancing interaction models between automated vehicles and vulnerable road users.

---

## Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound
**URL:** https://arxiv.org/abs/2506.23721

**Abstract:** Ultrasound (US) is widely accessible and radiation-free but has a steep learning curve due to its dynamic nature and non-standard imaging planes. Additionally, the constant need to shift focus between the US screen and the patient poses a challenge. To address these issues, we integrate deep learning (DL)-based semantic segmentation for real-time (RT) automated kidney volumetric measurements, which are essential for clinical assessment but are traditionally time-consuming and prone to fatigue. This automation allows clinicians to concentrate on image interpretation rather than manual measurements. Complementing DL, augmented reality (AR) enhances the usability of US by projecting the display directly into the clinician's field of view, improving ergonomics and reducing the cognitive load associated with screen-to-patient transitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one streams directly via the application programming interface for a wireless setup, while the other supports any US device with video output for broader accessibility. We evaluate RT feasibility and accuracy using the Open Kidney Dataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with MedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model implementations, measurement algorithms, and a Wi-Fi-based streaming solution, enhancing US training and diagnostics, especially in point-of-care settings.

**AI Summary:** This research introduces a deep learning-based semantic segmentation approach for real-time kidney imaging and measurements using ultrasound, addressing the challenges of manual measurements and the need to shift focus between the screen and the patient. The integration of augmented reality technology enhances the usability of ultrasound by projecting the display directly into the clinician's field of view, improving ergonomics and reducing cognitive load. The proposed pipelines show feasibility and accuracy in real-time measurements, providing a valuable tool for clinicians in point-of-care settings.

---

## Not quite a piece of CHERI-cake: Are new digital security by design architectures usable?
**URL:** https://arxiv.org/abs/2506.23682

**Abstract:** A digital security-by-design computer architecture, like CHERI, lets you program without fear of buffer overflows or other memory safety errors, but CHERI also rewrites some of the assumptions about how C works and how fundamental types (such as pointers) are implemented in hardware. We conducted a usability study to examine how developers react to the changes required by CHERI when porting software to run on it. We find that developers struggle with CHERI's display of warnings and errors and a lack of diverse documentation.

**AI Summary:** The research explores the usability of the CHERI digital security-by-design computer architecture, which aims to eliminate memory safety errors like buffer overflows. The study reveals that developers face challenges in adapting to CHERI's changes to C programming and hardware implementations of fundamental types like pointers, particularly due to issues with warnings, errors, and limited documentation. This highlights the importance of considering usability factors in the development and adoption of new security architectures.

---

## CooT: Learning to Coordinate In-Context with Coordination Transformers
**URL:** https://arxiv.org/abs/2506.23549

**Abstract:** Effective coordination among artificial agents in dynamic and uncertain environments remains a significant challenge in multi-agent systems. Existing approaches, such as self-play and population-based methods, either generalize poorly to unseen partners or require extensive training. To overcome these limitations, we propose Coordination Transformers (CooT), a novel in-context coordination framework that uses recent interaction histories to adapt to unseen partners rapidly. Unlike previous approaches that primarily aim to increase the diversity of training partners, CooT explicitly focuses on adapting to new partner behaviors by predicting actions aligned with observed partner interactions. Trained on interaction trajectories collected from diverse pairs of agents with complementary behaviors, CooT quickly learns effective coordination strategies without explicit supervision or fine-tuning. Evaluations on the Overcooked benchmark demonstrate that CooT significantly outperforms baseline methods in coordination tasks involving previously unseen partners. Human evaluations further confirm CooT as the most effective collaborative partner, while extensive ablations highlight its robustness, flexibility, and sensitivity to context in multi-agent scenarios.

**AI Summary:** The research introduces a new framework called Coordination Transformers (CooT) for improving coordination among artificial agents in dynamic environments. CooT adapts quickly to new partner behaviors by using recent interaction histories and predicting actions aligned with observed partner interactions. The evaluations show that CooT outperforms baseline methods in coordination tasks with unseen partners, demonstrating its effectiveness, flexibility, and sensitivity to context in multi-agent scenarios.

---

## Glyph-Based Multiscale Visualization of Turbulent Multi-Physics Statistics
**URL:** https://arxiv.org/abs/2506.23092

**Abstract:** Many scientific and engineering problems involving multi-physics span a wide range of scales. Understanding the interactions across these scales is essential for fully comprehending such complex problems. However, visualizing multivariate, multiscale data within an integrated view where correlations across space, scales, and fields are easily perceived remains challenging. To address this, we introduce a novel local spatial statistical visualization of flow fields across multiple fields and turbulence scales. Our method leverages the curvelet transform for scale decomposition of fields of interest, a level-set-restricted centroidal Voronoi tessellation to partition the spatial domain into local regions for statistical aggregation, and a set of glyph designs that combines information across scales and fields into a single, or reduced set of perceivable visual representations. Each glyph represents data aggregated within a Voronoi region and is positioned at the Voronoi site for direct visualization in a 3D view centered around flow features of interest. We implement and integrate our method into an interactive visualization system where the glyph-based technique operates in tandem with linked 3D spatial views and 2D statistical views, supporting a holistic analysis. We demonstrate with case studies visualizing turbulent combustion data--multi-scalar compressible flows--and turbulent incompressible channel flow data. This new capability enables scientists to better understand the interactions between multiple fields and length scales in turbulent flows.

**AI Summary:** This research introduces a novel method for visualizing multivariate, multiscale data in turbulent flows, allowing for the easy perception of correlations across space, scales, and fields. The method utilizes a combination of curvelet transform, centroidal Voronoi tessellation, and glyph designs to represent data aggregated within local regions. By implementing this method into an interactive visualization system, scientists can gain a better understanding of the interactions between multiple fields and length scales in turbulent flows.

---

## Agentic Enterprise: AI-Centric User to User-Centric AI
**URL:** https://arxiv.org/abs/2506.22893

**Abstract:** After a very long winter, the Artificial Intelligence (AI) spring is here. Or, so it seems over the last three years. AI has the potential to impact many areas of human life - personal, social, health, education, professional. In this paper, we take a closer look at the potential of AI for Enterprises, where decision-making plays a crucial and repeated role across functions, tasks, and operations. We consider Agents imbued with AI as means to increase decision-productivity of enterprises. We highlight six tenets for Agentic success in enterprises, by drawing attention to what the current, AI-Centric User paradigm misses, in the face of persistent needs of and usefulness for Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we offer six tenets and promote market mechanisms for platforms, aligning the design of AI and its delivery by Agents to the cause of enterprise users.

**AI Summary:** This research paper explores the potential of AI in enterprises, focusing on decision-making processes. The authors propose the concept of Agentic Enterprise, where AI-powered agents can enhance decision productivity. By shifting towards a User-Centric AI approach, the paper highlights six key principles for successful implementation in enterprise settings, emphasizing the importance of aligning AI design and delivery with the needs of users.

---

## Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding
**URL:** https://arxiv.org/abs/2506.22803

**Abstract:** Recent advances in deep learning have led to increasingly complex models with deeper layers and more parameters, reducing interpretability and making their decisions harder to understand. While many methods explain black-box reasoning, most lack effective interventions or only operate at sample-level without modifying the model itself. To address this, we propose the Concept Bottleneck Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU). CBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable framework to approximate black-box reasoning and communicate conceptual understanding. Detrimental concepts are automatically identified and refined (removed/replaced) based on global gradient contributions. The modified CBM then distills corrected knowledge back into the black-box model, enhancing both interpretability and accuracy. We evaluate CBM-HNMU on various CNN and transformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft, and CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum increase in average accuracy across 1.03%. Source code is available at: this https URL.

**AI Summary:** The research introduces the Concept Bottleneck Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU) to address the lack of effective interventions in explaining black-box reasoning in deep learning models. The CBM-HNMU leverages the Concept Bottleneck Model (CBM) to identify and refine detrimental concepts in the model, improving interpretability and accuracy. The proposed method achieved a maximum accuracy improvement of 2.64% and an increase in average accuracy across various CNN and transformer-based models, demonstrating its effectiveness in enhancing mutual understanding between humans and neural networks.

---

## Bootstrapping Human-Like Planning via LLMs
**URL:** https://arxiv.org/abs/2506.22604

**Abstract:** Robot end users increasingly require accessible means of specifying tasks for robots to perform. Two common end-user programming paradigms include drag-and-drop interfaces and natural language programming. Although natural language interfaces harness an intuitive form of human communication, drag-and-drop interfaces enable users to meticulously and precisely dictate the key actions of the robot's task. In this paper, we investigate the degree to which both approaches can be combined. Specifically, we construct a large language model (LLM)-based pipeline that accepts natural language as input and produces human-like action sequences as output, specified at a level of granularity that a human would produce. We then compare these generated action sequences to another dataset of hand-specified action sequences. Although our results reveal that larger models tend to outperform smaller ones in the production of human-like action sequences, smaller models nonetheless achieve satisfactory performance.

**AI Summary:** This research explores the combination of drag-and-drop interfaces and natural language programming to create a pipeline that generates human-like action sequences for robots. The study finds that larger language models tend to outperform smaller ones in producing these sequences, but even smaller models can achieve satisfactory performance. This work is significant as it addresses the need for accessible methods for end users to specify tasks for robots, bridging the gap between human communication and robot programming.

---

## An Interpretable Transformer-Based Foundation Model for Cross-Procedural Skill Assessment Using Raw fNIRS Signals
**URL:** https://arxiv.org/abs/2506.22476

**Abstract:** Objective skill assessment in high-stakes procedural environments requires models that not only decode underlying cognitive and motor processes but also generalize across tasks, individuals, and experimental contexts. While prior work has demonstrated the potential of functional near-infrared spectroscopy (fNIRS) for evaluating cognitive-motor performance, existing approaches are often task-specific, rely on extensive preprocessing, and lack robustness to new procedures or conditions. Here, we introduce an interpretable transformer-based foundation model trained on minimally processed fNIRS signals for cross-procedural skill assessment. Pretrained using self-supervised learning on data from laparoscopic surgical tasks and endotracheal intubation (ETI), the model achieves greater than 88% classification accuracy on all tasks, with Matthews Correlation Coefficient exceeding 0.91 on ETI. It generalizes to a novel emergency airway procedure--cricothyrotomy--using fewer than 30 labeled samples and a lightweight (less than 2k parameter) adapter module, attaining an AUC greater than 87%. Interpretability is achieved via a novel channel attention mechanism--developed specifically for fNIRS--that identifies functionally coherent prefrontal sub-networks validated through ablation studies. Temporal attention patterns align with task-critical phases and capture stress-induced changes in neural variability, offering insight into dynamic cognitive states.

**AI Summary:** This research introduces an interpretable transformer-based model trained on minimally processed fNIRS signals for skill assessment in procedural environments. The model achieves high classification accuracy across tasks, generalizes well to new procedures with few labeled samples, and includes a novel channel attention mechanism for interpretability. The temporal attention patterns captured by the model provide insight into cognitive states and stress-induced changes in neural variability during tasks.

---

## Golden Ratio Assisted Localization for Wireless Sensor Network
**URL:** https://arxiv.org/abs/2506.22464

**Abstract:** This paper presents a novel localization algorithm for wireless sensor networks (WSNs) called Golden Ratio Localization (GRL), which leverages the mathematical properties of the golden ratio (phi 1.618) to optimize both node placement and communication range. GRL introduces phi-based anchor node deployment and hop-sensitive weighting using phi-exponents to improve localization accuracy while minimizing energy consumption. Through extensive simulations conducted on a 100 m * 100 m sensor field with 100 nodes and 10 anchors, GRL achieved an average localization error of 2.35 meters, outperforming DV- Hop (3.87 meters) and Centroid (4.95 meters). In terms of energy efficiency, GRL reduced localization energy consumption to 1.12 microJ per node, compared to 1.78 microJ for DV-Hop and 1.45 microJ for Centroid. These results confirm that GRL provides a more balanced and efficient localization approach, making it especially suitable for energy-constrained and large-scale WSN deployments.

**AI Summary:** The paper introduces a new localization algorithm for wireless sensor networks called Golden Ratio Localization (GRL), which utilizes the golden ratio to optimize node placement and communication range. Through simulations, GRL was shown to achieve a lower localization error and consume less energy compared to existing methods like DV-Hop and Centroid, making it a more efficient and balanced approach for energy-constrained and large-scale WSN deployments.

---

## Privacy-aware IoT Fall Detection Services For Aging in Place
**URL:** https://arxiv.org/abs/2506.22462

**Abstract:** Fall detection is critical to support the growing elderly population, projected to reach 2.1 billion by 2050. However, existing methods often face data scarcity challenges or compromise privacy. We propose a novel IoT-based Fall Detection as a Service (FDaaS) framework to assist the elderly in living independently and safely by accurately detecting falls. We design a service-oriented architecture that leverages Ultra-wideband (UWB) radar sensors as an IoT health-sensing service, ensuring privacy and minimal intrusion. We address the challenges of data scarcity by utilizing a Fall Detection Generative Pre-trained Transformer (FD-GPT) that uses augmentation techniques. We developed a protocol to collect a comprehensive dataset of the elderly daily activities and fall events. This resulted in a real dataset that carefully mimics the elderly's routine. We rigorously evaluate and compare various models using this dataset. Experimental results show our approach achieves 90.72% accuracy and 89.33% precision in distinguishing between fall events and regular activities of daily living.

**AI Summary:** This research focuses on developing a privacy-aware IoT fall detection service for the elderly population, addressing data scarcity challenges and privacy concerns. The proposed IoT-based Fall Detection as a Service (FDaaS) framework utilizes Ultra-wideband radar sensors and a Fall Detection Generative Pre-trained Transformer (FD-GPT) to accurately detect falls while ensuring privacy and minimal intrusion. The study demonstrates a high accuracy rate of 90.72% in distinguishing between fall events and regular activities of daily living, highlighting the significance of this approach in supporting aging in place.

---

## Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition
**URL:** https://arxiv.org/abs/2506.22443

**Abstract:** Rule-based models offer interpretability but struggle with complex data, while deep neural networks excel in performance yet lack transparency. This work investigates a neuro-symbolic rule learning neural network named RL-Net that learns interpretable rule lists through neural optimization, applied for the first time to radar-based hand gesture recognition (HGR). We benchmark RL-Net against a fully transparent rule-based system (MIRA) and an explainable black-box model (XentricAI), evaluating accuracy, interpretability, and user adaptability via transfer learning. Our results show that RL-Net achieves a favorable trade-off, maintaining strong performance (93.03% F1) while significantly reducing rule complexity. We identify optimization challenges specific to rule pruning and hierarchy bias and propose stability-enhancing modifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical middle ground between transparency and performance. This study highlights the real-world feasibility of neuro-symbolic models for interpretable HGR and offers insights for extending explainable AI to edge-deployable sensing systems.

**AI Summary:** The research introduces RL-Net, a neuro-symbolic rule learning neural network that achieves a balance between interpretability and performance in radar-based hand gesture recognition. RL-Net outperforms a rule-based system and an explainable black-box model in terms of accuracy and interpretability, showing promise for real-world applications in edge-deployable sensing systems. The study also identifies optimization challenges and proposes modifications to enhance stability, emphasizing the potential of neuro-symbolic models in AI research.

---

## How to Evaluate the Accuracy of Online and AI-Based Symptom Checkers: A Standardized Methodological Framework
**URL:** https://arxiv.org/abs/2506.22379

**Abstract:** Online and AI-based symptom checkers are applications that assist medical laypeople in diagnosing their symptoms and determining which course of action to take. When evaluating these tools, previous studies primarily used an approach introduced a decade ago that lacked any type of quality control. Numerous studies have criticized this approach, and several empirical studies have sought to improve specific aspects of evaluations. However, even after a decade, a high-quality methodological framework for standardizing the evaluation of symptom checkers remains missing. This article synthesizes empirical studies to outline a framework for standardized evaluations based on representative case selection, an externally and internally valid evaluation design, and metrics that increase cross-study comparability. This approach is backed up by several open-access resources to facilitate implementation. Ultimately, this approach should enhance the quality and comparability of future evaluations of online and AI-based symptom checkers to enable meta-analyses and help stakeholders make more informed decisions.

**AI Summary:** This research article addresses the need for a standardized methodological framework to evaluate the accuracy of online and AI-based symptom checkers. Previous studies lacked quality control and criticized approaches used in evaluations. The proposed framework includes representative case selection, valid evaluation design, and metrics for cross-study comparability, aiming to improve the quality and comparability of future evaluations to aid stakeholders in making informed decisions.

---

## Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education
**URL:** https://arxiv.org/abs/2506.22231

**Abstract:** The rapid proliferation of generative artificial intelligence (AI) tools - especially large language models (LLMs) such as ChatGPT - has ushered in a transformative era in higher education. Universities in developed regions are increasingly integrating these technologies into research, teaching, and assessment. On one hand, LLMs can enhance productivity by streamlining literature reviews, facilitating idea generation, assisting with coding and data analysis, and even supporting grant proposal drafting. On the other hand, their use raises significant concerns regarding academic integrity, ethical boundaries, and equitable access. Recent empirical studies indicate that nearly 47% of students use LLMs in their coursework - with 39% using them for exam questions and 7% for entire assignments - while detection tools currently achieve around 88% accuracy, leaving a 12% error margin. This article critically examines the opportunities offered by generative AI, explores the multifaceted challenges it poses, and outlines robust policy solutions. Emphasis is placed on redesigning assessments to be AI-resilient, enhancing staff and student training, implementing multi-layered enforcement mechanisms, and defining acceptable use. By synthesizing data from recent research and case studies, the article argues that proactive policy adaptation is imperative to harness AI's potential while safeguarding the core values of academic integrity and equity.

**AI Summary:** The use of generative artificial intelligence tools like large language models (LLMs) in higher education presents opportunities for enhancing productivity but also raises concerns about academic integrity and equitable access. Research shows a significant percentage of students are using LLMs for coursework, with detection tools having a 12% error margin. The article emphasizes the need for universities to adapt policies to ensure AI-resilient assessments, provide training for staff and students, implement enforcement mechanisms, and define acceptable use to maintain academic integrity and equity.

---

## NoticeLight: Embracing Socio-Technical Asymmetry through Tangible Peripheral Robotic Embodiment in Hybrid Collaboration
**URL:** https://arxiv.org/abs/2506.22125

**Abstract:** Hybrid collaboration has become a fixture in modern workplaces, yet it introduces persistent socio-technical asymmetries-especially disadvantaging remote participants, who struggle with presence disparity, reduced visibility, and limited non-verbal communication. Traditional solutions often seek to erase these asymmetries, but recent research suggests embracing them as productive design constraints. In this context, we introduce NoticeLight: a tangible, peripheral robotic embodiment designed to augment hybrid meetings. NoticeLight transforms remote participants' digital presence into ambient, physical signals -- such as mood dynamics, verbal contribution mosaics, and attention cues -- within the co-located space. By abstracting group states into subtle light patterns, NoticeLight fosters peripheral awareness and balanced participation without disrupting meeting flow or demanding cognitive overload. This approach aligns with emerging perspectives in human-robot synergy, positioning robots as mediators that reshape, rather than replicate, human presence. Our work thereby advances the discourse on how robotic embodiments can empower equitable, dynamic collaboration in the workplace.

**AI Summary:** The research introduces NoticeLight, a tangible, peripheral robotic embodiment designed to address socio-technical asymmetries in hybrid collaboration by transforming remote participants' digital presence into ambient, physical signals within the co-located space. By abstracting group states into subtle light patterns, NoticeLight fosters peripheral awareness and balanced participation without disrupting meeting flow. This approach aligns with emerging perspectives in human-robot synergy, positioning robots as mediators that reshape human presence and empower equitable, dynamic collaboration in the workplace.

---

