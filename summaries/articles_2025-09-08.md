# arXiv cs.AI Summary â€“ 2025-09-08

## Conversational AI increases political knowledge as effectively as self-directed internet search
**URL:** https://arxiv.org/abs/2509.05219

**Abstract:** Conversational AI systems are increasingly being used in place of traditional search engines to help users complete information-seeking tasks. This has raised concerns in the political domain, where biased or hallucinated outputs could misinform voters or distort public opinion. However, in spite of these concerns, the extent to which conversational AI is used for political information-seeking, as well the potential impact of this use on users' political knowledge, remains uncertain. Here, we address these questions: First, in a representative national survey of the UK public (N = 2,499), we find that in the week before the 2024 election as many as 32% of chatbot users - and 13% of eligible UK voters - have used conversational AI to seek political information relevant to their electoral choice. Second, in a series of randomised controlled trials (N = 2,858 total) we find that across issues, models, and prompting strategies, conversations with AI increase political knowledge (increase belief in true information and decrease belief in misinformation) to the same extent as self-directed internet search. Taken together, our results suggest that although people in the UK are increasingly turning to conversational AI for information about politics, this shift may not lead to increased public belief in political misinformation.

**AI Summary:** This research investigates the use of conversational AI for political information-seeking and its impact on users' political knowledge. The study found that a significant portion of the UK public uses conversational AI for political information, and that conversations with AI increase political knowledge as effectively as self-directed internet search. These findings suggest that while conversational AI is being increasingly utilized for political information, it may not necessarily lead to increased belief in political misinformation.

---

## Transition of car-based human-mobility in the pandemic era: Data insight from a cross-border region in Europe
**URL:** https://arxiv.org/abs/2509.05166

**Abstract:** Many transport authorities are collecting and publishing almost real-time road traffic data to meet the growing trend of massive open data, a vital resource for foresight decision support systems considering deep data insights. Using such a traffic count dataset, we explored the spatio-temporal transitions in the cross-country road traffic volumes in the context of modelling behavioural transitions in car-based human mobility. We developed a reproducible workflow for computing multi-dimensional variables of traffic flow. This study reports on individual car-based daily travel behaviour detected, before (2016-2018) and during the COVID pandemic (2019-2021), between Germany and neighbouring countries (Luxembourg, France and Belgium). In relevance to the net-zero carbon transition, further study should shed light on the interpolation and downscaling approaches at the comprehensive road-network level for identifying pollution hot spots, causal link to functional landuse patterns and calculation of spatial influence area. In the case of Luxembourg, the Bridges and Roads Authority has installed a large digital traffic observatory infrastructure through the adoption of sensor-based IoT technologies, like other European member states. Since 2016, they have provided high-performance data processing and published open data on the country's road traffic. The dataset contains an hourly traffic count for different vehicle types, daily for representative observation points, followed by a major road network. The original dataset contains significant missing entries, so comprehensive data harmonization was performed.

**AI Summary:** This research study examines the impact of the COVID-19 pandemic on car-based human mobility in a cross-border region in Europe using real-time road traffic data. The study highlights the importance of data insights for decision support systems and proposes a reproducible workflow for analyzing traffic flow variables. The findings suggest the need for further research on identifying pollution hot spots and understanding the relationship between traffic patterns and land use, emphasizing the potential for data-driven approaches to address transportation challenges.

---

## Exploring Situated Stabilities of a Rhythm Generation System through Variational Cross-Examination
**URL:** https://arxiv.org/abs/2509.05145

**Abstract:** This paper investigates GrooveTransformer, a real-time rhythm generation system, through the postphenomenological framework of Variational Cross-Examination (VCE). By reflecting on its deployment across three distinct artistic contexts, we identify three stabilities: an autonomous drum accompaniment generator, a rhythmic control voltage sequencer in Eurorack format, and a rhythm driver for a harmonic accompaniment system. The versatility of its applications was not an explicit goal from the outset of the project. Thus, we ask: how did this multistability emerge? Through VCE, we identify three key contributors to its emergence: the affordances of system invariants, the interdisciplinary collaboration, and the situated nature of its development. We conclude by reflecting on the viability of VCE as a descriptive and analytical method for Digital Musical Instrument (DMI) design, emphasizing its value in uncovering how technologies mediate, co-shape, and are co-shaped by users and contexts.

**AI Summary:** This research explores the versatility and stability of the GrooveTransformer rhythm generation system across different artistic contexts using the Variational Cross-Examination (VCE) framework. The study identifies three key stabilities of the system: as a drum accompaniment generator, a rhythmic control voltage sequencer, and a rhythm driver for a harmonic accompaniment system. The emergence of these multiple stabilities is attributed to system invariants, interdisciplinary collaboration, and the situated nature of its development, highlighting the importance of understanding how technologies interact with users and contexts in Digital Musical Instrument (DMI) design.

---

## Long-Term Experiences From Working with Extended Reality in the Wild
**URL:** https://arxiv.org/abs/2509.05067

**Abstract:** Extended Reality (XR) is increasingly used as a productivity tool and recent commercial XR devices have even been specifically designed as productivity tools, or, at least, are heavily advertised for such purposes, such as the Apple Vision Pro (AVP), which has now been available for more than one year. In spite of what marketing suggests, research still lacks an understanding of the long-term usage of such devices in ecologically valid everyday settings, as most studies are conducted in very controlled environments. Therefore, we conducted interviews with ten AVP users to better understand how experienced users engage with the device, and which limitations persist. Our participants report that XR can increase productivity and that they got used to the device after some time. Yet, a range of limitations persist that might hinder the widespread use of XR as a productivity tool, such as a lack of native applications, difficulties when integrating XR into current workflows, and limited possibilities to adapt and customize the XR experience.

**AI Summary:** This research explores the long-term experiences of users working with the Apple Vision Pro (AVP) extended reality device in everyday settings. Despite the potential for increased productivity, users reported limitations such as a lack of native applications, difficulties integrating XR into current workflows, and limited customization options. These findings suggest that while XR can enhance productivity, there are still barriers that need to be addressed for widespread adoption as a productivity tool.

---

## Evaluating Idle Animation Believability: a User Perspective
**URL:** https://arxiv.org/abs/2509.05023

**Abstract:** Animating realistic avatars requires using high quality animations for every possible state the avatar can be in. This includes actions like walking or running, but also subtle movements that convey emotions and personality. Idle animations, such as standing, breathing or looking around, are crucial for realism and believability. In games and virtual applications, these are often handcrafted or recorded with actors, but this is costly. Furthermore, recording realistic idle animations can be very complex, because the actor must not know they are being recorded in order to make genuine movements. For this reasons idle animation datasets are not widely available. Nevertheless, this paper concludes that both acted and genuine idle animations are perceived as real, and that users are not able to distinguish between them. It also states that handmade and recorded idle animations are perceived differently. These two conclusions mean that recording idle animations should be easier than it is thought to be, meaning that actors can be specifically told to act the movements, significantly simplifying the recording process. These conclusions should help future efforts to record idle animation datasets. Finally, we also publish ReActIdle, a 3 dimensional idle animation dataset containing both real and acted idle motions.

**AI Summary:** This research evaluates the believability of idle animations in avatars, finding that both acted and genuine idle animations are perceived as real by users. The study also concludes that handmade and recorded idle animations are perceived differently. These findings suggest that recording idle animations may be easier than previously thought, and the researchers have published a new dataset, ReActIdle, containing both real and acted idle motions, to aid future efforts in this area.

---

## SePA: A Search-enhanced Predictive Agent for Personalized Health Coaching
**URL:** https://arxiv.org/abs/2509.04752

**Abstract:** This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM health coaching system that integrates personalized machine learning and retrieval-augmented generation to deliver adaptive, evidence-based guidance. SePA combines: (1) Individualized models predicting daily stress, soreness, and injury risk from wearable sensor data (28 users, 1260 data points); and (2) A retrieval module that grounds LLM-generated feedback in expert-vetted web content to ensure contextual relevance and reliability. Our predictive models, evaluated with rolling-origin cross-validation and group k-fold cross-validation show that personalized models outperform generalized baselines. In a pilot expert study (n=4), SePA's retrieval-based advice was preferred over a non-retrieval baseline, yielding meaningful practical effect (Cliff's $\delta$=0.3, p=0.05). We also quantify latency performance trade-offs between response quality and speed, offering a transparent blueprint for next-generation, trustworthy personal health informatics systems.

**AI Summary:** The research introduces SePA, a personalized health coaching system that combines machine learning and retrieval-augmented generation to provide adaptive guidance. The study shows that personalized models outperform generalized baselines in predicting daily stress, soreness, and injury risk. Additionally, a pilot expert study indicates that SePA's retrieval-based advice is preferred over a non-retrieval baseline, highlighting the system's potential for improving personalized health informatics systems.

---

## Transforming Fashion with AI: A Comparative Study of Large Language Models in Apparel Design
**URL:** https://arxiv.org/abs/2509.04705

**Abstract:** Fashion has evolved from handcrafted designs to automated production over the years, where AI has added another dimension to it. Nowadays, practically every industry uses artificial models to automate their operations. To explore their role, we examined three prominent LLMs (OpenAI, GeminiAI, Deepseek) in multiple stages of textile manufacturing (e.g., sustainable choice, cost effectiveness, production planning, etc.). We assessed the models' ability to replicate garment design using certain parameters (fabric construction, shade, weave, silhouette, etc.). We compared the models in terms of different body types and functional purposes (e.g., fashionwear, sportswear) so that designers could evaluate effectiveness before developing actual patterns, make necessary modifications, and conduct fashion forecasting beforehand. To facilitate deeper analysis, we created a custom dataset specifically for fabric image generation and classification. Our analysis revealed that, in terms of fabric construction, the OpenAI DALL-E model integrated with ChatGPT outperformed other models, achieving a lower LPIPS (Learned Perceptual Image Patch Similarity) score of approximately $0.2$. In fabric classification from images, we found OpenAI offered the best results by breaking down certain factors (e.g., breathability, moisture-wicking, and tactile comfort), achieving approximately $80\%$ accuracy for base construction and $55\%$ for detailed construction. However, our results indicate that Deepseek faced significant challenges in generating and recognizing fabric images. Overall, all the models struggled to recognize complex fabric constructions and intricate designs from images, and relying too much on AI might hinder human creativity. We also observed that all three models performed effectively in providing recommendations and insights for fabric design in textual form.

**AI Summary:** This research study compares three large language models (LLMs) in the context of apparel design, specifically focusing on fabric construction and classification. The study found that the OpenAI DALL-E model integrated with ChatGPT performed the best in fabric construction, while OpenAI also excelled in fabric classification. However, all models struggled with recognizing complex fabric constructions and intricate designs from images, highlighting the importance of balancing AI assistance with human creativity in the fashion industry.

---

## Computational Cognitive Modeling to understand the effects of Racializing AI on Human-AI cooperation with PigChase Task
**URL:** https://arxiv.org/abs/2509.04636

**Abstract:** Despite the continued anthropomorphization of AI systems, the potential impact of racialization during human-AI interaction is understudied. This study explores how human-AI cooperation may be impacted by the belief that data used to train an AI system is racialized, that is, it was trained on data from a specific group of people. During this study, participants completed a human-AI cooperation task using the Pig Chase game. Participants of different self-identified demographics interacted with AI agents whose perceived racial identities were manipulated, allowing us to assess how sociocultural perspectives influence the decision-making of participants in the game. After the game, participants completed a survey questionnaire to explain the strategies they used while playing the game and to understand the perceived intelligence of their AI teammates. Statistical analysis of task behavior data revealed a statistically significant effect of the participant's demographic, as well as the interaction between this self-identified demographic and the treatment condition (i.e., the perceived demographic of the agent). The results indicated that Non-White participants viewed AI agents racialized as White in a positive way compared to AI agents racialized as Black. Both Black and White participants viewed the AI agent in the control treatment in a negative way. A baseline cognitive model of the task using ACT-R cognitive architecture was used to understand a cognitive-level, process-based explanation of the participants' perspectives based on results found from the study. This model helps us better understand the factors affecting the decision-making strategies of the game participants. Results from analysis of these data, as well as cognitive modeling, indicate a need to expand understanding of the ways racialization (whether implicit or explicit) impacts interaction with AI systems.

**AI Summary:** This study investigates the impact of racializing AI on human-AI cooperation using the Pig Chase game. Participants of different demographics interacted with AI agents whose perceived racial identities were manipulated, revealing that Non-White participants viewed AI agents racialized as White more positively than those racialized as Black. The study highlights the importance of considering sociocultural perspectives and the effects of racialization on decision-making strategies in human-AI interaction.

---

## AI Agents for Web Testing: A Case Study in the Wild
**URL:** https://arxiv.org/abs/2509.05197

**Abstract:** Automated web testing plays a critical role in ensuring high-quality user experiences and delivering business value. Traditional approaches primarily focus on code coverage and load testing, but often fall short of capturing complex user behaviors, leaving many usability issues undetected. The emergence of large language models (LLM) and AI agents opens new possibilities for web testing by enabling human-like interaction with websites and a general awareness of common usability problems. In this work, we present WebProber, a prototype AI agent-based web testing framework. Given a URL, WebProber autonomously explores the website, simulating real user interactions, identifying bugs and usability issues, and producing a human-readable report. We evaluate WebProber through a case study of 120 academic personal websites, where it uncovered 29 usability issues--many of which were missed by traditional tools. Our findings highlight agent-based testing as a promising direction while outlining directions for developing next-generation, user-centered testing frameworks.

**AI Summary:** The research explores the use of AI agents in web testing to improve the detection of usability issues on websites. The study introduces WebProber, a prototype AI agent-based web testing framework that autonomously explores websites, simulates user interactions, and identifies bugs and usability issues. Through a case study of 120 academic personal websites, WebProber uncovered 29 usability issues that traditional tools had missed, demonstrating the potential of agent-based testing in improving user-centered testing frameworks.

---

## SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing
**URL:** https://arxiv.org/abs/2509.04908

**Abstract:** The existing Multimodal Large Language Models (MLLMs) for GUI perception have made great progress. However, the following challenges still exist in prior methods: 1) They model discrete coordinates based on text autoregressive mechanism, which results in lower grounding accuracy and slower inference speed. 2) They can only locate predefined sets of elements and are not capable of parsing the entire interface, which hampers the broad application and support for downstream tasks. To address the above issues, we propose SparkUI-Parser, a novel end-to-end framework where higher localization precision and fine-grained parsing capability of the entire interface are simultaneously achieved. Specifically, instead of using probability-based discrete modeling, we perform continuous modeling of coordinates based on a pre-trained Multimodal Large Language Model (MLLM) with an additional token router and coordinate decoder. This effectively mitigates the limitations inherent in the discrete output characteristics and the token-by-token generation process of MLLMs, consequently boosting both the accuracy and the inference speed. To further enhance robustness, a rejection mechanism based on a modified Hungarian matching algorithm is introduced, which empowers the model to identify and reject non-existent elements, thereby reducing false positives. Moreover, we present ScreenParse, a rigorously constructed benchmark to systematically assess structural perception capabilities of GUI models across diverse scenarios. Extensive experiments demonstrate that our approach consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2, CAGUI-Grounding and ScreenParse benchmarks. The resources are available at this https URL.

**AI Summary:** The research introduces SparkUI-Parser, a novel framework that improves GUI perception by enhancing localization precision and parsing capabilities of the entire interface. By using continuous modeling of coordinates based on a pre-trained Multimodal Large Language Model (MLLM) with additional components, the framework achieves higher accuracy and faster inference speed compared to existing methods. The introduction of a rejection mechanism based on a modified Hungarian matching algorithm also enhances robustness by identifying and rejecting non-existent elements, reducing false positives. Extensive experiments show that SparkUI-Parser outperforms state-of-the-art methods on various benchmarks, demonstrating its effectiveness in improving GUI perception.

---

## SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models
**URL:** https://arxiv.org/abs/2509.04889

**Abstract:** Advances in computer vision have opened new avenues for clinical applications, particularly in computerized exposure therapy where visual stimuli can be dynamically adjusted based on patient responses. As a critical step toward such adaptive systems, we investigated whether pretrained computer vision models can accurately predict fear levels from spider-related images. We adapted three diverse models using transfer learning to predict human fear ratings (on a 0-100 scale) from a standardized dataset of 313 images. The models were evaluated using cross-validation, achieving an average mean absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis revealed that reducing the dataset size significantly harmed performance, though further increases yielded no substantial gains. Explainability assessments showed the models' predictions were based on spider-related features. A category-wise error analysis further identified visual conditions associated with higher errors (e.g., distant views and artificial/painted spiders). These findings demonstrate the potential of explainable computer vision models in predicting fear ratings, highlighting the importance of both model explainability and a sufficient dataset size for developing effective emotion-aware therapeutic technologies.

**AI Summary:** This research explores the use of pretrained computer vision models to accurately predict fear levels from spider-related images, which is crucial for developing adaptive systems in computerized exposure therapy. The study found that the models achieved an average mean absolute error between 10.1 and 11.0 when predicting human fear ratings on a 0-100 scale. The findings emphasize the potential of explainable computer vision models in predicting fear ratings and highlight the importance of model explainability and dataset size for developing effective emotion-aware therapeutic technologies.

---

## TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models
**URL:** https://arxiv.org/abs/2509.04809

**Abstract:** Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents (Coordinator, Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agent's actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and coder-debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agent's actions and contextualized their meaning within the problem domain.

**AI Summary:** The research introduces TalkToAgent, a framework utilizing Large Language Models to provide natural language explanations for Reinforcement Learning policies. The framework includes five specialized agents to help users interact with and understand RL agents more effectively. The validation on a control benchmark showed that TalkToAgent accurately mapped user queries to XRL tasks and effectively interpreted agent actions within the problem domain.

---

## An Approach to Grounding AI Model Evaluations in Human-derived Criteria
**URL:** https://arxiv.org/abs/2509.04676

**Abstract:** In the rapidly evolving field of artificial intelligence (AI), traditional benchmarks can fall short in attempting to capture the nuanced capabilities of AI models. We focus on the case of physical world modeling and propose a novel approach to augment existing benchmarks with human-derived evaluation criteria, aiming to enhance the interpretability and applicability of model behaviors. Grounding our study in the Perception Test and OpenEQA benchmarks, we conducted in-depth interviews and large-scale surveys to identify key cognitive skills, such as Prioritization, Memorizing, Discerning, and Contextualizing, that are critical for both AI and human reasoning. Our findings reveal that participants perceive AI as lacking in interpretive and empathetic skills yet hold high expectations for AI performance. By integrating insights from our findings into benchmark design, we offer a framework for developing more human-aligned means of defining and measuring progress. This work underscores the importance of user-centered evaluation in AI development, providing actionable guidelines for researchers and practitioners aiming to align AI capabilities with human cognitive processes. Our approach both enhances current benchmarking practices and sets the stage for future advancements in AI model evaluation.

**AI Summary:** This research explores the limitations of traditional benchmarks in evaluating AI models and proposes a novel approach to incorporate human-derived evaluation criteria, specifically focusing on physical world modeling. Through interviews and surveys, key cognitive skills critical for both AI and human reasoning were identified, highlighting the need for interpretive and empathetic skills in AI models. By integrating these insights into benchmark design, the study offers a framework for developing more human-aligned means of measuring AI progress, emphasizing the importance of user-centered evaluation in AI development.

---

## Combine Virtual Reality and Machine-Learning to Identify the Presence of Dyslexia: A Cross-Linguistic Approach
**URL:** https://arxiv.org/abs/2509.04510

**Abstract:** This study explores the use of virtual reality (VR) and artificial intelligence (AI) to predict the presence of dyslexia in Italian and Spanish university students. In particular, the research investigates whether VR-derived data from Silent Reading (SR) tests and self-esteem assessments can differentiate between students that are affected by dyslexia and students that are not, employing machine learning (ML) algorithms. Participants completed VR-based tasks measuring reading performance and self-esteem. A preliminary statistical analysis (t tests and Mann Whitney tests) on these data was performed, to compare the obtained scores between individuals with and without dyslexia, revealing significant differences in completion time for the SR test, but not in accuracy, nor in self esteem. Then, supervised ML models were trained and tested, demonstrating an ability to classify the presence/absence of dyslexia with an accuracy of 87.5 per cent for Italian, 66.6 per cent for Spanish, and 75.0 per cent for the pooled group. These findings suggest that VR and ML can effectively be used as supporting tools for assessing dyslexia, particularly by capturing differences in task completion speed, but language-specific factors may influence classification accuracy.

**AI Summary:** This study investigates the use of virtual reality and machine learning to identify dyslexia in Italian and Spanish university students. The research shows that VR-based tasks can differentiate between students with dyslexia and those without, particularly in terms of completion time for silent reading tests. Machine learning models were able to classify the presence of dyslexia with high accuracy, indicating that VR and ML could be effective tools for assessing dyslexia, although language-specific factors may impact classification accuracy.

---

## Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the Roles of Information Transparency, User Control, and Proactivity
**URL:** https://arxiv.org/abs/2509.04358

**Abstract:** Social robots are increasingly recognized as valuable supporters in the field of well-being coaching. They can function as independent coaches or provide support alongside human coaches, and healthcare professionals. In coaching interactions, these robots often handle sensitive information shared by users, making privacy a relevant issue. Despite this, little is known about the factors that shape users' privacy perceptions. This research aims to examine three key factors systematically: (1) the transparency about information usage, (2) the level of specific user control over how the robot uses their information, and (3) the robot's behavioral approach - whether it acts proactively or only responds on demand. Our results from an online study (N = 200) show that even when users grant the robot general access to personal data, they additionally expect the ability to explicitly control how that information is interpreted and shared during sessions. Experimental conditions that provided such control received significantly higher ratings for perceived privacy appropriateness and trust. Compared to user control, the effects of transparency and proactivity on privacy appropriateness perception were low, and we found no significant impact. The results suggest that merely informing users or proactive sharing is insufficient without accompanying user control. These insights underscore the need for further research on mechanisms that allow users to manage robots' information processing and sharing, especially when social robots take on more proactive roles alongside humans.

**AI Summary:** This research examines the factors that shape users' privacy perceptions in robot-assisted well-being coaching, specifically focusing on information transparency, user control, and proactivity. The study found that users expect the ability to control how their personal information is interpreted and shared during coaching sessions, even when granting general access to the robot. Providing users with explicit control over their information led to higher ratings for perceived privacy appropriateness and trust, highlighting the importance of user control in managing robots' information processing and sharing in coaching interactions.

---

## SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic Avatars
**URL:** https://arxiv.org/abs/2509.04356

**Abstract:** We present SRWToolkit, an open-source Wizard of Oz toolkit designed to facilitate the rapid prototyping of social robotic avatars powered by local large language models (LLMs). Our web-based toolkit enables multimodal interaction through text input, button-activated speech, and wake-word command. The toolkit offers real-time configuration of avatar appearance, behavior, language, and voice via an intuitive control panel. In contrast to prior works that rely on cloud-based LLM services, SRWToolkit emphasizes modularity and ensures on-device functionality through local LLM inference. In our small-scale user study ($n=11$), participants created and interacted with diverse robotic roles (hospital receptionist, mathematics teacher, and driving assistant), which demonstrated positive outcomes in the toolkit's usability, trust, and user experience. The toolkit enables rapid and efficient development of robot characters customized to researchers' needs, supporting scalable research in human-robot interaction.

**AI Summary:** The SRWToolkit is an open-source Wizard of Oz toolkit that allows for the creation of social robotic avatars powered by local large language models. It enables multimodal interaction and real-time configuration of avatar appearance, behavior, language, and voice through an intuitive control panel. A small-scale user study showed positive outcomes in usability, trust, and user experience, highlighting the toolkit's potential for rapid and efficient development of customized robot characters for research in human-robot interaction.

---

## Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing Clinical Notes
**URL:** https://arxiv.org/abs/2509.04340

**Abstract:** Large Language Models (LLMs) are often proposed as tools to streamline clinical documentation, a task viewed as both high-volume and low-risk. However, even seemingly straightforward applications of LLMs raise complex sociotechnical considerations to translate into practice. This case study, conducted at KidsAbility, a pediatric rehabilitation facility in Ontario, Canada examined the use of LLMs to support occupational therapists in reducing documentation this http URL conducted a qualitative study involving 20 clinicians who participated in pilot programs using two AI technologies: a general-purpose proprietary LLM and a bespoke model fine-tuned on proprietary historical documentation.
Our findings reveal that documentation challenges are sociotechnical in nature, shaped by clinical workflows, organizational policies, and system constraints. Four key themes emerged: (1) the heterogeneity of workflows, (2) the documentation burden is systemic and not directly linked to the creation of any single type of documentation, (3) the need for flexible tools and clinician autonomy, and (4) effective implementation requires mutual learning between clinicians and AI systems.
While LLMs show promise in easing documentation tasks, their success will depend on flexible, adaptive integration that supports clinician autonomy. Beyond technical performance, sustained adoption will require training programs and implementation strategies that reflect the complexity of clinical environments.

**AI Summary:** This research explores the challenges faced by occupational therapists in using Large Language Models (LLMs) to streamline clinical documentation at a pediatric rehabilitation facility. The study highlights the sociotechnical nature of documentation challenges, emphasizing the importance of flexible tools, clinician autonomy, and mutual learning between clinicians and AI systems for successful implementation. The findings suggest that while LLMs have the potential to ease documentation tasks, their effectiveness in clinical settings will depend on adaptive integration and comprehensive training programs.

---

## HumAIne-Chatbot: Real-Time Personalized Conversational AI via Reinforcement Learning
**URL:** https://arxiv.org/abs/2509.04303

**Abstract:** Current conversational AI systems often provide generic, one-size-fits-all interactions that overlook individual user characteristics and lack adaptive dialogue management. To address this gap, we introduce \textbf{HumAIne-chatbot}, an AI-driven conversational agent that personalizes responses through a novel user profiling framework. The system is pre-trained on a diverse set of GPT-generated virtual personas to establish a broad prior over user types. During live interactions, an online reinforcement learning agent refines per-user models by combining implicit signals (e.g. typing speed, sentiment, engagement duration) with explicit feedback (e.g., likes and dislikes). This profile dynamically informs the chatbot dialogue policy, enabling real-time adaptation of both content and style. To evaluate the system, we performed controlled experiments with 50 synthetic personas in multiple conversation domains. The results showed consistent improvements in user satisfaction, personalization accuracy, and task achievement when personalization features were enabled. Statistical analysis confirmed significant differences between personalized and nonpersonalized conditions, with large effect sizes across key metrics. These findings highlight the effectiveness of AI-driven user profiling and provide a strong foundation for future real-world validation.

**AI Summary:** The research introduces HumAIne-chatbot, a conversational AI system that personalizes responses based on user characteristics through a novel user profiling framework. The system uses a combination of pre-training on virtual personas and online reinforcement learning to adapt its dialogue policy in real-time, leading to improved user satisfaction, personalization accuracy, and task achievement. The results of controlled experiments with 50 synthetic personas in various conversation domains demonstrated significant improvements in personalized interactions, emphasizing the effectiveness of AI-driven user profiling for enhancing conversational AI systems.

---

## MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition from Physiological Signals
**URL:** https://arxiv.org/abs/2509.04254

**Abstract:** We present MuMTAffect, a novel Multimodal Multitask Affective Embedding Network designed for joint emotion classification and personality prediction (re-identification) from short physiological signal segments. MuMTAffect integrates multiple physiological modalities pupil dilation, eye gaze, facial action units, and galvanic skin response using dedicated, transformer-based encoders for each modality and a fusion transformer to model cross-modal interactions. Inspired by the Theory of Constructed Emotion, the architecture explicitly separates core affect encoding (valence/arousal) from higher-level conceptualization, thereby grounding predictions in contemporary affective neuroscience. Personality trait prediction is leveraged as an auxiliary task to generate robust, user-specific affective embeddings, significantly enhancing emotion recognition performance. We evaluate MuMTAffect on the AFFEC dataset, demonstrating that stimulus-level emotional cues (Stim Emo) and galvanic skin response substantially improve arousal classification, while pupil and gaze data enhance valence discrimination. The inherent modularity of MuMTAffect allows effortless integration of additional modalities, ensuring scalability and adaptability. Extensive experiments and ablation studies underscore the efficacy of our multimodal multitask approach in creating personalized, context-aware affective computing systems, highlighting pathways for further advancements in cross-subject generalisation.

**AI Summary:** MuMTAffect is a new framework for emotion recognition and personality prediction from physiological signals. It integrates multiple modalities and uses a transformer-based architecture to improve emotion classification performance. By separating core affect encoding from higher-level conceptualization, MuMTAffect generates user-specific affective embeddings, enhancing emotion recognition accuracy. The framework's modularity allows for easy integration of additional modalities, making it scalable and adaptable for creating personalized affective computing systems.

---

## Would I regret being different? The influence of social norms on attitudes toward AI usage
**URL:** https://arxiv.org/abs/2509.04241

**Abstract:** Prior research shows that social norms can reduce algorithm aversion, but little is known about how such norms become established. Most accounts emphasize technological and individual determinants, yet AI adoption unfolds within organizational social contexts shaped by peers and supervisors. We ask whether the source of the norm-peers or supervisors-shapes AI usage behavior. This question is practically relevant for organizations seeking to promote effective AI adoption. We conducted an online vignette experiment, complemented by qualitative data on participants' feelings and justifications after (counter-)normative behavior. In line with the theory, counter-normative choices elicited higher regret than norm-adherent choices. On average, choosing AI increased regret compared to choosing an human. This aversion was weaker when AI use was presented as the prevailing norm, indicating a statistically significant interaction between AI use and an AI-favoring norm. Participants also attributed less blame to technology than to humans, which increased regret when AI was chosen over human expertise. Both peer and supervisor influence emerged as relevant factors, though contrary to expectations they did not significantly affect regret. Our findings suggest that regret aversion, embedded in social norms, is a central mechanism driving imitation in AI-related decision-making.

**AI Summary:** This research explores the impact of social norms on attitudes towards AI usage within organizational contexts. The study found that counter-normative choices regarding AI usage elicited higher levels of regret compared to norm-adherent choices. Participants were more likely to choose AI over human expertise when it was presented as the prevailing norm, indicating the influence of social norms on decision-making in AI adoption within organizations. The study highlights the importance of understanding and leveraging social norms to promote effective AI adoption in organizational settings.

---

## Unobtrusive In-Situ Measurement of Behavior Change by Deep Metric Similarity Learning of Motion Patterns
**URL:** https://arxiv.org/abs/2509.04174

**Abstract:** This paper introduces an unobtrusive in-situ measurement method to detect user behavior changes during arbitrary exposures in XR systems. Here, such behavior changes are typically associated with the Proteus effect or bodily affordances elicited by different avatars that the users embody in XR. We present a biometric user model based on deep metric similarity learning, which uses high-dimensional embeddings as reference vectors to identify behavior changes of individual users. We evaluate our model against two alternative approaches: a (non-learned) motion analysis based on central tendencies of movement patterns and subjective post-exposure embodiment questionnaires frequently used in various XR exposures. In a within-subject study, participants performed a fruit collection task while embodying avatars of different body heights (short, actual-height, and tall). Subjective assessments confirmed the effective manipulation of perceived body schema, while the (non-learned) objective analyses of head and hand movements revealed significant differences across conditions. Our similarity learning model trained on the motion data successfully identified the elicited behavior change for various query and reference data pairings of the avatar conditions. The approach has several advantages in comparison to existing methods: 1) In-situ measurement without additional user input, 2) generalizable and scalable motion analysis for various use cases, 3) user-specific analysis on the individual level, and 4) with a trained model, users can be added and evaluated in real time to study how avatar changes affect behavior.

**AI Summary:** This research introduces a method for measuring behavior changes in XR systems using deep metric similarity learning of motion patterns. The study found that the model was able to accurately detect behavior changes in users embodying different avatars without the need for additional user input. This approach has the potential to provide a more effective and scalable way to analyze behavior changes in XR systems compared to traditional methods.

---

## Spiking Neural Network Decoders of Finger Forces from High-Density Intramuscular Microelectrode Arrays
**URL:** https://arxiv.org/abs/2509.04088

**Abstract:** Restoring naturalistic finger control in assistive technologies requires the continuous decoding of motor intent with high accuracy, efficiency, and robustness. Here, we present a spike-based decoding framework that integrates spiking neural networks (SNNs) with motor unit activity extracted from high-density intramuscular microelectrode arrays. We demonstrate simultaneous and proportional decoding of individual finger forces from motor unit spike trains during isometric contractions at 15% of maximum voluntary contraction using SNNs. We systematically evaluated alternative SNN decoder configurations and compared two possible input modalities: physiologically grounded motor unit spike trains and spike-encoded intramuscular EMG signals. Through this comparison, we quantified trade-offs between decoding accuracy, memory footprint, and robustness to input errors. The results showed that shallow SNNs can reliably decode finger-level motor intent with competitive accuracy and minimal latency, while operating with reduced memory requirements and without the need for external preprocessing buffers. This work provides a practical blueprint for integrating SNNs into finger-level force decoding systems, demonstrating how the choice of input representation can be strategically tailored to meet application-specific requirements for accuracy, robustness, and memory efficiency.

**AI Summary:** This research presents a spike-based decoding framework using spiking neural networks to decode individual finger forces from motor unit spike trains. The study compares different SNN decoder configurations and input modalities, showing that shallow SNNs can decode finger-level motor intent with high accuracy and minimal latency while operating efficiently with reduced memory requirements. These findings provide a practical blueprint for integrating SNNs into assistive technologies for naturalistic finger control, offering insights into optimizing decoding systems for accuracy, robustness, and memory efficiency.

---

## "Low Frequency Tweeters Have More to Say!" A New Approach to Identify Importance of Tweets
**URL:** https://arxiv.org/abs/2509.03931

**Abstract:** Twitter is one of the most popular social media this http URL a large number of tweets, the activity feed of users becomes noisy, challenging to read, and most importantly tweets often get lost. We present a new approach to personalise the ranking of the tweets toward solving the problem of information overload which is achieved by analysing the relationship between the importance of tweets to the frequency at which the author tweets. The hypothesis tested is that "low-frequency tweeters have more to say", i.e. if a user who tweets infrequently actually goes to the effort of tweeting, then it is more likely to be of more importance or contain more "meaning" than a tweet by a user who tweets continuously. We propose six new measures to evaluate the importance of tweets based on the ability of the tweet to drive interaction among its readers, which is measured through metrics such as retweets, favourites, and comments, and the extent of the author's network interacting with the tweet. Our study shows that users who tweeted less than ten tweets per week were more likely to be perceived as important by their followers and have the most important messages. This identified tweet-frequency band could be used to reorder the activity feed of users and such reordering would ensure the messages of low-frequency tweeters do not get lost in the stream of tweets. This could also serve as a scoring index for Twitter users to identify users frequently tweeting important messages.

**AI Summary:** This research introduces a new approach to personalize the ranking of tweets on Twitter by analyzing the relationship between tweet importance and the frequency at which the author tweets. The study found that users who tweet less frequently, specifically less than ten tweets per week, are more likely to be perceived as important by their followers and have the most impactful messages. This identified tweet-frequency band could be used to reorder the activity feed of users and serve as a scoring index for Twitter users to identify users frequently tweeting important messages.

---

## Exploring the Integration of Extended Reality and Artificial Intelligence (AI) for Remote STEM Education and Assessment
**URL:** https://arxiv.org/abs/2509.03812

**Abstract:** This paper presents a dynamic gamification architecture for an Extended Reality Artificial Intelligence virtual training environment designed to enhance STEM education through immersive adaptive, and kinesthetic learning. The proposed system can be introduced in four phases: Introduction Phase, Component Development Phase, Fault Introduction and Correction Phase and Generative AI XR scenarios Phase. Security and privacy are discussed via a defense-in-depth approach spanning client, middleware, and backend layers, incorporating AES 256 encryption, multi-factor authentication, role-based access control and GDPR or FERPA compliance. Risks such as sensor exploitation, perceptual manipulation, and virtual physical harm are identified, with mitigation strategies embedded at the design stage. Potential barriers to large scale adoption-including technical complexity, cost of deployment, and need for cybersecurity expertise are discussed.

**AI Summary:** This research explores the integration of Extended Reality and Artificial Intelligence for remote STEM education and assessment, presenting a dynamic gamification architecture for immersive learning. The proposed system includes phases for introduction, development, fault correction, and generative AI scenarios, with a focus on security and privacy measures. Identified risks such as sensor exploitation and virtual physical harm are addressed with mitigation strategies, while potential barriers to adoption are discussed, highlighting the need for cybersecurity expertise and addressing technical complexity and deployment costs.

---

## Map as a By-product: Collective Landmark Mapping from IMU Data and User-provided Texts in Situated Tasks
**URL:** https://arxiv.org/abs/2509.03792

**Abstract:** This paper presents Collective Landmark Mapper, a novel map-as-a-by-product system for generating semantic landmark maps of indoor environments. Consider users engaged in situated tasks that require them to navigate these environments and regularly take notes on their smartphones. Collective Landmark Mapper exploits the smartphone's IMU data and the user's free text input during these tasks to identify a set of landmarks encountered by the user. The identified landmarks are then aggregated across multiple users to generate a unified map representing the positions and semantic information of all landmarks. In developing the proposed system, we focused specifically on retail applications and conducted a formative interview with stakeholders to confirm their practical needs that motivate the map-as-a-byproduct approach. Our user study demonstrates the feasibility of the proposed system and its superior mapping performance in two different setups: creating a product availability map from restocking checklist tasks at a retail store and constructing a room usage map from office inspection tasks, further demonstrating the potential applicability to non-retail applications.

**AI Summary:** This research introduces a map-as-a-by-product system called Collective Landmark Mapper that generates semantic landmark maps of indoor environments using IMU data and user-provided texts. The system identifies landmarks encountered by users during situated tasks and aggregates this information across multiple users to create a unified map. The study demonstrates the feasibility and superior mapping performance of the system in retail and non-retail applications, showing its potential for practical use in various scenarios.

---

## Designing Gaze Analytics for ELA Instruction: A User-Centered Dashboard with Conversational AI Support
**URL:** https://arxiv.org/abs/2509.03741

**Abstract:** Eye-tracking offers rich insights into student cognition and engagement, but remains underutilized in classroom-facing educational technology due to challenges in data interpretation and accessibility. In this paper, we present the iterative design and evaluation of a gaze-based learning analytics dashboard for English Language Arts (ELA), developed through five studies involving teachers and students. Guided by user-centered design and data storytelling principles, we explored how gaze data can support reflection, formative assessment, and instructional decision-making. Our findings demonstrate that gaze analytics can be approachable and pedagogically valuable when supported by familiar visualizations, layered explanations, and narrative scaffolds. We further show how a conversational agent, powered by a large language model (LLM), can lower cognitive barriers to interpreting gaze data by enabling natural language interactions with multimodal learning analytics. We conclude with design implications for future EdTech systems that aim to integrate novel data modalities in classroom contexts.

**AI Summary:** This research focuses on the design and evaluation of a gaze-based learning analytics dashboard for English Language Arts (ELA) education, developed through multiple studies involving teachers and students. The findings suggest that gaze analytics can be valuable for reflection, formative assessment, and instructional decision-making when presented in familiar visualizations with layered explanations and narrative scaffolds. Additionally, the study shows that incorporating a conversational agent powered by a large language model can help lower cognitive barriers to interpreting gaze data by enabling natural language interactions with multimodal learning analytics, providing insights for future educational technology systems.

---

## Designing Effective AI Explanations for Misinformation Detection: A Comparative Study of Content, Social, and Combined Explanations
**URL:** https://arxiv.org/abs/2509.03693

**Abstract:** In this paper, we study the problem of AI explanation of misinformation, where the goal is to identify explanation designs that help improve users' misinformation detection abilities and their overall user experiences. Our work is motivated by the limitations of current Explainable AI (XAI) approaches, which predominantly focus on content explanations that elucidate the linguistic features and sentence structures of the misinformation. To address this limitation, we explore various explanations beyond content explanation, such as "social explanation" that considers the broader social context surrounding misinformation, as well as a "combined explanation" where both the content and social explanations are presented in scenarios that are either aligned or misaligned with each other. To evaluate the comparative effectiveness of these AI explanations, we conduct two online crowdsourcing experiments in the COVID-19 (Study 1 on Prolific) and Politics domains (Study 2 on MTurk). Our results show that AI explanations are generally effective in aiding users to detect misinformation, with effectiveness significantly influenced by the alignment between content and social explanations. We also find that the order in which explanation types are presented - specifically, whether a content or social explanation comes first - can influence detection accuracy, with differences found between the COVID-19 and Political domains. This work contributes towards more effective design of AI explanations, fostering a deeper understanding of how different explanation types and their combinations influence misinformation detection.

**AI Summary:** This research explores the effectiveness of different AI explanations in aiding users to detect misinformation. The study compares content explanations, social explanations, and combined explanations in two online experiments related to COVID-19 and Politics domains. The results show that the alignment between content and social explanations significantly influences detection accuracy, highlighting the importance of considering different explanation types and their combinations in designing effective AI explanations for misinformation detection.

---

## Promisedland: An XR Narrative Attraction Integrating Diorama-to-Virtual Workflow and Elemental Storytelling
**URL:** https://arxiv.org/abs/2509.03678

**Abstract:** Promisedland is a mixed-reality (MR) narrative attraction that combines cultural storytelling, ecological education, and an innovative hybrid production workflow. Set in a future Earth suffering from elemental imbalance, users embark on an interactive journey guided by symbolic characters to restore harmony through the collection of five classical elements: metal, wood, water, fire, and earth. To prototype this experience, we introduce a low-cost, high-fidelity Diorama-to-Virtual pipeline - handcrafting physical scale models, 3D scanning, and integrating them into Unreal Engine. This process enables rapid spatial prototyping while preserving the material expressiveness and narrative consistency of the physical environment. To further enhance immersion, the experience incorporates a Stewart Platform to provide motion feedback synchronized with the virtual ride dynamics, reinforcing spatial presence and embodied engagement. The final prototype runs on Meta Quest, supporting dynamic interactions and real-time visual feedback. Promisedland offers a replicable design blueprint for future XR narrative installations across museums, cultural exhibitions, and themed entertainment. It proposes a new framework for XR Narrative Attractions - where physical and digital elements converge to deepen immersion, agency, and emotional engagement.

**AI Summary:** The research presents Promisedland, a mixed-reality narrative attraction that combines cultural storytelling and ecological education through an interactive journey to restore elemental balance on a future Earth. The study introduces a low-cost Diorama-to-Virtual pipeline for rapid spatial prototyping and immersive experiences, utilizing physical scale models and 3D scanning integrated into Unreal Engine. The findings highlight the potential for replicable design blueprints for XR narrative installations in museums, cultural exhibitions, and themed entertainment, proposing a new framework where physical and digital elements converge to enhance immersion and emotional engagement.

---

## DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation
**URL:** https://arxiv.org/abs/2509.04441

**Abstract:** We introduce perioperation, a paradigm for robotic data collection that sensorizes and records human manipulation while maximizing the transferability of the data to real robots. We implement this paradigm in DEXOP, a passive hand exoskeleton designed to maximize human ability to collect rich sensory (vision + tactile) data for diverse dexterous manipulation tasks in natural environments. DEXOP mechanically connects human fingers to robot fingers, providing users with direct contact feedback (via proprioception) and mirrors the human hand pose to the passive robot hand to maximize the transfer of demonstrated skills to the robot. The force feedback and pose mirroring make task demonstrations more natural for humans compared to teleoperation, increasing both speed and accuracy. We evaluate DEXOP across a range of dexterous, contact-rich tasks, demonstrating its ability to collect high-quality demonstration data at scale. Policies learned with DEXOP data significantly improve task performance per unit time of data collection compared to teleoperation, making DEXOP a powerful tool for advancing robot dexterity. Our project page is at this https URL.

**AI Summary:** The research introduces a device called DEXOP, which allows for the collection of human manipulation data for robotic tasks in natural environments. By mechanically connecting human fingers to robot fingers and providing feedback and pose mirroring, DEXOP enables the transfer of human skills to robots more effectively than traditional teleoperation methods. The device has been shown to improve task performance and data collection efficiency, making it a valuable tool for enhancing robot dexterity.

---

## No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in Resume Screening
**URL:** https://arxiv.org/abs/2509.04404

**Abstract:** In this study, we conduct a resume-screening experiment (N=528) where people collaborate with simulated AI models exhibiting race-based preferences (bias) to evaluate candidates for 16 high and low status occupations. Simulated AI bias approximates factual and counterfactual estimates of racial bias in real-world AI systems. We investigate people's preferences for White, Black, Hispanic, and Asian candidates (represented through names and affinity groups on quality-controlled resumes) across 1,526 scenarios and measure their unconscious associations between race and status using implicit association tests (IATs), which predict discriminatory hiring decisions but have not been investigated in human-AI collaboration. When making decisions without AI or with AI that exhibits no race-based preferences, people select all candidates at equal rates. However, when interacting with AI favoring a particular group, people also favor those candidates up to 90% of the time, indicating a significant behavioral shift. The likelihood of selecting candidates whose identities do not align with common race-status stereotypes can increase by 13% if people complete an IAT before conducting resume screening. Finally, even if people think AI recommendations are low quality or not important, their decisions are still vulnerable to AI bias under certain circumstances. This work has implications for people's autonomy in AI-HITL scenarios, AI and work, design and evaluation of AI hiring systems, and strategies for mitigating bias in collaborative decision-making tasks. In particular, organizational and regulatory policy should acknowledge the complex nature of AI-HITL decision making when implementing these systems, educating people who use them, and determining which are subject to oversight.

**AI Summary:** This study examines how people's decision-making in resume screening is influenced by AI models exhibiting race-based biases. The research shows that when interacting with biased AI, individuals tend to favor candidates from the group favored by the AI, indicating a significant behavioral shift. Additionally, the study highlights the importance of understanding and mitigating bias in AI-human collaboration in hiring processes to ensure fair and unbiased decision-making.

---

## Psychologically Enhanced AI Agents
**URL:** https://arxiv.org/abs/2509.04343

**Abstract:** We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of Large Language Model (LLM) agents through psychologically grounded personality conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology, cognition and affect. We show that such personality priming yields consistent, interpretable behavioral biases across diverse tasks: emotionally expressive agents excel in narrative generation, while analytically primed agents adopt more stable strategies in game-theoretic settings. Our framework supports experimenting with structured multi-agent communication protocols and reveals that self-reflection prior to interaction improves cooperation and reasoning quality. To ensure trait persistence, we integrate the official 16Personalities test for automated verification. While our focus is on MBTI, we show that our approach generalizes seamlessly to other psychological frameworks such as Big Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior design, we establish a foundation for psychologically enhanced AI agents without any fine-tuning.

**AI Summary:** The research introduces a framework called MBTI-in-Thoughts that enhances the effectiveness of AI agents by priming them with distinct personality archetypes based on the Myers-Briggs Type Indicator. This priming results in consistent and interpretable behavioral biases across different tasks, with emotionally expressive agents excelling in narrative generation and analytically primed agents adopting more stable strategies in game-theoretic settings. The framework supports structured multi-agent communication protocols and shows that self-reflection prior to interaction improves cooperation and reasoning quality, with trait persistence ensured through automated verification.

---

## Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue
**URL:** https://arxiv.org/abs/2509.04104

**Abstract:** Lexical alignment, where speakers start to use similar words across conversation, is known to contribute to successful communication. However, its implementation in conversational agents remains underexplored, particularly considering the recent advancements in large language models (LLMs). As a first step towards enabling lexical alignment in human-agent dialogue, this study draws on strategies for personalising conversational agents and investigates the construction of stable, personalised lexical profiles as a basis for lexical alignment. Specifically, we varied the amounts of transcribed spoken data used for construction as well as the number of items included in the profiles per part-of-speech (POS) category and evaluated profile performance across time using recall, coverage, and cosine similarity metrics. It was shown that smaller and more compact profiles, created after 10 min of transcribed speech containing 5 items for adjectives, 5 items for conjunctions, and 10 items for adverbs, nouns, pronouns, and verbs each, offered the best balance in both performance and data efficiency. In conclusion, this study offers practical insights into constructing stable, personalised lexical profiles, taking into account minimal data requirements, serving as a foundational step toward lexical alignment strategies in conversational agents.

**AI Summary:** This study explores the implementation of lexical alignment in conversational agents using strategies for personalizing agents and constructing stable, personalized lexical profiles. The research found that smaller, more compact profiles created with minimal data requirements offered the best balance in performance and data efficiency, specifically with 10 items for adjectives, conjunctions, adverbs, nouns, pronouns, and verbs each after 10 minutes of transcribed speech. These findings provide practical insights for enabling lexical alignment in human-agent dialogue, serving as a foundational step in advancing conversational agents.

---

## The MolecularWeb Universe: Web-Based, Immersive, Multiuser Molecular Graphics And Modeling, for Education and Work in Chemistry, Structural Biology, and Materials Sciences
**URL:** https://arxiv.org/abs/2509.04056

**Abstract:** Molecular visualization software has long supported research and education in chemical and structural sciences, but consumer devices constrained to 2D inputs and outputs pose two major challenges: they poorly convey 3D nature, and 3D manipulation is very difficult. eXtended Reality (XR, including AR and VR) offers new ways to see and interact with molecules in three dimensions. This chapter presents the "MolecularWeb" ecosystem (this https URL), a set of web-based tools for immersive visualization, modeling, and simulations, already widely used in education and science communication and now expanding toward research applications. We cover moleculARweb, which provides AR educational activities via phones, tablets, and computers; MolecularWebXR, a multiuser WebXR platform accessible from both headsets and simpler devices, supporting immersive education, outreach, and scientific discussion; and PDB2AR, which enables users to generate custom content for MolecularWebXR and standalone AR/VR. Finally, we introduce a prototype and an upcoming version of HandMol, our latest WebXR software which allows concurrent multiuser immersive visualization and modeling of molecules with bare hands supported by real-time molecular mechanics, natural language input via a language model, and access through both high-end headsets or consumer devices like smartphones and laptops. Together, these tools demonstrate the present and near-future of accessible, interactive molecular science on the web.

**AI Summary:** The abstract discusses the challenges of 3D molecular visualization with consumer devices and introduces the MolecularWeb ecosystem, a set of web-based tools for immersive visualization, modeling, and simulations in chemistry, structural biology, and materials sciences. The tools include moleculARweb for AR educational activities, MolecularWebXR for multiuser immersive education and scientific discussion, PDB2AR for custom content generation, and HandMol for multiuser immersive visualization and modeling of molecules with natural language input. These tools showcase the accessibility and interactive potential of molecular science on the web.

---

## Towards an Understanding of Developer Experience-Driven Transparency in Software Ecosystems
**URL:** https://arxiv.org/abs/2509.03848

**Abstract:** Software ecosystems (SECO) have become a dominant paradigm in the software industry, enabling third-party developers to co-create value through complementary components and services. While Developer Experience (DX) is increasingly recognized as critical for sustainable SECO, transparency remains an underexplored factor shaping how developers perceive and interact with ecosystems. Existing studies acknowledge transparency as essential for trust, fairness, and engagement, yet its relationship with DX has not been systematically conceptualized. Hence, this work aims to advance the understanding of transparency in SECO from a developer-centered perspective. To this end, we propose SECO-TransDX (Transparency in Software Ecosystems from a Developer Experience Perspective), a conceptual model that introduces the notion of DX-driven transparency. The model identifies 63 interrelated concepts, including conditioning factors, ecosystem procedures, artifacts, and relational dynamics that influence how transparency is perceived and constructed during developer interactions. SECO-TransDX was built upon prior research and refined through a Delphi study with experts from academia and industry. It offers a structured lens to examine how transparency mediates DX across technical, social, and organizational layers. For researchers, it lays the groundwork for future studies and tool development; for practitioners, it supports the design of trustworthy, developer-centered platforms that improve transparency and foster long-term engagement in SECO.

**AI Summary:** This research explores the relationship between transparency and Developer Experience (DX) in software ecosystems (SECO). The study introduces a conceptual model, SECO-TransDX, which identifies 63 interrelated concepts that shape how transparency is perceived and constructed by developers. The model aims to provide a structured lens for researchers to examine how transparency influences DX across technical, social, and organizational layers, and for practitioners to design trustworthy, developer-centered platforms that improve transparency and foster long-term engagement in SECO.

---

## PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming
**URL:** https://arxiv.org/abs/2509.03728

**Abstract:** Recent developments in AI governance and safety research have called for red-teaming methods that can effectively surface potential risks posed by AI models. Many of these calls have emphasized how the identities and backgrounds of red-teamers can shape their red-teaming strategies, and thus the kinds of risks they are likely to uncover. While automated red-teaming approaches promise to complement human red-teaming by enabling larger-scale exploration of model behavior, current approaches do not consider the role of identity. As an initial step towards incorporating people's background and identities in automated red-teaming, we develop and evaluate a novel method, PersonaTeaming, that introduces personas in the adversarial prompt generation process to explore a wider spectrum of adversarial strategies. In particular, we first introduce a methodology for mutating prompts based on either "red-teaming expert" personas or "regular AI user" personas. We then develop a dynamic persona-generating algorithm that automatically generates various persona types adaptive to different seed prompts. In addition, we develop a set of new metrics to explicitly measure the "mutation distance" to complement existing diversity measurements of adversarial prompts. Our experiments show promising improvements (up to 144.1%) in the attack success rates of adversarial prompts through persona mutation, while maintaining prompt diversity, compared to RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the strengths and limitations of different persona types and mutation methods, shedding light on future opportunities to explore complementarities between automated and human red-teaming approaches.

**AI Summary:** The research explores how incorporating personas in automated AI red-teaming can improve the identification of potential risks posed by AI models. The study introduces a method called PersonaTeaming, which uses personas in the adversarial prompt generation process to enhance the diversity of adversarial strategies. Results show a significant increase in attack success rates through persona mutation, highlighting the potential for incorporating identity and background in automated red-teaming to complement human red-teaming approaches.

---

## PG-Agent: An Agent Powered by Page Graph
**URL:** https://arxiv.org/abs/2509.03536

**Abstract:** Graphical User Interface (GUI) agents possess significant commercial and social value, and GUI agents powered by advanced multimodal large language models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI agents usually utilize sequential episodes of multi-step operations across pages as the prior GUI knowledge, which fails to capture the complex transition relationship between pages, making it challenging for the agents to deeply perceive the GUI environment and generalize to new scenarios. Therefore, we design an automated pipeline to transform the sequential episodes into page graphs, which explicitly model the graph structure of the pages that are naturally connected by actions. To fully utilize the page graphs, we further introduce Retrieval-Augmented Generation (RAG) technology to effectively retrieve reliable perception guidelines of GUI from them, and a tailored multi-agent framework PG-Agent with task decomposition strategy is proposed to be injected with the guidelines so that it can generalize to unseen scenarios. Extensive experiments on various benchmarks demonstrate the effectiveness of PG-Agent, even with limited episodes for page graph construction.

**AI Summary:** The research focuses on developing a GUI agent powered by page graphs, which capture the complex transition relationships between pages in a GUI environment. By utilizing Retrieval-Augmented Generation technology and a multi-agent framework with task decomposition strategy, the PG-Agent is able to generalize to new scenarios effectively. Experimental results show the effectiveness of PG-Agent, even with limited episodes for page graph construction, highlighting its potential for commercial and social applications.

---

## SmartPoser: Arm Pose Estimation with a Smartphone and Smartwatch Using UWB and IMU Data
**URL:** https://arxiv.org/abs/2509.03451

**Abstract:** The ability to track a user's arm pose could be valuable in a wide range of applications, including fitness, rehabilitation, augmented reality input, life logging, and context-aware assistants. Unfortunately, this capability is not readily available to consumers. Systems either require cameras, which carry privacy issues, or utilize multiple worn IMUs or markers. In this work, we describe how an off-the-shelf smartphone and smartwatch can work together to accurately estimate arm pose. Moving beyond prior work, we take advantage of more recent ultra-wideband (UWB) functionality on these devices to capture absolute distance between the two devices. This measurement is the perfect complement to inertial data, which is relative and suffers from drift. We quantify the performance of our software-only approach using off-the-shelf devices, showing it can estimate the wrist and elbow joints with a \hl{median positional error of 11.0~cm}, without the user having to provide training data.

**AI Summary:** The research introduces SmartPoser, a system that uses a smartphone and smartwatch to accurately estimate arm pose without the need for cameras or multiple IMUs. By utilizing ultra-wideband functionality and inertial data, the system can capture absolute distance between the devices and reduce drift in the measurements. The software-only approach demonstrated promising results, with a median positional error of 11.0 cm for wrist and elbow joint estimation, making it a valuable tool for various applications such as fitness, rehabilitation, and augmented reality input.

---

## EclipseTouch: Touch Segmentation on Ad Hoc Surfaces using Worn Infrared Shadow Casting
**URL:** https://arxiv.org/abs/2509.03430

**Abstract:** The ability to detect touch events on uninstrumented, everyday surfaces has been a long-standing goal for mixed reality systems. Prior work has shown that virtual interfaces bound to physical surfaces offer performance and ergonomic benefits over tapping at interfaces floating in the air. A wide variety of approaches have been previously developed, to which we contribute a new headset-integrated technique called \systemname. We use a combination of a computer-triggered camera and one or more infrared emitters to create structured shadows, from which we can accurately estimate hover distance (mean error of 6.9~mm) and touch contact (98.0\% accuracy). We discuss how our technique works across a range of conditions, including surface material, interaction orientation, and environmental lighting.

**AI Summary:** The research introduces a new technique called EclipseTouch, which uses a combination of a computer-triggered camera and infrared emitters to detect touch events on everyday surfaces. The technique is able to accurately estimate hover distance and touch contact with high accuracy. The study demonstrates the effectiveness of EclipseTouch across various conditions, highlighting its potential for improving mixed reality systems and virtual interfaces.

---

## More AI Assistance Reduces Cognitive Engagement: Examining the AI Assistance Dilemma in AI-Supported Note-Taking
**URL:** https://arxiv.org/abs/2509.03392

**Abstract:** As AI tools become increasingly embedded in cognitively demanding tasks such as note-taking, questions remain about whether they enhance or undermine cognitive engagement. This paper examines the "AI Assistance Dilemma" in note-taking, investigating how varying levels of AI support affect user engagement and comprehension. In a within-subject experiment, we asked participants (N=30) to take notes during lecture videos under three conditions: Automated AI (high assistance with structured notes), Intermediate AI (moderate assistance with real-time summary, and Minimal AI (low assistance with transcript). Results reveal that Intermediate AI yields the highest post-test scores and Automated AI the lowest. Participants, however, preferred the automated setup due to its perceived ease of use and lower cognitive effort, suggesting a discrepancy between preferred convenience and cognitive benefits. Our study provides insights into designing AI assistance that preserves cognitive engagement, offering implications for designing moderate AI support in cognitive tasks.

**AI Summary:** This study explores the impact of varying levels of AI assistance on cognitive engagement in note-taking tasks. The results show that intermediate AI support leads to the highest post-test scores, while automated AI support leads to the lowest scores. Despite the cognitive benefits of intermediate AI, participants preferred the automated setup for its perceived ease of use, highlighting the need to balance convenience with cognitive engagement when designing AI assistance in cognitive tasks.

---

## Beyond Quantification: Navigating Uncertainty in Professional AI Systems
**URL:** https://arxiv.org/abs/2509.03271

**Abstract:** The growing integration of large language models across professional domains transforms how experts make critical decisions in healthcare, education, and law. While significant research effort focuses on getting these systems to communicate their outputs with probabilistic measures of reliability, many consequential forms of uncertainty in professional contexts resist such quantification. A physician pondering the appropriateness of documenting possible domestic abuse, a teacher assessing cultural sensitivity, or a mathematician distinguishing procedural from conceptual understanding face forms of uncertainty that cannot be reduced to percentages. This paper argues for moving beyond simple quantification toward richer expressions of uncertainty essential for beneficial AI integration. We propose participatory refinement processes through which professional communities collectively shape how different forms of uncertainty are communicated. Our approach acknowledges that uncertainty expression is a form of professional sense-making that requires collective development rather than algorithmic optimization.

**AI Summary:** This research paper discusses the limitations of using probabilistic measures to quantify uncertainty in professional AI systems in fields such as healthcare, education, and law. The authors argue for a more nuanced approach to expressing uncertainty that takes into account the complex and subjective nature of decision-making in these contexts. They propose participatory refinement processes to allow professional communities to collectively shape how uncertainty is communicated, emphasizing the importance of human input in developing effective AI integration strategies.

---

## Card Sorting with Fewer Cards and the Same Mental Models? A Re-examination of an Established Practice
**URL:** https://arxiv.org/abs/2509.03232

**Abstract:** To keep card sorting with a lot of cards concise, a common strategy for gauging mental models involves presenting participants with fewer randomly selected cards instead of the full set. This is a decades-old practice, but its effects lacked systematic examination. To assess how randomized subsets affect data, we conducted an experiment with 160 participants. We compared results between full and randomized 60\% card sets, then analyzed sample size requirements and the impacts of individual personality and cognitive factors. Our results demonstrate that randomized subsets can yield comparable similarity matrices to standard card sorting, but thematic patterns in categories can differ. Increased data variability also warrants larger sample sizes (25-35 for 60% card subset). Results indicate that personality traits and cognitive reflection interact with card sorting. Our research suggests evidence-based practices for conducting card sorting while exposing the influence of study design and individual differences on measurement of mental models.

**AI Summary:** This research re-examined the practice of using randomized subsets of cards in card sorting to gauge mental models. The study found that while randomized subsets can yield similar similarity matrices to using the full set of cards, thematic patterns in categories may differ. The research also highlighted the importance of considering individual personality traits and cognitive factors when conducting card sorting, and suggested evidence-based practices for improving the accuracy of measuring mental models.

---

## Finding My Way: Influence of Different Audio Augmented Reality Navigation Cues on User Experience and Subjective Usefulness
**URL:** https://arxiv.org/abs/2509.03199

**Abstract:** As augmented reality (AR) becomes increasingly prevalent in mobile and context-aware applications, the role of auditory cues in guiding users through physical environments is becoming critical. This study investigates the effectiveness and user experience of various categories of audio cues, including fully non-verbal sounds and speech-derived Spearcons, during outdoor navigation tasks using the Meta Quest 3 headset. Twenty participants navigated five outdoor routes using audio-only cue types: Artificial Sounds, Nature Sounds, Spearcons, Musical Instruments, and Auditory Icons. Subjective evaluations were collected to assess the perceived effectiveness and user experience of each sound type. Results revealed significant differences in perceived novelty and stimulation across sound types. Artificial Sounds and Musical Instruments were rated higher than Spearcons in novelty, while Artificial Sounds were also rated higher than Spearcons in stimulation. Overall preference was evenly split between Nature Sounds and Artificial Sounds. These findings suggest that incorporating aspects of novelty and user engagement in auditory feedback design may enhance the effectiveness of AR navigation systems.

**AI Summary:** This study explores the impact of different audio cues on user experience and effectiveness in outdoor navigation tasks using augmented reality. The results show that Artificial Sounds and Musical Instruments were perceived as more novel and stimulating compared to Spearcons, with Nature Sounds and Artificial Sounds being preferred overall. The findings suggest that incorporating novelty and user engagement in auditory feedback design can improve the effectiveness of AR navigation systems.

---

## Beyond Words: Interjection Classification for Improved Human-Computer Interaction
**URL:** https://arxiv.org/abs/2509.03181

**Abstract:** In the realm of human-computer interaction, fostering a natural dialogue between humans and machines is paramount. A key, often overlooked, component of this dialogue is the use of interjections such as "mmm" and "hmm". Despite their frequent use to express agreement, hesitation, or requests for information, these interjections are typically dismissed as "non-words" by Automatic Speech Recognition (ASR) engines. Addressing this gap, we introduce a novel task dedicated to interjection classification, a pioneer in the field to our knowledge. This task is challenging due to the short duration of interjection signals and significant inter- and intra-speaker variability. In this work, we present and publish a dataset of interjection signals collected specifically for interjection classification. We employ this dataset to train and evaluate a baseline deep learning model. To enhance performance, we augment the training dataset using techniques such as tempo and pitch transformation, which significantly improve classification accuracy, making models more robust. The interjection dataset, a Python library for the augmentation pipeline, baseline model, and evaluation scripts, are available to the research community.

**AI Summary:** This research focuses on the classification of interjections in human-computer interaction to improve dialogue between humans and machines. Interjections like "mmm" and "hmm" are often overlooked by ASR engines despite their importance in expressing agreement, hesitation, or requests for information. The study introduces a novel task of interjection classification, presents a dataset for training and evaluation, and demonstrates that augmenting the dataset with techniques like tempo and pitch transformation significantly improves classification accuracy, making models more robust for natural dialogue interactions.

---

## OPRA-Vis: Visual Analytics System to Assist Organization-Public Relationship Assessment with Large Language Models
**URL:** https://arxiv.org/abs/2509.03164

**Abstract:** Analysis of public opinions collected from digital media helps organizations maintain positive relationships with the public. Such public relations (PR) analysis often involves assessing opinions, for example, measuring how strongly people trust an organization. Pre-trained Large Language Models (LLMs) hold great promise for supporting Organization-Public Relationship Assessment (OPRA) because they can map unstructured public text to OPRA dimensions and articulate rationales through prompting. However, adapting LLMs for PR analysis typically requires fine-tuning on large labeled datasets, which is both labor-intensive and knowledge-intensive, making it difficult for PR researchers to apply these models. In this paper, we present OPRA-Vis, a visual analytics system that leverages LLMs for OPRA without requiring extensive labeled data. Our framework employs Chain-of-Thought prompting to guide LLMs in analyzing public opinion data by incorporating PR expertise directly into the reasoning process. Furthermore, OPRA-Vis provides visualizations that reveal the clues and reasoning paths used by LLMs, enabling users to explore, critique, and refine model decisions. We demonstrate the effectiveness of OPRA-Vis through two real-world use cases and evaluate it quantitatively, through comparisons with alternative LLMs and prompting strategies, and qualitatively, through assessments of usability, effectiveness, and expert feedback.

**AI Summary:** The research introduces OPRA-Vis, a visual analytics system that utilizes Large Language Models (LLMs) to assist in Organization-Public Relationship Assessment (OPRA) without the need for extensive labeled data. The system incorporates Chain-of-Thought prompting to guide LLMs in analyzing public opinion data and provides visualizations to reveal the reasoning paths used by the models. This approach allows PR researchers to effectively assess public opinions and maintain positive relationships with the public, demonstrating the potential of LLMs in supporting OPRA.

---

## Demonstrating Visual Information Manipulation Attacks in Augmented Reality: A Hands-On Miniature City-Based Setup
**URL:** https://arxiv.org/abs/2509.02933

**Abstract:** Augmented reality (AR) enhances user interaction with the real world but also presents vulnerabilities, particularly through Visual Information Manipulation (VIM) attacks. These attacks alter important real-world visual cues, leading to user confusion and misdirected actions. In this demo, we present a hands-on experience using a miniature city setup, where users interact with manipulated AR content via the Meta Quest 3. The demo highlights the impact of VIM attacks on user decision-making and underscores the need for effective security measures in AR systems. Future work includes a user study and cross-platform testing.

**AI Summary:** This research demonstrates the potential impact of Visual Information Manipulation (VIM) attacks in augmented reality (AR) systems, showing how altered visual cues can lead to user confusion and misdirected actions. Using a miniature city setup with the Meta Quest 3, the demo highlights the importance of implementing security measures in AR to protect users from such attacks. Future work will involve a user study and testing across different platforms to further understand and address these vulnerabilities.

---

## The Basic B*** Effect: The Use of LLM-based Agents Reduces the Distinctiveness and Diversity of People's Choices
**URL:** https://arxiv.org/abs/2509.02910

**Abstract:** Large language models (LLMs) increasingly act on people's behalf: they write emails, buy groceries, and book restaurants. While the outsourcing of human decision-making to AI can be both efficient and effective, it raises a fundamental question: how does delegating identity-defining choices to AI reshape who people become? We study the impact of agentic LLMs on two identity-relevant outcomes: interpersonal distinctiveness - how unique a person's choices are relative to others - and intrapersonal diversity - the breadth of a single person's choices over time. Using real choices drawn from social-media behavior of 1,000 U.S. users (110,000 choices in total), we compare a generic and personalized agent to a human baseline. Both agents shift people's choices toward more popular options, reducing the distinctiveness of their behaviors and preferences. While the use of personalized agents tempers this homogenization (compared to the generic AI), it also more strongly compresses the diversity of people's preference portfolios by narrowing what they explore across topics and psychological affinities. Understanding how AI agents might flatten human experience, and how using generic versus personalized agents involves distinctiveness-diversity trade-offs, is critical for designing systems that augment rather than constrain human agency, and for safeguarding diversity in thought, taste, and expression.

**AI Summary:** This research examines the impact of using agentic large language models (LLMs) on people's identity-defining choices. The study found that delegating decisions to AI agents, whether generic or personalized, leads to a reduction in the distinctiveness and diversity of individuals' choices. While personalized agents can mitigate some of this homogenization, they also restrict the variety of topics and preferences individuals explore. This research highlights the importance of understanding how AI can influence human behavior and the need to design systems that enhance rather than limit human agency and diversity of thought.

---

## Designing a Lightweight GenAI Interface for Visual Data Analysis
**URL:** https://arxiv.org/abs/2509.02878

**Abstract:** Recent advances in Generative AI have transformed how users interact with data analysis through natural language interfaces. However, many systems rely too heavily on LLMs, creating risks of hallucination, opaque reasoning, and reduced user control. We present a hybrid visual analysis system that integrates GenAI in a constrained, high-level role to support statistical modeling while preserving transparency and user agency. GenAI translates natural language intent into formal statistical formulations, while interactive visualizations surface model behavior, residual patterns, and hypothesis comparisons to guide iterative exploration. Model fitting, diagnostics, and hypothesis testing are delegated entirely to a structured R-based backend, ensuring correctness, interpretability, and reproducibility. By combining GenAI-assisted intent translation with visualization-driven reasoning, our approach broadens access to modeling tools without compromising rigor. We present an example use case of the tool and discuss challenges and opportunities for future research.

**AI Summary:** The research presents a hybrid visual analysis system that integrates Generative AI in a limited role to support statistical modeling while maintaining transparency and user control. By combining GenAI-assisted intent translation with visualization-driven reasoning, the system allows for broader access to modeling tools without sacrificing rigor. The approach delegates model fitting, diagnostics, and hypothesis testing to a structured R-based backend, ensuring correctness, interpretability, and reproducibility in data analysis.

---

## STRive: An association rule-based system for the exploration of spatiotemporal categorical data
**URL:** https://arxiv.org/abs/2509.02732

**Abstract:** Effectively analyzing spatiotemporal data plays a central role in understanding real-world phenomena and informing decision-making. Capturing the interaction between spatial and temporal dimensions also helps explain the underlying structure of the data. However, most datasets do not reveal attribute relationships, requiring additional algorithms to extract meaningful patterns. Existing visualization tools often focus either on attribute relationships or spatiotemporal analysis, but rarely support both simultaneously. In this paper, we present STRive (SpatioTemporal Rule Interactive Visual Explorer), a visual analytics system that enables users to uncover and explore spatial and temporal patterns in data. At the core of STRive lies Association Rule Mining (ARM), which we apply to spatiotemporal datasets to generate interpretable and actionable insights. We combine ARM with multiple interactive mechanisms to analyze the extracted relationships. Association rules serve as interpretable guidance mechanisms for visual analytics by highlighting the meaningful aspects of the data that users should investigate. Our methodology includes three key steps: rule generation, rule clustering, and interactive visualization. STRive offers two modes of analysis. The first operates at the rule cluster level and includes four coordinated views, each showing a different facet of a cluster, including its temporal and spatial behavior. The second mode mirrors the first but focuses on individual rules within a selected cluster. We evaluate the effectiveness of STRive through two case studies involving real-world datasets -- fatal vehicle accidents and urban crime. Results demonstrate the system's ability to support the discovery and analysis of interpretable patterns in complex spatiotemporal contexts.

**AI Summary:** The research introduces STRive, a visual analytics system that uses Association Rule Mining to uncover spatial and temporal patterns in data. By combining ARM with interactive mechanisms, the system allows users to analyze relationships in spatiotemporal datasets and generate actionable insights. Case studies with real-world datasets show that STRive is effective in discovering and analyzing interpretable patterns in complex spatiotemporal contexts.

---

## Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data
**URL:** https://arxiv.org/abs/2509.03501

**Abstract:** Next-generation AI companions must go beyond general video understanding to resolve spatial and temporal references in dynamic, real-world environments. Existing Video Large Language Models (Video LLMs), while capable of coarse-level comprehension, struggle with fine-grained, spatiotemporal reasoning, especially when user queries rely on time-based event references for temporal anchoring, or gestural cues for spatial anchoring to clarify object references and positions. To bridge this critical gap, we introduce Strefer, a synthetic instruction data generation framework designed to equip Video LLMs with spatiotemporal referring and reasoning capabilities. Strefer produces diverse instruction-tuning data using a data engine that pseudo-annotates temporally dense, fine-grained video metadata, capturing rich spatial and temporal information in a structured manner, including subjects, objects, their locations as masklets, and their action descriptions and timelines. Our approach enhances the ability of Video LLMs to interpret spatial and temporal references, fostering more versatile, space-time-aware reasoning essential for real-world AI companions. Without using proprietary models, costly human annotation, or the need to annotate large volumes of new videos, experimental evaluations show that models trained with data produced by Strefer outperform baselines on tasks requiring spatial and temporal disambiguation. Additionally, these models exhibit enhanced space-time-aware reasoning, establishing a new foundation for perceptually grounded, instruction-tuned Video LLMs.

**AI Summary:** The research introduces Strefer, a framework that generates synthetic instruction data to enhance Video Large Language Models (LLMs) with spatiotemporal referring and reasoning capabilities. This approach addresses the challenge of fine-grained spatiotemporal reasoning in real-world environments, enabling AI companions to interpret spatial and temporal references more effectively. Experimental evaluations demonstrate that models trained with data generated by Strefer outperform baselines on tasks requiring spatial and temporal disambiguation, establishing a new foundation for instruction-tuned Video LLMs with enhanced space-time-aware reasoning.

---

## Cost-Optimized Systems Engineering for IoT-Enabled Robot Nurse in Infectious Pandemic Management
**URL:** https://arxiv.org/abs/2509.03436

**Abstract:** The utilization of robotic technology has gained traction in healthcare facilities due to progress in the field that enables time and cost savings, minimizes waste, and improves patient care. Digital healthcare technologies that leverage automation, such as robotics and artificial intelligence, have the potential to enhance the sustainability and profitability of healthcare systems in the long run. However, the recent COVID-19 pandemic has amplified the need for cyber-physical robots to automate check-ups and medication administration. A robot nurse is controlled by the Internet of Things (IoT) and can serve as an automated medical assistant while also allowing supervisory control based on custom commands. This system helps reduce infection risk and improves outcomes in pandemic settings. This research presents a test case with a nurse robot that can assess a patient's health status and take action accordingly. We also evaluate the system's performance in medication administration, health-status monitoring, and life-cycle considerations.

**AI Summary:** This research explores the use of IoT-enabled robot nurses in managing infectious pandemics, highlighting the potential for cost savings, waste reduction, and improved patient care in healthcare facilities. The study demonstrates the effectiveness of a robot nurse controlled by IoT in automating check-ups and medication administration, reducing infection risk, and improving outcomes during pandemics. The findings suggest that integrating robotics and AI in healthcare systems can enhance sustainability and profitability in the long term.

---

