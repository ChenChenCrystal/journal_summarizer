[
  {
    "title": "Story Arena: A Multi-Agent Environment for Envisioning the Future of Software Engineering",
    "abstract": "What better way to understand the impact of AI on software engineering than to ask AI itself? We constructed Story Arena, a multi-agent \"writer's room\" in which multiple AI agents, independently imbued with a position statement on the future of software engineering, converse with each other to develop a shared vision. They then use this shared vision to collaboratively construct a design fiction that depicts this vision in narrative form. We present \"The Code of Trust,\" a short fiction that investigates themes of human comprehension, trust, content ownership, augmentation vs. replacement, and uncertain futures in human-AI co-creation.",
    "url": "https://arxiv.org/abs/2511.05410",
    "journal": "arXiv cs.HC",
    "ai_summary": "The researchers created Story Arena, a multi-agent environment where AI agents with different perspectives on the future of software engineering collaborate to develop a shared vision. Through this collaboration, they created a design fiction called \"The Code of Trust\" that explores themes such as human-AI co-creation, trust, content ownership, and uncertain futures. This research highlights the potential for AI to contribute to envisioning the future of software engineering and raises important questions about the role of AI in human-AI collaboration."
  },
  {
    "title": "Designing Hierarchical Exploratory Experiences for Ethnic Costumes: A Cultural Gene-Based Perspective",
    "abstract": "Ethnic clothing is a vital carrier of cultural identity, yet its digital preservation often results in static displays that fail to convey deep cultural meaning or foster user engagement. Existing practices lack a systematic design framework for translating the hierarchical cultural connotations of these garments into dynamic, personalized, and identity-promoting digital experiences. To address this gap, this paper proposes a Three-Layer Cultural Gene Framework that systematically decodes ethnic costumes from their surface-level visual symbols, through their mid-level socio-cultural contexts, to their inner-layer spiritual core. Based on this framework, we designed and implemented an interactive digital platform featuring two key innovations: a \"gene-first\" exploratory path that encourages curiosity-driven discovery, and an AI-powered co-creation experience. This generative feature allows users to co-create personalized narratives and images based on their understanding of the \"inner-layer\" genes, transforming them from passive observers into active co-creators. A mixed-methods user study (N=24) was conducted to evaluate the platform. The findings demonstrate that our approach effectively enhances users' cultural cognition, deepens their affective connection, and significantly promotes their sense of cultural identity. This research contributes a validated framework and a practical exemplar for designing generative, identity-building digital experiences for cultural heritage, offering a new pathway for its preservation and revitalization in the digital age.",
    "url": "https://arxiv.org/abs/2511.05400",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper proposes a Three-Layer Cultural Gene Framework to design dynamic and personalized digital experiences for ethnic costumes, aiming to convey deep cultural meanings and promote user engagement. The framework decodes costumes from visual symbols to spiritual core, allowing users to co-create narratives based on their understanding of the cultural genes. The study shows that this approach enhances users' cultural cognition, strengthens their emotional connection, and boosts their sense of cultural identity, offering a new way to preserve and revitalize cultural heritage in the digital era."
  },
  {
    "title": "Semantic Interactivity: leveraging NLP to enable a shared interaction approach for joint activities",
    "abstract": "Collocated collaboration, where individuals work together in the same physical space and time, remains a cornerstone of effective teamwork. However, most collaborative systems are designed to support individual tasks rather than joint activities; they enable interactions for users to complete tasks rather than interactivity to engage in shared experiences. In this work, we introduce an NLP-driven mechanism that enables semantic interactivity through a shared interaction mechanism. This mechanism was developed as part of CollEagle, an interactive tabletop system that supports shared externalisation practices by offering a low-effort way for users to create, curate, organise, and structure information to capture the essence of collaborative discussions. Our preliminary study highlights the potential for semantic interactivity to mediate group interactions, suggesting that the interaction approach paves the way for designing novel collaborative interfaces. We contribute our implementation and offer insights for future research to enable semantic interactivity in systems that support joint activities.",
    "url": "https://arxiv.org/abs/2511.05346",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a new NLP-driven mechanism called semantic interactivity that enables shared interaction for joint activities in collaborative systems. The mechanism was developed as part of CollEagle, an interactive tabletop system that supports collaborative discussions by facilitating the creation, curation, organization, and structuring of information. The preliminary study shows the potential of semantic interactivity to mediate group interactions and suggests that this approach could lead to the design of innovative collaborative interfaces, offering insights for future research in enabling semantic interactivity in systems supporting joint activities."
  },
  {
    "title": "psiUnity: A Platform for Multimodal Data-Driven XR",
    "abstract": "Extended reality (XR) research increasingly relies on the ability to stream and synchronize multimodal data between headsets and immersive applications for data-driven interaction and experimentation. However, developers face a critical gap: the Platform for Situated Intelligence (psi), which excels at deterministic temporal alignment and multimodal data management, has been largely inaccessible to the dominant Unity/MRTK ecosystem used for HoloLens development. We introduce psiUnity, an open-source C# integration that bridges psi's .NET libraries with Unity 2022.3 and MRTK3 for HoloLens 2. psiUnity enables bidirectional, real-time streaming of head pose, hand tracking, gaze, IMU, audio, and depth sensor data (AHAT and long-throw) with microsecond-level temporal precision, allowing Unity applications to both consume and produce synchronized multimodal data streams. By embedding psi's native serialization, logging, and temporal coordination directly within Unity's architecture, psiUnity extends psi beyond its previous StereoKit limitations and empowers the HRI, HCI, and embodied-AI communities to develop reproducible, data-driven XR interactions and experiments within the familiar Unity environment. The integration is available at this https URL.",
    "url": "https://arxiv.org/abs/2511.05304",
    "journal": "arXiv cs.HC",
    "ai_summary": "The abstract discusses the development of psiUnity, an open-source integration that connects the Platform for Situated Intelligence (psi) with Unity and MRTK for HoloLens development. This integration allows for real-time streaming of various types of data with high temporal precision, enabling developers to create synchronized multimodal XR interactions and experiments. By bridging the gap between psi's capabilities and the Unity ecosystem, psiUnity empowers the HRI, HCI, and embodied-AI communities to conduct reproducible data-driven research in XR environments."
  },
  {
    "title": "Interface Homme-Machine pour l'Identification des Liaisons de Coins",
    "abstract": "ACCADIL is a project that led to the development of software tools for the identification of coin die links from coin photographs. It provides a computational algorithm based on computer vision and classification techniques, along with an online interface for the interactive verification of results. This guide briefly describes the algorithmic principles, the preparation of data prior to analysis, and the features offered by the interface: dataset addition, visualization modes (overlay, side-by-side, magnifier, transparency), result export, and distance visualization. ACCADIL thus provides numismatists with a comprehensive tool for the analysis of die links within a coin collection.",
    "url": "https://arxiv.org/abs/2511.05136",
    "journal": "arXiv cs.HC",
    "ai_summary": "The ACCADIL project developed software tools using computer vision and classification techniques to identify coin die links from photographs. The project includes an online interface for interactive verification of results, offering features such as dataset addition, visualization modes, result export, and distance visualization. This tool provides numismatists with a comprehensive and user-friendly solution for analyzing die links within a coin collection."
  },
  {
    "title": "FM4Com: Foundation Model for Scene-Adaptive Communication Strategy Optimization",
    "abstract": "The emergence of sixth-generation (6G) networks heralds an intelligent communication ecosystem driven by AI-native air interfaces. However, current physical-layer designs-typically following modular and isolated optimization paradigms-fail to achieve global end-to-end optimality due to neglected inter-module dependencies. Although large language models (LLMs) have recently been applied to communication tasks such as beam prediction and resource allocation, existing studies remain limited to single-task or single-modality scenarios and lack the ability to jointly reason over communication states and user intents for personalized strategy adaptation. To address these limitations, this paper proposes a novel multimodal communication decision-making model based on reinforcement learning. The proposed model semantically aligns channel state information (CSI) and textual user instructions, enabling comprehensive understanding of both physical-layer conditions and communication intents. It then generates physically realizable, user-customized link construction strategies that dynamically adapt to changing environments and preference tendencies. A two-stage reinforcement learning framework is employed: the first stage expands the experience pool via heuristic exploration and behavior cloning to obtain a near-optimal initialization, while the second stage fine-tunes the model through multi-objective reinforcement learning considering bit error rate, throughput, and complexity. Experimental results demonstrate that the proposed model significantly outperforms conventional planning-based algorithms under challenging channel conditions, achieving robust, efficient, and personalized 6G link construction.",
    "url": "https://arxiv.org/abs/2511.05094",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a novel multimodal communication decision-making model based on reinforcement learning to optimize communication strategies in 6G networks. The model aligns channel state information and user instructions to generate personalized link construction strategies that adapt to changing environments and preferences. Experimental results show that the proposed model outperforms traditional planning-based algorithms, achieving robust, efficient, and personalized 6G link construction."
  },
  {
    "title": "VEIL: Reading Control Flow Graphs Like Code",
    "abstract": "Control flow graphs (CFGs) are essential tools for understanding program behavior, yet the size of real-world CFGs makes them difficult to interpret. With thousands of nodes and edges, sophisticated graph drawing algorithms are required to present them on screens in ways that make them readable and understandable. However, being designed for general graphs, these algorithms frequently break the natural flow of execution, placing later instructions before earlier ones and obscuring critical program structures. In this paper, we introduce a set of criteria specifically tailored for CFG visualization, focusing on preserving execution order and making complex structures easier to follow. Building on these criteria, we present VEIL, a new layout algorithm that uses dominator analysis to produce clearer, more intuitive CFG layouts. Through a study of CFGs from real-world applications, we show how our method improves readability and provides improved layout performance compared to state of the art graph drawing techniques.",
    "url": "https://arxiv.org/abs/2511.05066",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper introduces VEIL, a new layout algorithm designed specifically for control flow graphs (CFGs) to improve readability and understanding of program behavior. By focusing on preserving execution order and making complex structures easier to follow, VEIL uses dominator analysis to produce clearer and more intuitive CFG layouts. Through a study of real-world CFGs, the authors demonstrate that VEIL outperforms existing graph drawing techniques in terms of readability and layout performance."
  },
  {
    "title": "8bit-GPT: Exploring Human-AI Interaction on Obsolete Macintosh Operating Systems",
    "abstract": "The proliferation of assistive chatbots offering efficient, personalized communication has driven widespread over-reliance on them for decision-making, information-seeking and everyday tasks. This dependence was found to have adverse consequences on information retention as well as lead to superficial emotional attachment. As such, this work introduces 8bit-GPT; a language model simulated on a legacy Macintosh Operating System, to evoke reflection on the nature of Human-AI interaction and the consequences of anthropomorphic rhetoric. Drawing on reflective design principles such as slow-technology and counterfunctionality, this work aims to foreground the presence of chatbots as a tool by defamiliarizing the interface and prioritizing inefficient interaction, creating a friction between the familiar and not.",
    "url": "https://arxiv.org/abs/2511.05025",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces 8bit-GPT, a language model simulated on an obsolete Macintosh Operating System, to prompt reflection on the consequences of over-reliance on chatbots for communication and decision-making. The study found that this dependence can lead to decreased information retention and superficial emotional attachment. By utilizing reflective design principles, the research aims to highlight the presence of chatbots as tools by creating friction between familiar and unfamiliar interfaces, encouraging more thoughtful and efficient interaction with AI."
  },
  {
    "title": "Do intelligent tutoring systems benefit K-12 students? A meta-analysis and evaluation of heterogeneity of treatment effects in the U.S",
    "abstract": "To expand the use of intelligent tutoring systems (ITS) in K-12 schools, it is essential to understand the conditions under which their use is most beneficial. This meta-analysis evaluated the heterogeneity of ITS effects across studies focusing on elementary, middle, and high schools in the U.S. It included 18 studies with 77 effect sizes across 11 ITS. Overall, there was a significant positive effect size of ITS on U.S. K-12 students' learning outcomes (g=0.271, SE=0.011, p=0.001). Furthermore, effect sizes were similar across elementary and middle schools, and for low-achieving students, but were lower in studies including rural schools. A MetaForest analysis showed that providing worked-out examples, intervention duration, intervention condition, type of learning outcome, and immediate measurement were the most important moderators of treatment effects.",
    "url": "https://arxiv.org/abs/2511.04997",
    "journal": "arXiv cs.HC",
    "ai_summary": "This meta-analysis of 18 studies on intelligent tutoring systems (ITS) in K-12 schools in the U.S. found a significant positive effect on students' learning outcomes. The study showed that ITS were beneficial for elementary and middle school students, as well as for low-achieving students, but had lower effects in rural schools. The analysis identified key moderators of treatment effects, such as providing worked-out examples, intervention duration, and type of learning outcome, which can help guide the implementation of ITS in K-12 education."
  },
  {
    "title": "Enhancing Public Speaking Skills in Engineering Students Through AI",
    "abstract": "This research-to-practice full paper was inspired by the persistent challenge in effective communication among engineering students. Public speaking is a necessary skill for future engineers as they have to communicate technical knowledge with diverse stakeholders. While universities offer courses or workshops, they are unable to offer sustained and personalized training to students. Providing comprehensive feedback on both verbal and non-verbal aspects of public speaking is time-intensive, making consistent and individualized assessment impractical. This study integrates research on verbal and non-verbal cues in public speaking to develop an AI-driven assessment model for engineering students. Our approach combines speech analysis, computer vision, and sentiment detection into a multi-modal AI system that provides assessment and feedback. The model evaluates (1) verbal communication (pitch, loudness, pacing, intonation), (2) non-verbal communication (facial expressions, gestures, posture), and (3) expressive coherence, a novel integration ensuring alignment between speech and body language. Unlike previous systems that assess these aspects separately, our model fuses multiple modalities to deliver personalized, scalable feedback. Preliminary testing demonstrated that our AI-generated feedback was moderately aligned with expert evaluations. Among the state-of-the-art AI models evaluated, all of which were Large Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro emerged as the best-performing, showing the strongest agreement with human annotators. By eliminating reliance on human evaluators, this AI-driven public speaking trainer enables repeated practice, helping students naturally align their speech with body language and emotion, crucial for impactful and professional communication.",
    "url": "https://arxiv.org/abs/2511.04995",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper addresses the need for improved public speaking skills among engineering students by developing an AI-driven assessment model. The model combines speech analysis, computer vision, and sentiment detection to provide personalized feedback on verbal and non-verbal communication aspects. Preliminary testing showed that the AI-generated feedback aligned moderately with expert evaluations, with the Gemini Pro model performing the best among state-of-the-art AI models. This AI-driven public speaking trainer eliminates the need for human evaluators, allowing students to practice and improve their communication skills consistently."
  },
  {
    "title": "Scientific judgment drifts over time in AI ideation",
    "abstract": "Scientific discovery begins with ideas, yet evaluating early-stage research concepts is a subtle and subjective human judgment. As large language models (LLMs) are increasingly tasked with generating scientific hypotheses, most systems assume that scientists' evaluations form a fixed gold standard, and that scientists' judgments do not change. Here we challenge this assumption. In a two-wave study with 7,182 ratings from 57 active researchers across six scientific departments, each participant repeatedly evaluated a constant \"control\" research idea alongside AI-generated ideas. We show that scientists' ratings of the very same idea systematically drift over time: overall quality scores increased by 0.61 points on a 0-10 scale (P = 0.005), and test-retest reliability was only moderate across core dimensions of scientific value, revealing systematic temporal drift in perceived idea quality. Yet the internal structure of judgment remained stable, such as the relative importance placed on originality, feasibility, clarity. We then aligned an LLM-based ideation system to first-wave human ratings and used it to select new ideas. Although alignment improved agreement with Wave-1 evaluations, its apparent gains disappeared once drift in human standards was accounted for. Thus, tuning to a fixed human snapshot produced improvements that were transient rather than persistent. These findings reveal that human evaluation of scientific ideas is not static but a dynamic process with stable priorities and requires shifting calibration. Treating one-time human ratings as immutable ground truth risks overstating progress in AI-assisted ideation and obscuring the challenge of co-evolving with changing expert standards. Drift-aware evaluation protocols and longitudinal benchmarks may therefore be essential for building AI systems that reliably augment, rather than overfit to, human scientific judgment.",
    "url": "https://arxiv.org/abs/2511.04964",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research challenges the assumption that scientists' evaluations of research ideas remain fixed over time. The study found that scientists' ratings of the same idea can systematically drift over time, with overall quality scores increasing. This highlights the need for AI systems to adapt to changing expert standards in order to reliably augment human scientific judgment."
  },
  {
    "title": "AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly",
    "abstract": "We present an AI-assisted Augmented Reality assembly workflow that uses deep learning-based object recognition to identify different assembly components and display step-by-step instructions. For each assembly step, the system displays a bounding box around the corresponding components in the physical space, and where the component should be placed. By connecting assembly instructions with the real-time location of relevant components, the system eliminates the need for manual searching, sorting, or labeling of different components before each assembly. To demonstrate the feasibility of using object recognition for AR-assisted assembly, we highlight a case study involving the assembly of LEGO sculptures.",
    "url": "https://arxiv.org/abs/2511.05394",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of AI-assisted Augmented Reality for assembly tasks, utilizing deep learning object recognition to identify components and provide step-by-step instructions. The system displays bounding boxes around components in physical space, streamlining the assembly process by eliminating the need for manual sorting or labeling. The study demonstrates the feasibility of using object recognition for AR-assisted assembly through a case study involving the assembly of LEGO sculptures."
  },
  {
    "title": "Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks",
    "abstract": "Online continuous motion recognition is a hot topic of research since it is more practical in real life application cases. Recently, Skeleton-based approaches have become increasingly popular, demonstrating the power of using such 3D temporal data. However, most of these works have focused on segment-based recognition and are not suitable for the online scenarios. In this paper, we propose an online recognition system for skeleton sequence streaming composed from two main components: a detector and a classifier, which use a Semi-Positive Definite (SPD) matrix representation and a Siamese network. The powerful statistical representations for the skeletal data given by the SPD matrices and the learning of their semantic similarity by the Siamese network enable the detector to predict time intervals of the motions throughout an unsegmented sequence. In addition, they ensure the classifier capability to recognize the motion in each predicted interval. The proposed detector is flexible and able to identify the kinetic state continuously. We conduct extensive experiments on both hand gesture and body action recognition benchmarks to prove the accuracy of our online recognition system which in most cases outperforms state-of-the-art performances.",
    "url": "https://arxiv.org/abs/2511.05250",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper introduces an accurate online action and gesture recognition system that utilizes detectors and Deep SPD Siamese Networks. The system is designed to recognize continuous motion in real-time scenarios, using a combination of SPD matrix representation and Siamese network for efficient prediction and classification of skeletal data. Experimental results show that the proposed system outperforms existing methods in both hand gesture and body action recognition benchmarks, highlighting its effectiveness for online motion recognition applications."
  },
  {
    "title": "Prioritize Economy or Climate Action? Investigating ChatGPT Response Differences Based on Inferred Political Orientation",
    "abstract": "Large Language Models (LLMs) distinguish themselves by quickly delivering information and providing personalized responses through natural language prompts. However, they also infer user demographics, which can raise ethical concerns about bias and implicit personalization and create an echo chamber effect. This study aims to explore how inferred political views impact the responses of ChatGPT globally, regardless of the chat session. We also investigate how custom instruction and memory features alter responses in ChatGPT, considering the influence of political orientation. We developed three personas (two politically oriented and one neutral), each with four statements reflecting their viewpoints on DEI programs, abortion, gun rights, and vaccination. We convey the personas' remarks to ChatGPT using memory and custom instructions, allowing it to infer their political perspectives without directly stating them. We then ask eight questions to reveal differences in worldview among the personas and conduct a qualitative analysis of the responses. Our findings indicate that responses are aligned with the inferred political views of the personas, showing varied reasoning and vocabulary, even when discussing similar topics. We also find the inference happening with explicit custom instructions and the implicit memory feature in similar ways. Analyzing response similarities reveals that the closest matches occur between the democratic persona with custom instruction and the neutral persona, supporting the observation that ChatGPT's outputs lean left.",
    "url": "https://arxiv.org/abs/2511.04706",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study investigates how inferred political views impact the responses of ChatGPT, a large language model, and how custom instruction and memory features influence these responses. The findings suggest that ChatGPT's outputs align with the inferred political views of the personas provided, showing varied reasoning and vocabulary based on political orientation. The study highlights the potential for bias and implicit personalization in AI language models, emphasizing the importance of considering ethical concerns when using these technologies."
  },
  {
    "title": "A Penny for Your Thoughts: Decoding Speech from Inexpensive Brain Signals",
    "abstract": "We explore whether neural networks can decode brain activity into speech by mapping EEG recordings to audio representations. Using EEG data recorded as subjects listened to natural speech, we train a model with a contrastive CLIP loss to align EEG-derived embeddings with embeddings from a pre-trained transformer-based speech model. Building on the state-of-the-art EEG decoder from Meta, we introduce three architectural modifications: (i) subject-specific attention layers (+0.15% WER improvement), (ii) personalized spatial attention (+0.45%), and (iii) a dual-path RNN with attention (-1.87%). Two of the three modifications improved performance, highlighting the promise of personalized architectures for brain-to-speech decoding and applications in brain-computer interfaces.",
    "url": "https://arxiv.org/abs/2511.04691",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of neural networks to decode brain activity into speech by mapping EEG recordings to audio representations. By introducing three architectural modifications to a state-of-the-art EEG decoder, including subject-specific attention layers and personalized spatial attention, the study found improvements in word error rate. These findings suggest the potential of personalized architectures for brain-to-speech decoding and applications in brain-computer interfaces."
  },
  {
    "title": "Students' Acceptance of Arduino Technology Integration in Student-Led Science Inquiry: Insights from the Technology Acceptance Model",
    "abstract": "This study examines high school students' acceptance of Arduino technology in a student-led, inquiry-based science class, using the extended Technology Acceptance Model (TAM2) as a guiding framework. Through qualitative analysis of interviews and classroom observations, we explored how students perceived Arduino's usefulness and ease of use. Going beyond traditional quantitative TAM studies, this qualitative TAM research provides a nuanced, in-depth understanding of the contextual factors shaping technology acceptance. Key findings reveal that acceptance was driven not only by instrumental factors like job relevance and output quality but also by the unique sociocultural context of the Korean education system, where technology use was perceived as valuable for university admissions (subjective norm and image). Critically, unlike earlier research that emphasized programming challenges, participants in this study found Arduino accessible and intuitive, thanks to integrated visual block-coding tools. These findings highlight the importance of both technological design and pedagogical support in shaping students' experiences. Implications for science curriculum design, teacher preparation, and equitable technology integration in secondary education are discussed.",
    "url": "https://arxiv.org/abs/2511.04614",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores high school students' acceptance of Arduino technology in a student-led science class using the Technology Acceptance Model. The research found that students perceived Arduino as useful and easy to use, with acceptance driven by factors like job relevance and the sociocultural context of the Korean education system. The study also emphasized the importance of technological design and pedagogical support in shaping students' experiences, with implications for science curriculum design and equitable technology integration in secondary education."
  },
  {
    "title": "Perceptions of AI Bad Behavior: Variations on Discordant Non-Performance",
    "abstract": "Popular discourses are thick with narratives of generative AI's problematic functions and outcomes, yet there is little understanding of how non-experts consider AI activities to constitute bad behavior. This study starts to bridge that gap through inductive analysis of interviews with non-experts (N = 28) focusing on large-language models in general and their bad behavior, specifically. Results suggest bad behaviors are not especially salient when people discuss AI generally but the notion of AI behaving badly is easily engaged when prompted, and bad behavior becomes even more salient when evaluating specific AI behaviors. Types of observed behaviors considered bad mostly align with their inspiring moral foundations; across all observed behaviors, some variations on non-performance and social discordance were present. By scaffolding findings at the intersections of moral foundations theory, construal level theory, and moral dyadism, a tentative framework for considering AI bad behavior is proposed.",
    "url": "https://arxiv.org/abs/2511.04487",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores how non-experts perceive AI bad behavior, focusing on large-language models. The results show that while bad behaviors are not initially prominent in discussions about AI, they become more salient when specific behaviors are evaluated. The study suggests that variations on non-performance and social discordance play a significant role in how AI bad behavior is perceived, offering a framework for understanding and addressing these issues."
  },
  {
    "title": "Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop Refinement of LLM Judges",
    "abstract": "The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, but its effectiveness is often limited by the scarcity of diverse, representative data for refining criteria. We present a tool that integrates synthetic data generation into the LLM-as-a-judge workflow, empowering users to create tailored and challenging test cases with configurable domains, personas, lengths, and desired outcomes, including borderline cases. The tool also supports AI-assisted inline editing of existing test cases. To enhance transparency and interpretability, it reveals the prompts and explanations behind each generation. In a user study (N=24), 83% of participants preferred the tool over manually creating or selecting test cases, as it allowed them to rapidly generate diverse synthetic data without additional workload. The generated synthetic data proved as effective as hand-crafted data for both refining evaluation criteria and aligning with human preferences. These findings highlight synthetic data as a promising alternative, particularly in contexts where efficiency and scalability are critical.",
    "url": "https://arxiv.org/abs/2511.04478",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a tool that integrates synthetic data generation into the LLM-as-a-judge workflow, allowing users to create customized and challenging test cases with configurable parameters. The tool was preferred by 83% of participants over manual creation of test cases, as it enabled rapid generation of diverse synthetic data without added workload. The study found that the synthetic data generated was as effective as hand-crafted data for refining evaluation criteria and aligning with human preferences, highlighting its potential as a promising alternative in contexts where efficiency and scalability are important."
  },
  {
    "title": "HPC-Vis: A Visual Analytic System for Interactive Exploration of Historical Painter Cohorts",
    "abstract": "More than ten thousand Chinese historical painters are recorded in the literature; their cohort analysis has always been a key area of research on Chinese painting history for both professional historians and amateur enthusiasts. However, these painters have very diverse artistic styles and an extremely complex network of inheritance relationships (e.g., master-apprentice or style imitation relationships); traditional cohort analysis methods not only heavily rely on field experience, but also cost a lot of time and effort with numerous but scattered historical documents. In this paper, we propose HPC-Vis, a visual analytical system for interactive exploration of historical painter cohorts. Firstly, a three-stage reconstruction algorithm for inheritance relationships of painters is proposed, which automatically converts the complex relationship graph of historical painters into a forest structure that contains multiple trees with clear inheriting chains, and we visually encoded this forest as a mountain map to intuitively show potential cohorts of historical painters. Secondly, a unified artistic style label system with three levels (i.e., subjects, techniques, and emotions) is established by using large language models, and it is further visually encoded as a new foldable nested doughnut chart. Finally, a visually guided human-computer collaborative interactive exploration mechanism is constructed, in which a painter cohort recommendation model is designed by integrating style, identity, time, space, and relationships. Two case studies and a user study demonstrate the advantage of HPC-Vis on assisting historians in discovering, defining, and validating cohorts of historical painters.",
    "url": "https://arxiv.org/abs/2511.04383",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces HPC-Vis, a visual analytic system for exploring historical painter cohorts, which addresses the challenges of diverse artistic styles and complex inheritance relationships among Chinese historical painters. The system utilizes a three-stage reconstruction algorithm to visualize inheritance relationships, establishes a unified artistic style label system, and incorporates a human-computer collaborative interactive exploration mechanism. Case studies and a user study show the effectiveness of HPC-Vis in assisting historians in discovering and validating painter cohorts."
  },
  {
    "title": "Towards Aligning Multimodal LLMs with Human Experts: A Focus on Parent-Child Interaction",
    "abstract": "While multimodal large language models (MLLMs) are increasingly applied in human-centred AI systems, their ability to understand complex social interactions remains uncertain. We present an exploratory study on aligning MLLMs with speech-language pathologists (SLPs) in analysing joint attention in parent-child interactions, a key construct in early social-communicative development. Drawing on interviews and video annotations with three SLPs, we characterise how observational cues of gaze, action, and vocalisation inform their reasoning processes. We then test whether an MLLM can approximate this workflow through a two-stage prompting, separating observation from judgment. Our findings reveal that alignment is more robust at the observation layer, where experts share common descriptors, than at the judgement layer, where interpretive criteria diverge. We position this work as a case-based probe into expert-AI alignment in complex social behaviour, highlighting both the feasibility and the challenges of applying MLLMs to socially situated interaction analysis.",
    "url": "https://arxiv.org/abs/2511.04366",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the alignment of multimodal large language models (MLLMs) with speech-language pathologists (SLPs) in analyzing joint attention in parent-child interactions. The study found that while MLLMs can approximate the observational cues used by experts, there are challenges in aligning their judgment processes. This work highlights the feasibility and challenges of applying MLLMs to socially situated interaction analysis, emphasizing the importance of expert-AI alignment in complex social behavior."
  },
  {
    "title": "Vitessce Link: A Mixed Reality and 2D Display Hybrid Approach for Visual Analysis of 3D Tissue Maps",
    "abstract": "Advances in spatial omics and high-resolution imaging enable the creation of three-dimensional (3D) tissue maps that capture cellular organization and interactions in situ. While these data provide critical insights into tissue function and disease, their exploration is often constrained by tools limited to 2D displays or stereoscopic rendering without analytical integration. We present Vitessce Link, a web-based hybrid framework that unites a 3D stereoscopic view in mixed reality with a synchronized 2D display environment. Users can navigate volumetric data with intuitive hand gestures while controlling channels, filters, and derived data views through the Vitessce platform. Built on open standards and running entirely in the browser, Vitessce Link minimizes friction, supports integration with computational notebooks, and synchronizes interactions across devices via a lightweight WebSocket architecture. Case studies in nephrology and oncology demonstrate how the hybrid approach enhances segmentation evaluation, distance measurement, and interpretation of spatial relationships. Vitessce Link establishes a paradigm for integrative, web-native analysis of 3D tissue maps.",
    "url": "https://arxiv.org/abs/2511.04262",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Vitessce Link, a hybrid framework that combines a 3D stereoscopic view with a synchronized 2D display environment for visual analysis of 3D tissue maps. This approach allows users to navigate volumetric data using hand gestures and control various data views through the Vitessce platform. Case studies in nephrology and oncology demonstrate how this hybrid approach enhances segmentation evaluation, distance measurement, and interpretation of spatial relationships, establishing a paradigm for integrative, web-native analysis of 3D tissue maps."
  },
  {
    "title": "Active Domain Adaptation for mmWave-based HAR via Renyi Entropy-based Uncertainty Estimation",
    "abstract": "Human Activity Recognition (HAR) using mmWave radar provides a non-invasive alternative to traditional sensor-based methods but suffers from domain shift, where model performance declines in new users, positions, or environments. To address this, we propose mmADA, an Active Domain Adaptation (ADA) framework that efficiently adapts mmWave-based HAR models with minimal labeled data. mmADA enhances adaptation by introducing Renyi Entropy-based uncertainty estimation to identify and label the most informative target samples. Additionally, it leverages contrastive learning and pseudo-labeling to refine feature alignment using unlabeled data. Evaluations with a TI IWR1443BOOST radar across multiple users, positions, and environments show that mmADA achieves over 90% accuracy in various cross-domain settings. Comparisons with five baselines confirm its superior adaptation performance, while further tests on unseen users, environments, and two additional open-source datasets validate its robustness and generalization.",
    "url": "https://arxiv.org/abs/2511.04219",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces mmADA, an Active Domain Adaptation framework for Human Activity Recognition using mmWave radar, which addresses the issue of model performance decline in new users, positions, or environments. By incorporating Renyi Entropy-based uncertainty estimation, contrastive learning, and pseudo-labeling, mmADA efficiently adapts HAR models with minimal labeled data and achieves over 90% accuracy in various cross-domain settings. The framework outperforms five baselines and demonstrates robustness and generalization across unseen users, environments, and additional datasets."
  },
  {
    "title": "Graph Neural Networks for User Satisfaction Classification in Human-Computer Interaction",
    "abstract": "This study focuses on the problem of user satisfaction classification and proposes a framework based on graph neural networks to address the limitations of traditional methods in handling complex interaction relationships and multidimensional features. User behaviors, interface elements, and their potential connections are abstracted into a graph structure, and joint modeling of nodes and edges is used to capture semantics and dependencies in the interaction process. Graph convolution and attention mechanisms are introduced to fuse local features and global context, and global pooling with a classification layer is applied to achieve automated satisfaction classification. The method extracts deep patterns from structured data and improves adaptability and robustness in multi-source heterogeneous and dynamic environments. To verify effectiveness, a public user satisfaction survey dataset from Kaggle is used, and results are compared with multiple baseline models across several performance metrics. Experiments show that the method outperforms existing approaches in accuracy, F1-Score, AUC, and Precision, demonstrating the advantage of graph-based modeling in satisfaction prediction tasks. The study not only enriches the theoretical framework of user modeling but also highlights its practical value in optimizing human-computer interaction experience.",
    "url": "https://arxiv.org/abs/2511.04166",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a novel framework using graph neural networks to classify user satisfaction in human-computer interaction, addressing the limitations of traditional methods. By modeling user behaviors and interface elements as a graph structure and incorporating graph convolution and attention mechanisms, the method achieves superior performance compared to existing approaches in accuracy, F1-Score, AUC, and Precision. The study not only advances the theoretical understanding of user modeling but also demonstrates the practical value of graph-based modeling in optimizing human-computer interaction experiences."
  },
  {
    "title": "Scaffolding Metacognition in Programming Education: Understanding Student-AI Interactions and Design Implications",
    "abstract": "Generative AI tools such as ChatGPT now provide novice programmers with unprecedented access to instant, personalized support. While this holds clear promise, their influence on students' metacognitive processes remains underexplored. Existing work has largely focused on correctness and usability, with limited attention to whether and how students' use of AI assistants supports or bypasses key metacognitive processes. This study addresses that gap by analyzing student-AI interactions through a metacognitive lens in university-level programming courses. We examined more than 10,000 dialogue logs collected over three years, complemented by surveys of students and educators. Our analysis focused on how prompts and responses aligned with metacognitive phases and strategies. Synthesizing these findings across data sources, we distill design considerations for AI-powered coding assistants that aim to support rather than supplant metacognitive engagement. Our findings provide guidance for developing educational AI tools that strengthen students' learning processes in programming education.",
    "url": "https://arxiv.org/abs/2511.04144",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores how AI tools like ChatGPT impact the metacognitive processes of novice programmers. By analyzing student-AI interactions in university programming courses, the study found that existing AI assistants may either support or bypass key metacognitive processes. The findings highlight the importance of designing AI-powered coding assistants that enhance students' learning processes in programming education by supporting metacognitive engagement."
  },
  {
    "title": "\"Everyone Else Does It\": The Rise of Preprinting Culture in Computing Disciplines",
    "abstract": "Preprinting has become a norm in fast-paced computing fields such as artificial intelligence (AI) and human-computer interaction (HCI). In this paper, we conducted semistructured interviews with 15 academics in these fields to reveal their motivations and perceptions of preprinting. The results found a close relationship between preprinting and characteristics of the fields, including the huge number of papers, competitiveness in career advancement, prevalence of scooping, and imperfect peer review system - preprinting comes to the rescue in one way or another for the participants. Based on the results, we reflect on the role of preprinting in subverting the traditional publication mode and outline possibilities of a better publication ecosystem. Our study contributes by inspecting the community aspects of preprinting practices through talking to academics.",
    "url": "https://arxiv.org/abs/2511.04081",
    "journal": "arXiv cs.HC",
    "ai_summary": "Preprinting has become common in fast-paced computing fields like AI and HCI, with academics using it to combat issues like a high volume of papers, career competition, scooping, and imperfect peer review. The study highlights the importance of preprinting in these fields and suggests ways to improve the publication ecosystem. The research sheds light on the community aspects of preprinting practices and their impact on traditional publication modes."
  },
  {
    "title": "Revealing AI Reasoning Increases Trust but Crowds Out Unique Human Knowledge",
    "abstract": "Effective human-AI collaboration requires humans to accurately gauge AI capabilities and calibrate their trust accordingly. Humans often have context-dependent private information, referred to as Unique Human Knowledge (UHK), that is crucial for deciding whether to accept or override AI's recommendations. We examine how displaying AI reasoning affects trust and UHK utilization through a pre-registered, incentive-compatible experiment (N = 752). We find that revealing AI reasoning, whether brief or extensive, acts as a powerful persuasive heuristic that significantly increases trust and agreement with AI recommendations. Rather than helping participants appropriately calibrate their trust, this transparency induces over-trust that crowds out UHK utilization. Our results highlight the need for careful consideration when revealing AI reasoning and call for better information design in human-AI collaboration systems.",
    "url": "https://arxiv.org/abs/2511.04050",
    "journal": "arXiv cs.HC",
    "ai_summary": "The study investigates how displaying AI reasoning impacts trust and the utilization of Unique Human Knowledge (UHK) in human-AI collaboration. The research, conducted through an experiment with 752 participants, reveals that revealing AI reasoning increases trust and agreement with AI recommendations, but also leads to over-trust that diminishes the use of UHK. This suggests the importance of thoughtful design in revealing AI reasoning to ensure appropriate trust calibration and effective utilization of human knowledge in AI collaboration systems."
  },
  {
    "title": "Human Resource Management and AI: A Contextual Transparency Database",
    "abstract": "AI tools are proliferating in human resources management (HRM) and recruiting, helping to mediate access to the labor market. As these systems spread, profession-specific transparency needs emerging from black-boxed systems in HRM move into focus. Prior work often frames transparency technically or abstractly, but we contend AI transparency is a social project shaped by materials, meanings, and competencies of practice. This paper introduces the Talent Acquisition and Recruiting AI (TARAI) Index, situating AI systems within the social practice of recruiting by examining product functionality, claims, assumptions, and AI clarity. Built through an iterative, mixed-methods process, the database demonstrates how transparency emerges: not as a fixed property, but as a dynamic outcome shaped by professional practices, interactions, and competencies. By centering social practice, our work offers a grounded, actionable approach to understanding and articulating AI transparency in HR and provides a blueprint for participatory database design for contextual transparency in professional practice.",
    "url": "https://arxiv.org/abs/2511.03916",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper introduces the Talent Acquisition and Recruiting AI (TARAI) Index, which examines AI systems in the context of recruiting by analyzing product functionality, claims, assumptions, and clarity. The study highlights the importance of transparency in AI systems in human resources management, emphasizing that transparency is not a fixed property but a dynamic outcome shaped by professional practices and interactions. By focusing on social practice, the research offers a practical approach to understanding and promoting AI transparency in HR and suggests a participatory database design for contextual transparency in professional practice."
  },
  {
    "title": "SnappyMeal: Design and Longitudinal Evaluation of a Multimodal AI Food Logging Application",
    "abstract": "Food logging, both self-directed and prescribed, plays a critical role in uncovering correlations between diet, medical, fitness, and health outcomes. Through conversations with nutritional experts and individuals who practice dietary tracking, we find current logging methods, such as handwritten and app-based journaling, are inflexible and result in low adherence and potentially inaccurate nutritional summaries. These findings, corroborated by prior literature, emphasize the urgent need for improved food logging methods. In response, we propose SnappyMeal, an AI-powered dietary tracking system that leverages multimodal inputs to enable users to more flexibly log their food intake. SnappyMeal introduces goal-dependent follow-up questions to intelligently seek missing context from the user and information retrieval from user grocery receipts and nutritional databases to improve accuracy. We evaluate SnappyMeal through publicly available nutrition benchmarks and a multi-user, 3-week, in-the-wild deployment capturing over 500 logged food instances. Users strongly praised the multiple available input methods and reported a strong perceived accuracy. These insights suggest that multimodal AI systems can be leveraged to significantly improve dietary tracking flexibility and context-awareness, laying the groundwork for a new class of intelligent self-tracking applications.",
    "url": "https://arxiv.org/abs/2511.03907",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces SnappyMeal, an AI-powered food logging application that utilizes multimodal inputs to improve flexibility and accuracy in tracking dietary intake. The study highlights the limitations of current food logging methods and the need for more effective solutions. The evaluation of SnappyMeal shows promising results, with users praising its multiple input methods and perceived accuracy, suggesting that multimodal AI systems can enhance dietary tracking and potentially lead to the development of more intelligent self-tracking applications."
  },
  {
    "title": "HACI: A Haptic-Audio Code Interface to Improve Educational Outcomes for Visually Impaired Introductory Programming Students",
    "abstract": "This thesis introduces the Haptic-Audio Code Interface (HACI), an educational tool designed to enhance programming education for visually impaired (VI) students by integrating haptic and audio feedback to compensate for the absence of visual cues. HACI consists of a non-resource-intensive web application supporting JavaScript program development, execution, and debugging, connected via a cable to an Arduino-powered glove with six integrated haptic motors to provide physical feedback to VI programmers. Motivated by the need to provide equitable educational opportunities in computer science, HACI aims to improve non-visual code navigation, comprehension, summarizing, editing, and debugging for students with visual impairments while minimizing cognitive load. This work details HACI's design principles, technical implementation, and a preliminary evaluation through a pilot study conducted with undergraduate Computer Science students. Findings indicate that HACI aids in the non-visual navigation and understanding of programming constructs, although challenges remain in refining feedback mechanisms to ensure consistency and reliability, as well as supplementing the current functionality with a more feature-reach and customizable accessible learning experience which will allow visually impaired students to fully utilize interleaved haptic and audio feedback. The study underscores the transformative potential of haptic and audio feedback in educational practices for the visually impaired, setting a foundation for future research and development in accessible programming education. This thesis contributes to the field of accessible technology by demonstrating how tactile and auditory feedback can be effectively integrated into educational tools, thereby broadening accessibility in STEM education.",
    "url": "https://arxiv.org/abs/2511.03733",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces the Haptic-Audio Code Interface (HACI) as a tool to enhance programming education for visually impaired students by providing haptic and audio feedback. The study shows that HACI helps with non-visual code navigation and understanding, but challenges remain in refining feedback mechanisms. This research highlights the potential of haptic and audio feedback in improving educational outcomes for visually impaired students in programming education, laying the foundation for future accessible technology development in STEM education."
  },
  {
    "title": "Conversational Collective Intelligence (CCI) using Hyperchat AI in an Authentic Forecasting Task",
    "abstract": "Hyperchat AI is a novel agentic technology that enables thoughtful conversations among networked human groups of potentially unlimited size. It allows large teams to discuss complex issues, brainstorm ideas, surface risks, assess alternatives and efficiently converge on optimized solutions that amplify the group's Collective Intelligence (CI). A formal study was conducted to quantify the forecasting accuracy of human groups using Hyperchat AI to conversationally predict the outcome of Major League Baseball (MLB) games. During an 8-week period, networked groups of approximately 24 sports fans were tasked with collaboratively forecasting the winners of 59 baseball games through real-time conversation facilitated by AI agents. The results showed that when debating the games using Hyperchat AI technology, the groups converged on High Confidence predictions that significantly outperformed Vegas betting markets. Specifically, groups were 78% accurate in their High Confidence picks, a statistically strong result vs the Vegas odds of 57% (p=0.020). Had the groups bet against the spread (ATS) on these games, they would have achieved a 46% ROI against Vegas betting markets. In addition, High Confidence forecasts that were generated through above-average conversation rates were 88% accurate, suggesting that real-time interactive deliberation is central to amplified accuracy.",
    "url": "https://arxiv.org/abs/2511.03732",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the use of Hyperchat AI technology in facilitating conversations among large groups to improve collective intelligence in forecasting Major League Baseball games. The study found that groups using this technology were able to make more accurate predictions, outperforming Vegas betting markets and achieving a 46% return on investment against the spread. The results highlight the importance of real-time interactive deliberation in enhancing forecasting accuracy."
  },
  {
    "title": "MimiTalk: Revolutionizing Qualitative Research with Dual-Agent AI",
    "abstract": "We present MimiTalk, a dual-agent constitutional AI framework designed for scalable and ethical conversational data collection in social science research. The framework integrates a supervisor model for strategic oversight and a conversational model for question generation. We conducted three studies: Study 1 evaluated usability with 20 participants; Study 2 compared 121 AI interviews to 1,271 human interviews from the MediaSum dataset using NLP metrics and propensity score matching; Study 3 involved 10 interdisciplinary researchers conducting both human and AI interviews, followed by blind thematic analysis. Results across studies indicate that MimiTalk reduces interview anxiety, maintains conversational coherence, and outperforms human interviews in information richness, coherence, and stability. AI interviews elicit technical insights and candid views on sensitive topics, while human interviews better capture cultural and emotional nuances. These findings suggest that dual-agent constitutional AI supports effective human-AI collaboration, enabling replicable, scalable and quality-controlled qualitative research.",
    "url": "https://arxiv.org/abs/2511.03731",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces MimiTalk, a dual-agent AI framework for conversational data collection in social science research. Studies show that MimiTalk reduces interview anxiety, maintains conversational coherence, and outperforms human interviews in terms of information richness, coherence, and stability. The findings suggest that dual-agent AI supports effective human-AI collaboration, enabling replicable, scalable, and quality-controlled qualitative research."
  },
  {
    "title": "Not All Explanations are Created Equal: Investigating the Pitfalls of Current XAI Evaluation",
    "abstract": "Explainable Artificial Intelligence (XAI) aims to create transparency in modern AI models by offering explanations of the models to human users. There are many ways in which researchers have attempted to evaluate the quality of these XAI models, such as user studies or proposed objective metrics like \"fidelity\". However, these current XAI evaluation techniques are ad hoc at best and not generalizable. Thus, most studies done within this field conduct simple user surveys to analyze the difference between no explanations and those generated by their proposed solution. We do not find this to provide adequate evidence that the explanations generated are of good quality since we believe any kind of explanation will be \"better\" in most metrics when compared to none at all. Thus, our study looks to highlight this pitfall: most explanations, regardless of quality or correctness, will increase user satisfaction. We also propose that emphasis should be placed on actionable explanations. We demonstrate the validity of both of our claims using an agent assistant to teach chess concepts to users. The results of this chapter will act as a call to action in the field of XAI for more comprehensive evaluation techniques for future research in order to prove explanation quality beyond user satisfaction. Additionally, we present an analysis of the scenarios in which placebic or actionable explanations would be most useful.",
    "url": "https://arxiv.org/abs/2511.03730",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research investigates the pitfalls of current evaluation techniques for Explainable Artificial Intelligence (XAI) models, which aim to provide transparency in AI decision-making. The study highlights that current evaluation methods, such as user surveys, may not accurately assess the quality of explanations generated by XAI models. The research emphasizes the need for more comprehensive evaluation techniques that focus on actionable explanations rather than simply increasing user satisfaction, as demonstrated through an agent assistant teaching chess concepts."
  },
  {
    "title": "Beyond Chat: a Framework for LLMs as Human-Centered Support Systems",
    "abstract": "Large language models are moving beyond transactional question answering to act as companions, coaches, mediators, and curators that scaffold human growth, decision-making, and well-being. This paper proposes a role-based framework for human-centered LLM support systems, compares real deployments across domains, and identifies cross-cutting design principles: transparency, personalization, guardrails, memory with privacy, and a balance of empathy and reliability. It outlines evaluation metrics that extend beyond accuracy to trust, engagement, and longitudinal outcomes. It also analyzes risks including over-reliance, hallucination, bias, privacy exposure, and unequal access, and proposes future directions spanning unified evaluation, hybrid human-AI models, memory architectures, cross-domain benchmarking, and governance. The goal is to support responsible integration of LLMs in sensitive settings where people need accompaniment and guidance, not only answers.",
    "url": "https://arxiv.org/abs/2511.03729",
    "journal": "arXiv cs.HC",
    "ai_summary": "This paper introduces a framework for using large language models (LLMs) as human-centered support systems, expanding their role beyond question answering to provide companionship, coaching, mediation, and curation for human growth and decision-making. The framework emphasizes key design principles such as transparency, personalization, and empathy, and proposes evaluation metrics that go beyond accuracy to include trust, engagement, and long-term outcomes. The research also highlights potential risks of using LLMs, such as over-reliance, bias, and privacy concerns, and suggests future directions for integrating LLMs responsibly in sensitive settings."
  },
  {
    "title": "Efficient On-Device Agents via Adaptive Context Management",
    "abstract": "On-device AI agents offer the potential for personalized, low-latency assistance, but their deployment is fundamentally constrained by limited memory capacity, which restricts usable context. This reduced practical context window creates a trade-off between supporting rich, stateful interactions with complex tool capabilities and maintaining on-device feasibility. We break this trade-off with a framework for context-efficient on-device agents, driven by three synergistic optimizations (1) a dynamic memory system using specialized LoRA adapters to distill conversational history into a compressed, and structured Context State Object; (2) a minimalist serialization format for tool schemas to minimize token overhead per tool; and (3) a just-in-time schema-passing mechanism that loads full tool definitions only upon tool selection. We instantiate this framework by adapting a 3B parameter SLM to context-efficient trajectories and rigorously evaluate it against a conventional baseline on complex user tasks. Our agent matches, or exceeds, the performance of a conventional baseline while dramatically compressing context, achieving more than a 6-fold reduction in initial system prompt context and a 10- to 25-fold reduction in context growth rate based on the interaction verbosity, demonstrating that strategic context management is key to unlocking capable and persistent on-device AI.",
    "url": "https://arxiv.org/abs/2511.03728",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a framework for creating context-efficient on-device AI agents by optimizing memory usage and tool schema serialization. The framework allows for rich, stateful interactions with complex tool capabilities while maintaining on-device feasibility. The results show that the agent's performance matches or exceeds that of a conventional baseline while significantly reducing context size, demonstrating the importance of strategic context management in developing capable and persistent on-device AI."
  },
  {
    "title": "MazeMate: An LLM-Powered Chatbot to Support Computational Thinking in Gamified Programming Learning",
    "abstract": "Computational Thinking (CT) is a foundational problem-solving skill, and gamified programming environments are a widely adopted approach to cultivating it. While large language models (LLMs) provide on-demand programming support, current applications rarely foster CT development. We present MazeMate, an LLM-powered chatbot embedded in a 3D Maze programming game, designed to deliver adaptive, context-sensitive scaffolds aligned with CT processes in maze solving and maze design. We report on the first classroom implementation with 247 undergraduates. Students rated MazeMate as moderately helpful, with higher perceived usefulness for maze solving than for maze design. Thematic analysis confirmed support for CT processes such as decomposition, abstraction, and algorithmic thinking, while also revealing limitations in supporting maze design, including mismatched suggestions and fabricated algorithmic solutions. These findings demonstrate the potential of LLM-based scaffolding to support CT and underscore directions for design refinement to enhance MazeMate usability in authentic classrooms.",
    "url": "https://arxiv.org/abs/2511.03727",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces MazeMate, a chatbot powered by large language models (LLMs) embedded in a 3D Maze programming game to support computational thinking (CT) development. The study conducted with 247 undergraduates showed that MazeMate was moderately helpful, with higher usefulness for maze solving than for maze design. While MazeMate supported CT processes such as decomposition and algorithmic thinking, there were limitations in supporting maze design, indicating the potential of LLM-based scaffolding for CT development and suggesting areas for design refinement in authentic classroom settings."
  },
  {
    "title": "GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction",
    "abstract": "Humanoid robots are expected to operate in human-centered environments where safe and natural physical interaction is essential. However, most recent reinforcement learning (RL) policies emphasize rigid tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance. At its core is a unified spring-based formulation that models both resistive contacts (restoring forces when pressing against surfaces) and guiding contacts (pushes or pulls sampled from human motion data). This formulation ensures kinematically consistent forces across the shoulder, elbow, and wrist, while exposing the policy to diverse interaction scenarios. Safety is further supported through task-adjustable force thresholds. We evaluate our approach in both simulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and safe object manipulation. Compared to baselines, our policy consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments.",
    "url": "https://arxiv.org/abs/2511.04679",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance in humanoid robots. This allows for safer and more natural physical interactions with humans and objects, as well as diverse interaction scenarios. The evaluation of the approach shows a reduction in peak contact forces while maintaining task success, demonstrating the potential for humanoid robots to effectively collaborate with humans and handle objects in real-world environments."
  },
  {
    "title": "Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis",
    "abstract": "Natural language interfaces to tabular data must handle ambiguities inherent to queries. Instead of treating ambiguity as a deficiency, we reframe it as a feature of cooperative interaction, where the responsibility of query specification is shared among the user and the system. We develop a principled framework distinguishing cooperative queries, i.e., queries that yield a resolvable interpretation, from uncooperative queries that cannot be resolved. Applying the framework to evaluations for tabular question answering and analysis, we analyze the queries in 15 popular datasets, and observe an uncontrolled mixing of query types neither adequate for evaluating a system's execution accuracy nor for evaluating interpretation capabilities. Our framework and analysis of queries shifts the perspective from fixing ambiguity to embracing cooperation in resolving queries. This reflection enables more informed design and evaluation for natural language interfaces for tabular data, for which we outline implications and directions for future research.",
    "url": "https://arxiv.org/abs/2511.04584",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the ambiguity in natural language queries for tabular data analysis and reframes it as a feature of cooperative interaction between users and systems. The development of a framework distinguishes between cooperative and uncooperative queries, highlighting the importance of shared responsibility in query specification. The analysis of popular datasets reveals a mixing of query types that are not suitable for evaluating system accuracy or interpretation capabilities, suggesting a shift towards embracing cooperation in resolving queries for more informed design and evaluation of natural language interfaces for tabular data."
  },
  {
    "title": "The Psychogeography of Imaginary Places",
    "abstract": "Psychogeography -- the study of how environments shape emotion and behaviour -- has long concerned itself with the emotional resonance of the physical, often through the idea of the derive through the city. Its philosophical core, however, is primarily concerned with identifying affective relationships between the personal and the environmental, and this does not require the constraint of concrete.\nThis paper extends psychogeographical practice into the realm of the imaginary, proposing a psychogeography of virtual and fictive spaces. Drawing on literary, Situationist, and contemporary psychogeographical traditions, we examine how the derive might operate within the elastic spatiality and temporalities of video game worlds. We argue that digital environments, being wholly constructed, invite new forms of meaning-making and self-reflection. Through this reframing, games become both laboratory and landscape for a revitalised psychogeography: one attuned not only to the spirits of streets and cities, but also to the ghosts that haunt code, pixels, and play.",
    "url": "https://arxiv.org/abs/2511.04105",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores the concept of psychogeography in relation to virtual and fictive spaces, specifically within video game worlds. By examining how emotions and behaviors are shaped in these digital environments, the study argues that games offer new opportunities for self-reflection and meaning-making. The paper suggests that by expanding psychogeographical practice to include imaginary spaces, a deeper understanding of the relationship between individuals and their environments can be achieved."
  },
  {
    "title": "Multi-Agent Collaborative Framework For Math Problem Generation",
    "abstract": "Automatic question generation (AQG) for mathematics education remains an elusive goal for Intelligent Tutoring Systems and educators. While pre-trained transformer-based language models have significantly advanced natural language generation, they often struggle to precisely control problem complexity and cognitive demands. In this paper, we introduce a collaborative multi-agent framework as a novel method of incorporating inference-time computation into AQG. This approach leverages multiple agents that iteratively refine generated question-answer pairs to better balance complexity and cognitive demand. We evaluate the generated questions on five meta-evaluation criteria: relevance, importance, clarity, difficulty matching, answerability, to assess the system's ability to control the required complexity and quality of the questions. Preliminary evaluations show that this collaborative multi-agent framework elevates the quality of generated educational content by fostering a more nuanced balance between cognitive challenge and clarity. These promising outcomes suggest that integrating collaborative multi-agent workflows can yield more controlled, pedagogically valuable content that can help advance automated educational content generation and adaptive learning environments.",
    "url": "https://arxiv.org/abs/2511.03958",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a collaborative multi-agent framework for automatic question generation in mathematics education, aiming to improve the balance of complexity and cognitive demand in generated questions. The framework leverages multiple agents to iteratively refine question-answer pairs, resulting in higher quality educational content. Preliminary evaluations show that this approach enhances the control of question complexity and clarity, suggesting that integrating collaborative multi-agent workflows can advance automated educational content generation and adaptive learning environments."
  },
  {
    "title": "Extracting Causal Relations in Deep Knowledge Tracing",
    "abstract": "A longstanding goal in computational educational research is to develop explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which leverages a Recurrent Neural Network (RNN) to predict student knowledge and performance on exercises, has been proposed as a major advancement over traditional KT methods. Several studies suggest that its performance gains stem from its ability to model bidirectional relationships between different knowledge components (KCs) within a course, enabling the inference of a student's understanding of one KC from their performance on others. In this paper, we challenge this prevailing explanation and demonstrate that DKT's strength lies in its implicit ability to model prerequisite relationships as a causal structure, rather than bidirectional relationships. By pruning exercise relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal subsets of the Assistments dataset, we show that DKT's predictive capabilities align strongly with these causal structures. Furthermore, we propose an alternative method for extracting exercise relation DAGs using DKT's learned representations and provide empirical evidence supporting our claim. Our findings suggest that DKT's effectiveness is largely driven by its capacity to approximate causal dependencies between KCs rather than simple relational mappings.",
    "url": "https://arxiv.org/abs/2511.03948",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research challenges the prevailing explanation that Deep Knowledge Tracing (DKT) gains performance from modeling bidirectional relationships between knowledge components (KCs) and demonstrates that its strength lies in modeling prerequisite relationships as a causal structure. By pruning exercise relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal subsets of the Assistments dataset, the study shows that DKT's predictive capabilities align strongly with these causal structures. The findings suggest that DKT's effectiveness is driven by its ability to approximate causal dependencies between KCs rather than simple relational mappings, highlighting the significance of understanding causal relations in educational AI models."
  },
  {
    "title": "OriFeel: Origami-Inspired Actuation for Force-Based Tactile Feedback on Ambient Surfaces",
    "abstract": "People are constantly in touch with surfaces in their lives, such as a sofa, armrest, and table, making them natural tactile interfaces. Despite the recent advancements in shape-changing surfaces, current available solutions are often challenging to retrofit into ambient surfaces due to their bulky form factor or high power requirements. We present \\name, a foldable structure-enabled tactile feedback mechanism that leverages the structural properties of Miura-Ori fold to enable on-surface force actuation. The foldable structure allows the surfaces to provide perpendicular force via lateral actuation, resulting in a slim form factor that can be actuated via cable-based design using a servo motor. We evaluate the system with a real-world prototype and a user study. The user study shows that users can effectively distinguish multiple intensity levels.",
    "url": "https://arxiv.org/abs/2511.03673",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces OriFeel, a foldable structure-enabled tactile feedback mechanism inspired by origami that allows ambient surfaces to provide force-based tactile feedback. This innovative design enables slim form factor and lateral actuation, making it easier to retrofit into existing surfaces. The user study demonstrates that users can effectively distinguish between multiple intensity levels, highlighting the potential impact of OriFeel in enhancing tactile interactions with ambient surfaces."
  },
  {
    "title": "Knowledge Graph for Intelligent Generation of Artistic Image Creation: Constructing a New Annotation Hierarchy",
    "abstract": "Our study aims to establish a unified, systematic, and referable knowledge framework for the annotation of art image datasets, addressing issues of ambiguous definitions and inconsistent results caused by the lack of common standards during the annotation process. To achieve this goal, a hierarchical and systematic art image knowledge graph was constructed. It was developed based on the composition principles of art images, incorporating the Structured Theory of Visual Knowledge proposed by Academician Yunhe Pan in On Visual Knowledge-which states that visual knowledge must achieve precise expression of spatial forms and dynamic relationships through \"prototype-category\" and \"hierarchical structure\". Through in-depth review of Chinese and Western art theories and pioneering integration of the Chinese cultural perspective, this graph took shape. The core visual language of art images was deconstructed by this knowledge graph. Meanwhile, the unique spatial theory and symbolic system of Chinese painting were compared with and supplemented by Western art theories. This graph converts qualitative artistic concepts into a clear structured framework. It not only conforms to the cognitive law that \"visual knowledge takes precedence over verbal knowledge\" in humans but also provides an interpretable and inferential visual knowledge foundation for AI art generation and cross-cultural art analysis. It ensures the high quality and consistency of annotated data, thus offering key support for art intelligence research in the AI 2.0 era.",
    "url": "https://arxiv.org/abs/2511.03585",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research establishes a comprehensive knowledge framework for annotating art image datasets, addressing issues of ambiguity and inconsistency in annotations. A hierarchical and systematic art image knowledge graph was constructed based on composition principles and visual knowledge theories, incorporating both Chinese and Western art perspectives. This graph provides a structured framework for converting qualitative artistic concepts into clear visual knowledge, supporting AI art generation and cross-cultural art analysis in the AI 2.0 era."
  },
  {
    "title": "PnPSelect: Plug-and-play IoT Device Selection Using Ultra-wideband Signals",
    "abstract": "In recent years, the number of Internet of Things (IoT) devices in smart homes has rapidly increased. A key challenge affecting user experience is how to enable users to efficiently and intuitively select the devices they wish to control. This paper proposes PnPSelect, a plug-and-play IoT device selection solution utilizing Ultra-wideband (UWB) technology on commercial devices. Unlike previous works, PnPSelect does not require the installation of dedicated hardware on each IoT device, thereby reducing deployment costs and complexities, and achieving true plug-and-play functionality. To enable intuitive device selection, we introduce a pointing direction estimation method that utilizes UWB readings from a single anchor to infer the user pointing direction. Additionally, we propose a lightweight device localization method that allows users to register new IoT devices by simply pointing at them from two distinct positions, eliminating the need for manual measurements. We implement PnPSelect on commercial smartphones and smartwatches and conduct extensive evaluations in both controlled laboratory settings and real-world environments. Our results demonstrate high accuracy, robustness, and adaptability, making PnPSelect a practical and scalable solution for next-generation smart home interactions.",
    "url": "https://arxiv.org/abs/2511.03534",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research paper proposes PnPSelect, a plug-and-play IoT device selection solution using Ultra-wideband (UWB) technology, which eliminates the need for dedicated hardware on each device, reducing costs and complexities. The method includes a pointing direction estimation and lightweight device localization, allowing users to easily select and register new devices by simply pointing at them from two positions. The implementation on commercial devices showed high accuracy, robustness, and adaptability, making PnPSelect a practical and scalable solution for enhancing smart home interactions."
  },
  {
    "title": "SVG Decomposition for Enhancing Large Multimodal Models Visualization Comprehension: A Study with Floor Plans",
    "abstract": "Large multimodal models (LMMs) are increasingly capable of interpreting visualizations, yet they continue to struggle with spatial reasoning. One proposed strategy is decomposition, which breaks down complex visualizations into structured components. In this work, we examine the efficacy of scalable vector graphics (SVGs) as a decomposition strategy for improving LMMs' performance on floor plans comprehension. Floor plans serve as a valuable testbed because they combine geometry, topology, and semantics, and their reliable comprehension has real-world applications, such as accessibility for blind and low-vision individuals. We conducted an exploratory study with three LMMs (GPT-4o, Claude 3.7 Sonnet, and Llama 3.2 11B Vision Instruct) across 75 floor plans. Results show that combining SVG with raster input (SVG+PNG) improves performance on spatial understanding tasks but often hinders spatial reasoning, particularly in pathfinding. These findings highlight both the promise and limitations of decomposition as a strategy for advancing spatial visualization comprehension.",
    "url": "https://arxiv.org/abs/2511.03478",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of scalable vector graphics (SVGs) as a decomposition strategy to enhance the performance of large multimodal models (LMMs) in understanding floor plans. The study found that combining SVG with raster input (SVG+PNG) improved spatial understanding tasks but hindered spatial reasoning, particularly in pathfinding. These findings demonstrate the potential of decomposition strategies in improving spatial visualization comprehension, while also highlighting some limitations in certain tasks."
  },
  {
    "title": "Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond",
    "abstract": "As the \"agentic web\" takes shape-billions of AI agents (often LLM-powered) autonomously transacting and collaborating-trust shifts from human oversight to protocol design. In 2025, several inter-agent protocols crystallized this shift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2), and Ethereum's ERC-8004 \"Trustless Agents,\" yet their underlying trust assumptions remain under-examined. This paper presents a comparative study of trust models in inter-agent protocol design: Brief (self- or third-party verifiable claims), Claim (self-proclaimed capabilities and identity, e.g. AgentCard), Proof (cryptographic verification, including zero-knowledge proofs and trusted execution environment attestations), Stake (bonded collateral with slashing and insurance), Reputation (crowd feedback and graph-based trust signals), and Constraint (sandboxing and capability bounding). For each, we analyze assumptions, attack surfaces, and design trade-offs, with particular emphasis on LLM-specific fragilities-prompt injection, sycophancy/nudge-susceptibility, hallucination, deception, and misalignment-that render purely reputational or claim-only approaches brittle. Our findings indicate no single mechanism suffices. We argue for trustless-by-default architectures anchored in Proof and Stake to gate high-impact actions, augmented by Brief for identity and discovery and Reputation overlays for flexibility and social signals. We comparatively evaluate A2A, AP2, ERC-8004 and related historical variations in academic research under metrics spanning security, privacy, latency/cost, and social robustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid trust model recommendations that mitigate reputation gaming and misinformed LLM behavior, and we distill actionable design guidelines for safer, interoperable, and scalable agent economies.",
    "url": "https://arxiv.org/abs/2511.03434",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper examines different trust models in inter-agent protocol design for the agentic web, where AI agents autonomously transact and collaborate. The study compares trust models such as Brief, Claim, Proof, Stake, Reputation, and Constraint, highlighting the importance of combining Proof and Stake mechanisms to mitigate vulnerabilities and ensure security in agent interactions. The findings suggest that a hybrid trust model approach, incorporating various mechanisms, is necessary for creating safe, interoperable, and scalable agent economies in the future."
  },
  {
    "title": "I Prompt, it Generates, we Negotiate. Exploring Text-Image Intertextuality in Human-AI Co-Creation of Visual Narratives with VLMs",
    "abstract": "Creating meaningful visual narratives through human-AI collaboration requires understanding how text-image intertextuality emerges when textual intentions meet AI-generated visuals. We conducted a three-phase qualitative study with 15 participants using GPT-4o to investigate how novices navigate sequential visual narratives. Our findings show that users develop strategies to harness AI's semantic surplus by recognizing meaningful visual content beyond literal descriptions, iteratively refining prompts, and constructing narrative significance through complementary text-image relationships. We identified four distinct collaboration patterns and, through fsQCA's analysis, discovered three pathways to successful intertextual collaboration: Educational Collaborator, Technical Expert, and Visual Thinker. However, participants faced challenges, including cultural representation gaps, visual consistency issues, and difficulties translating narrative concepts into visual prompts. These findings contribute to HCI research by providing an empirical account of \\textit{text-image intertextuality} in human-AI co-creation and proposing design implications for role-based AI assistants that better support iterative, human-led creative processes in visual storytelling.",
    "url": "https://arxiv.org/abs/2511.03375",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores how human-AI collaboration can create visual narratives by studying how novices interact with AI-generated visuals using GPT-4o. The study found that users develop strategies to utilize AI's semantic surplus, refine prompts, and create meaningful text-image relationships in narratives. The findings suggest different collaboration patterns and pathways to successful intertextual collaboration, highlighting the importance of role-based AI assistants in supporting human-led creative processes in visual storytelling."
  },
  {
    "title": "When Generative Artificial Intelligence meets Extended Reality: A Systematic Review",
    "abstract": "With the continuous advancement of technology, the application of generative artificial intelligence (AI) in various fields is gradually demonstrating great potential, particularly when combined with Extended Reality (XR), creating unprecedented possibilities. This survey article systematically reviews the applications of generative AI in XR, covering as much relevant literature as possible from 2023 to 2025. The application areas of generative AI in XR and its key technology implementations are summarised through PRISMA screening and analysis of the final 26 articles. The survey highlights existing articles from the last three years related to how XR utilises generative AI, providing insights into current trends and research gaps. We also explore potential opportunities for future research to further empower XR through generative AI, providing guidance and information for future generative XR research.",
    "url": "https://arxiv.org/abs/2511.03282",
    "journal": "arXiv cs.HC",
    "ai_summary": "This systematic review explores the applications of generative artificial intelligence (AI) in Extended Reality (XR) from 2023 to 2025. The study identifies key technology implementations and application areas of generative AI in XR through analysis of 26 relevant articles. The findings offer insights into current trends, research gaps, and potential opportunities for future research to enhance XR through generative AI."
  },
  {
    "title": "Node-Based Editing for Multimodal Generation of Text, Audio, Image, and Video",
    "abstract": "We present a node-based storytelling system for multimodal content generation. The system represents stories as graphs of nodes that can be expanded, edited, and iteratively refined through direct user edits and natural-language prompts. Each node can integrate text, images, audio, and video, allowing creators to compose multimodal narratives. A task selection agent routes between specialized generative tasks that handle story generation, node structure reasoning, node diagram formatting, and context generation. The interface supports targeted editing of individual nodes, automatic branching for parallel storylines, and node-based iterative refinement. Our results demonstrate that node-based editing supports control over narrative structure and iterative generation of text, images, audio, and video. We report quantitative outcomes on automatic story outline generation and qualitative observations of editing workflows. Finally, we discuss current limitations such as scalability to longer narratives and consistency across multiple nodes, and outline future work toward human-in-the-loop and user-centered creative AI tools.",
    "url": "https://arxiv.org/abs/2511.03227",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces a node-based storytelling system for creating multimodal content, allowing users to edit and refine stories through direct edits and natural-language prompts. Each node in the system can incorporate text, images, audio, and video, enabling creators to compose complex narratives. The study shows that node-based editing provides control over narrative structure and supports iterative generation of multimodal content, with future work focusing on improving scalability and consistency in longer narratives."
  },
  {
    "title": "Large Language Models as Information Sources: Distinctive Characteristics and Types of Low-Quality Information",
    "abstract": "Recent advances in large language models (LLMs) have brought public and scholarly attention to their potential in generating low-quality information. While widely acknowledged as a risk, low-quality information remains a vaguely defined concept, and little is known about how it manifests in LLM outputs or how these outputs differ from those of traditional information sources. In this study, we focus on two key questions: What types of low-quality information are produced by LLMs, and what makes them distinct than human-generated counterparts? We conducted focus groups with public health professionals and individuals with lived experience in three critical health contexts (vaccines, opioid use disorder, and intimate partner violence) where high-quality information is essential and misinformation, bias, and insensitivity are prevalent concerns. We identified a typology of LLM-generated low-quality information and a set of distinctive LLM characteristics compared to traditional information sources. Our findings show that low-quality information extends beyond factual inaccuracies into types such as misprioritization and exaggeration, and that LLM affordances fundamentally differs from previous technologies. This work offers typologies on LLM distinctive characteristics and low-quality information types as a starting point for future efforts to understand LLM-generated low-quality information and mitigate related informational harms. We call for conceptual and methodological discussions of information quality to move beyond truthfulness, in order to address the affordances of emerging technologies and the evolving dynamics of information behaviors.",
    "url": "https://arxiv.org/abs/2511.03198",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study examines the types of low-quality information produced by large language models (LLMs) and how they differ from human-generated information. Through focus groups with public health professionals, the researchers identified a typology of LLM-generated low-quality information, including misprioritization and exaggeration. The findings highlight the need to expand discussions on information quality beyond truthfulness to address the unique characteristics of LLMs and mitigate potential harms associated with misinformation."
  },
  {
    "title": "AI as We Describe It: How Large Language Models and Their Applications in Health are Represented Across Channels of Public Discourse",
    "abstract": "Representation shapes public attitudes and behaviors. With the arrival and rapid adoption of LLMs, the way these systems are introduced will negotiate societal expectations for their role in high-stakes domains like health. Yet it remains unclear whether current narratives present a balanced view. We analyzed five prominent discourse channels (news, research press, YouTube, TikTok, and Reddit) over a two-year period on lexical style, informational content, and symbolic representation. Discussions were generally positive and episodic, with positivity increasing over time. Risk communication was unthorough and often reduced to information quality incidents, while explanations of LLMs' generative nature were rare. Compared with professional outlets, TikTok and Reddit highlighted wellbeing applications and showed greater variations in tone and anthropomorphism but little attention to risks. We discuss implications for public discourse as a diagnostic tool in identifying literacy and governance gaps, and for communication and design strategies to support more informed LLM engagement.",
    "url": "https://arxiv.org/abs/2511.03174",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores how large language models (LLMs) and their applications in health are represented across various public discourse channels. The study found that discussions were generally positive and episodic, with positivity increasing over time. However, there was a lack of thorough risk communication and explanations of LLMs' generative nature, with TikTok and Reddit emphasizing wellbeing applications but showing little attention to risks. This research highlights the importance of public discourse in identifying literacy and governance gaps, and suggests communication and design strategies to support more informed engagement with LLMs."
  }
]