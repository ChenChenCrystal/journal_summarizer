[
  {
    "title": "Interactive Narrative Analytics: Bridging Computational Narrative Extraction and Human Sensemaking",
    "abstract": "Information overload and misinformation create significant challenges in extracting meaningful narratives from large news collections. This paper defines the nascent field of Interactive Narrative Analytics (INA), which combines computational narrative extraction with interactive visual analytics to support sensemaking. INA approaches enable the interactive exploration of narrative structures through computational methods and visual interfaces that facilitate human interpretation. The field faces challenges in scalability, interactivity, knowledge integration, and evaluation standardization, yet offers promising opportunities across news analysis, intelligence, scientific literature exploration, and social media analysis. Through the combination of computational and human insight, INA addresses complex challenges in narrative sensemaking.",
    "url": "https://arxiv.org/abs/2601.11459",
    "journal": "arXiv cs.HC",
    "ai_summary": "The paper introduces the field of Interactive Narrative Analytics (INA), which combines computational narrative extraction with interactive visual analytics to help make sense of large news collections. INA allows for interactive exploration of narrative structures through computational methods and visual interfaces, offering promising opportunities in news analysis, intelligence, scientific literature exploration, and social media analysis. Despite facing challenges in scalability, interactivity, knowledge integration, and evaluation standardization, INA shows potential in addressing complex challenges in narrative sensemaking by combining computational and human insight."
  },
  {
    "title": "Sociotechnical Challenges of Machine Learning in Healthcare and Social Welfare",
    "abstract": "Sociotechnical challenges of machine learning in healthcare and social welfare are mismatches between how a machine learning tool functions and the structure of care practices. While prior research has documented many such issues, existing accounts often attribute them either to designers' limited social understanding or to inherent technical constraints, offering limited support for systematic description and comparison across settings. In this paper, we present a framework for conceptualizing sociotechnical challenges of machine learning grounded in qualitative fieldwork, a review of longitudinal deployment studies, and co-design workshops with healthcare and social welfare practitioners. The framework comprises (1) a categorization of eleven sociotechnical challenges organized along an ML-enabled care pathway, and (2) a process-oriented account of the conditions through which these challenges emerge across design and use. By providing a parsimonious vocabulary and an explanatory lens focused on practice, this work supports more precise analysis of how machine learning tools function and malfunction within real-world care delivery.",
    "url": "https://arxiv.org/abs/2601.11417",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores the sociotechnical challenges of implementing machine learning in healthcare and social welfare settings. The study presents a framework that categorizes eleven key challenges along an ML-enabled care pathway, providing a systematic way to analyze and compare issues across different settings. By focusing on the practical implications of machine learning tools, this framework helps to better understand how they function and malfunction in real-world care delivery, offering valuable insights for improving their effectiveness and integration."
  },
  {
    "title": "Show me the evidence: Evaluating the role of evidence and natural language explanations in AI-supported fact-checking",
    "abstract": "Although much research has focused on AI explanations to support decisions in complex information-seeking tasks such as fact-checking, the role of evidence is surprisingly under-researched. In our study, we systematically varied explanation type, AI prediction certainty, and correctness of AI system advice for non-expert participants, who evaluated the veracity of claims and AI system predictions. Participants were provided the option of easily inspecting the underlying evidence. We found that participants consistently relied on evidence to validate AI claims across all experimental conditions. When participants were presented with natural language explanations, evidence was used less frequently although they relied on it when these explanations seemed insufficient or flawed. Qualitative data suggests that participants attempted to infer evidence source reliability, despite source identities being deliberately omitted. Our results demonstrate that evidence is a key ingredient in how people evaluate the reliability of information presented by an AI system and, in combination with natural language explanations, offers valuable support for decision-making. Further research is urgently needed to understand how evidence ought to be presented and how people engage with it in practice.",
    "url": "https://arxiv.org/abs/2601.11387",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores the role of evidence and natural language explanations in AI-supported fact-checking. The research found that participants consistently relied on evidence to validate AI claims, even when presented with natural language explanations. The results highlight the importance of evidence in evaluating the reliability of information from AI systems and suggest the need for further research on how evidence should be presented and utilized in practice."
  },
  {
    "title": "Human Factors in Immersive Analytics",
    "abstract": "It has been ten years since the term ''Immersive Analytics'' (IA) was coined and research interest in the topic remains strong. Researchers in this field have produced practical and conceptual knowledge concerning the use of emerging immersive spatial display and interaction technologies for sense-making tasks through a number of papers, surveys, and books. However, a lack of truly physically and psychologically ergonomic techniques, as well as standardized human-centric validation protocols for these, remains a significant barrier to wider acceptance of practical IA systems in ubiquitous applications. Building upon a series of workshops on immersive analytics at various conferences, this workshop aims to explore new approaches and establish standard practices for evaluating immersive analytics systems from a human factors perspective. We will gather immersive analytics researchers and practitioners to look closely at these human factors -- including cognitive and physical functions as well as behaviour and performance -- to see how they inform the design and deployment of immersive analytics techniques and applications and to inform future research.",
    "url": "https://arxiv.org/abs/2601.11365",
    "journal": "arXiv cs.HC",
    "ai_summary": "The abstract discusses the ongoing research interest in Immersive Analytics (IA) and the need for standardized human-centric validation protocols for these systems. Despite advancements in immersive spatial display and interaction technologies, there is a lack of truly ergonomic techniques, hindering wider acceptance of IA in practical applications. The workshop aims to address this issue by exploring new approaches and establishing standard practices for evaluating IA systems from a human factors perspective, with a focus on cognitive and physical functions, behavior, and performance to inform future research and development in this field."
  },
  {
    "title": "ProjecTA: A Semi-Humanoid Robotic Teaching Assistant with In-Situ Projection for Guided Tours",
    "abstract": "Robotic teaching assistants (TAs) often use body-mounted screens to deliver content. In nomadic, walk-and-talk learning, such as tours in makerspaces, these screens can distract learners from real-world objects, increasing extraneous cognitive load. HCI research lacks empirical comparisons of potential alternatives, such as robots with in-situ projection versus screen-based counterparts; little knowledge has been derived for designing such alternatives. We introduce ProjecTA, a semi-humanoid, gesture-capable TA that guides learners while projecting near-object overlays coordinated with speech and gestures. In a mixed-method study (N=24) in a university makerspace, ProjecTA significantly reduced extraneous load and outperformed its screen-based counterpart in perceived usability, usefulness of visual display, and cross-modal complementarity. Qualitative analyses revealed how ProjecTA's coordinated projections, gestures, and speech anchored explanations in place and time, enhancing understanding in ways a screen could not. We derive key design implications for future robotic TAs leveraging spatial projection to support mobile learning in physical environments.",
    "url": "https://arxiv.org/abs/2601.11328",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces ProjecTA, a semi-humanoid robotic teaching assistant that uses in-situ projection to guide learners during tours in makerspaces. The study found that ProjecTA significantly reduced extraneous cognitive load and outperformed screen-based counterparts in perceived usability and usefulness of visual display. The coordinated projections, gestures, and speech of ProjecTA enhanced understanding by anchoring explanations in place and time, providing key design implications for future robotic TAs in physical environments."
  },
  {
    "title": "Seek and You Shall Find: Design & Evaluation of a Context-Aware Interactive Search Companion",
    "abstract": "Many users struggle with effective online search and critical evaluation, especially in high-stakes domains like health, while often overestimating their digital literacy. Thus, in this demo, we present an interactive search companion that seamlessly integrates expert search strategies into existing search engine result pages. Providing context-aware tips on clarifying information needs, improving query formulation, encouraging result exploration, and mitigating biases, our companion aims to foster reflective search behaviour while minimising cognitive burden. A user study demonstrates the companion's successful encouragement of more active and exploratory search, leading users to submit 75 % more queries and view roughly twice as many results, as well as performance gains in difficult tasks. This demo illustrates how lightweight, contextual guidance can enhance search literacy and empower users through micro-learning opportunities. While the vision involves real-time LLM adaptivity, this study utilises a controlled implementation to test the underlying intervention strategies.",
    "url": "https://arxiv.org/abs/2601.11287",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research presents an interactive search companion designed to improve users' search behavior by integrating expert strategies into search engine result pages. The companion provides context-aware tips to help users clarify their information needs, formulate better queries, explore results, and mitigate biases, leading to more active and exploratory search behavior. A user study showed that the companion increased the number of queries submitted by 75% and the number of results viewed by roughly twice as much, indicating its effectiveness in enhancing search literacy and empowering users through micro-learning opportunities."
  },
  {
    "title": "\"Can You Tell Me?\": Designing Copilots to Support Human Judgement in Online Information Seeking",
    "abstract": "Generative AI (GenAI) tools are transforming information seeking, but their fluent, authoritative responses risk overreliance and discourage independent verification and reasoning. Rather than replacing the cognitive work of users, GenAI systems should be designed to support and scaffold it. Therefore, this paper introduces an LLM-based conversational copilot designed to scaffold information evaluation rather than provide answers and foster digital literacy skills. In a pre-registered, randomised controlled trial (N=261) examining three interface conditions including a chat-based copilot, our mixed-methods analysis reveals that users engaged deeply with the copilot, demonstrating metacognitive reflection. However, the copilot did not significantly improve answer correctness or search engagement, largely due to a \"time-on-chat vs. exploration\" trade-off and users' bias toward positive information. Qualitative findings reveal tension between the copilot's Socratic approach and users' desire for efficiency. These results highlight both the promise and pitfalls of pedagogical copilots, and we outline design pathways to reconcile literacy goals with efficiency demands.",
    "url": "https://arxiv.org/abs/2601.11284",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the design of a conversational copilot using Generative AI to support information evaluation and foster digital literacy skills in online information seeking. The study found that while users engaged deeply with the copilot and demonstrated metacognitive reflection, it did not significantly improve answer correctness or search engagement. The results highlight the challenges in balancing pedagogical goals with efficiency demands in designing AI systems to support human judgement in information seeking."
  },
  {
    "title": "Game Accessibility Through Shared Control for People With Upper-Limb Impairments",
    "abstract": "Accessing video games is challenging for people with upper-limb impairments, especially when multiple inputs are required in rapid succession. Human cooperation, where a copilot assists the main player, has been proposed as a solution, but relying on a human assistant poses limitations in terms of availability and co-location. An alternative solution is to use partial automation, where the player is assisted by a software agent. In this work, we present a study with 13 participants with upper-limb impairments, comparatively evaluating how participants collaborate with their copilot in human cooperation and partial automation. The experiment is supported by GamePals, a modular framework that enables both human cooperation and partial automation on existing third-party video games.",
    "url": "https://arxiv.org/abs/2601.11218",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of shared control in video games to improve accessibility for individuals with upper-limb impairments. The study compares the effectiveness of human cooperation and partial automation in assisting players with impairments. The results suggest that both methods can enhance gameplay for individuals with disabilities, highlighting the potential of using software agents to improve accessibility in gaming."
  },
  {
    "title": "Noisy Graph Patterns via Ordered Matrices",
    "abstract": "The high-level structure of a graph is a crucial ingredient for the analysis and visualization of relational data. However, discovering the salient graph patterns that form this structure is notoriously difficult for two reasons. (1) Finding important patterns, such as cliques and bicliques, is computationally hard. (2) Real-world graphs contain noise, and therefore do not always exhibit patterns in their pure form. Defining meaningful noisy patterns and detecting them efficiently is a currently unsolved challenge. In this paper, we propose to use well-ordered matrices as a tool to both define and effectively detect noisy patterns. Specifically, we represent a graph as its adjacency matrix and optimally order it using Moran's $I$. Standard graph patterns (cliques, bicliques, and stars) now translate to rectangular submatrices. Using Moran's $I$, we define a permitted level of noise for such patterns. A combination of exact algorithms and heuristics allows us to efficiently decompose the matrix into noisy patterns. We also introduce a novel motif simplification that visualizes noisy patterns while explicitly encoding the level of noise. We showcase our techniques on several real-world data sets.",
    "url": "https://arxiv.org/abs/2601.11171",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research addresses the challenge of identifying important graph patterns in noisy real-world data. The study proposes using well-ordered matrices to define and detect noisy patterns in graphs, such as cliques and bicliques. By optimizing the adjacency matrix using Moran's $I$ and setting a permitted level of noise for patterns, the researchers were able to efficiently decompose the matrix into noisy patterns and visualize them while encoding the level of noise. The techniques were demonstrated on various real-world datasets, highlighting their effectiveness in analyzing relational data."
  },
  {
    "title": "AI Twin: Enhancing ESL Speaking Practice through AI Self-Clones of a Better Me",
    "abstract": "Advances in AI have enabled ESL learners to practice speaking through conversational systems. However, most tools rely on explicit correction, which can interrupt the conversation and undermine confidence. Grounded in second language acquisition and motivational psychology, we present AI Twin, a system that rephrases learner utterances into more fluent English and delivers them in the learner's voice. Embodying a more confident and proficient version of the learner, AI Twin reinforces motivation through alignment with their aspirational Ideal L2 Self. Also, its use of implicit feedback through rephrasing preserves conversational flow and fosters an emotionally supportive environment. In a within-subject study with 20 adult ESL learners, we compared AI Twin with explicit correction and a non-personalized rephrasing agent. Results show that AI Twin elicited higher emotional engagement, with participants describing the experience as more motivating. These findings highlight the potential of self-representative AI for personalized, psychologically grounded support in ESL learning.",
    "url": "https://arxiv.org/abs/2601.11103",
    "journal": "arXiv cs.HC",
    "ai_summary": "The study introduces AI Twin, a system that rephrases ESL learner utterances into more fluent English in the learner's voice, embodying a more confident and proficient version of the learner. The system aims to reinforce motivation by aligning with the learner's aspirational Ideal L2 Self and provides implicit feedback through rephrasing to preserve conversational flow. Results from a study with 20 adult ESL learners show that AI Twin elicited higher emotional engagement and was described as more motivating compared to explicit correction and non-personalized rephrasing agents, highlighting the potential of self-representative AI for personalized, psychologically grounded support in ESL learning."
  },
  {
    "title": "More Human or More AI? Visualizing Human-AI Collaboration Disclosures in Journalistic News Production",
    "abstract": "Within journalistic editorial processes, disclosing AI usage is currently limited to simplistic labels, which misses the nuance of how humans and AI collaborated on a news article. Through co-design sessions (N=10), we elicited 69 disclosure designs and implemented four prototypes that visually disclose human-AI collaboration in journalism. We then ran a within-subjects lab study (N=32) to examine how disclosure visualizations (Textual, Role-based Timeline, Task-based Timeline, Chatbot) and collaboration ratios (Primarily Human vs. Primarily AI) influenced visualization perceptions, gaze patterns, and post-experience responses. We found that textual disclosures were least effective in communicating human-AI collaboration, whereas Chatbot offered the most in-depth information. Furthermore, while role-based timelines amplified AI contribution in primarily human articles, task-based timeline shifted perceptions toward human involvement in primarily AI articles. We contribute Human-AI collaboration disclosure visualizations and their evaluation, and cautionary considerations on how visualizations can alter perceptions of AI's actual role during news article creation.",
    "url": "https://arxiv.org/abs/2601.11072",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores how human-AI collaboration in journalistic news production is currently disclosed and proposes new visualizations to better communicate this collaboration. The study found that textual disclosures were less effective than visualizations like role-based timelines and chatbots in conveying the extent of AI involvement. The findings highlight the importance of transparent and nuanced disclosure of AI usage in journalism to accurately represent the contributions of both humans and AI in news production."
  },
  {
    "title": "Children's Expectations, Engagement, and Evaluation of an LLM-enabled Spherical Visualization Platform in the Classroom",
    "abstract": "We present our first stage results from deploying an LLM-augmented visualization software in a classroom setting to engage primary school children with earth-related datasets. Motivated by the growing interest in conversational AI as a means to support inquiry-based learning, we investigate children's expectations, engagement, and evaluation of a spoken LLM interface with a shared, immersive visualization system in a formal educational context. Our system integrates a speech-capable large language model with an interactive spherical display. It enables children to ask natural-language questions and receive coordinated verbal explanations and visual responses through the LLM-augmented visualization updating in real time based on spoken queries. We report on a classroom study with Swedish children aged 9-10, combining structured observation and small-group discussions to capture expectations prior to interaction, interaction patterns during facilitated sessions, and children's reflections on their encounter afterward. Our results provide empirical insights into children's initial encounters with an LLM-enabled visualization platform within a classroom setting and their expectations, interactions, and evaluations of the system. These findings inform the technology's potential for educational use and highlight important directions for future research.",
    "url": "https://arxiv.org/abs/2601.11060",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research study explores the use of a large language model (LLM) in a classroom setting to engage primary school children with earth-related datasets. The study found that children were able to ask natural-language questions and receive verbal explanations and visual responses in real-time through the LLM-augmented visualization system. The results provide valuable insights into the potential of using LLM-enabled visualization platforms for educational purposes and suggest important avenues for future research in this area."
  },
  {
    "title": "Predicting Biased Human Decision-Making with Large Language Models in Conversational Settings",
    "abstract": "We examine whether large language models (LLMs) can predict biased decision-making in conversational settings, and whether their predictions capture not only human cognitive biases but also how those effects change under cognitive load. In a pre-registered study (N = 1,648), participants completed six classic decision-making tasks via a chatbot with dialogues of varying complexity. Participants exhibited two well-documented cognitive biases: the Framing Effect and the Status Quo Bias. Increased dialogue complexity resulted in participants reporting higher mental demand. This increase in cognitive load selectively, but significantly, increased the effect of the biases, demonstrating the load-bias interaction. We then evaluated whether LLMs (GPT-4, GPT-5, and open-source models) could predict individual decisions given demographic information and prior dialogue. While results were mixed across choice problems, LLM predictions that incorporated dialogue context were significantly more accurate in several key scenarios. Importantly, their predictions reproduced the same bias patterns and load-bias interactions observed in humans. Across all models tested, the GPT-4 family consistently aligned with human behavior, outperforming GPT-5 and open-source models in both predictive accuracy and fidelity to human-like bias patterns. These findings advance our understanding of LLMs as tools for simulating human decision-making and inform the design of conversational agents that adapt to user biases.",
    "url": "https://arxiv.org/abs/2601.11049",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of large language models (LLMs) in predicting biased decision-making in conversational settings. The study found that participants exhibited cognitive biases such as the Framing Effect and the Status Quo Bias, which were amplified under increased cognitive load. LLMs, particularly the GPT-4 family, were able to accurately predict individual decisions based on demographic information and dialogue context, reproducing human bias patterns and load-bias interactions. These findings suggest that LLMs can be valuable tools for simulating human decision-making and designing conversational agents that adapt to user biases."
  },
  {
    "title": "Haptic Light-Emitting Diodes: Miniature, Luminous Tactile Actuators",
    "abstract": "We present Haptic Light-Emitting Diodes (HLEDs), luminous thermopneumatic actuators that directly convert pulsed light into mechanical forces and displacements. Each device packages a miniature surface-mount LED in a gas-filled cavity that contains a low-inertia graphite photoabsorber. The cavity is sealed by an elastic membrane, which functions as a working diaphragm. Brief optical pulses heat the photoabsorber, which heats the gas. The resulting rapid pressure increases generate forces and displacements at the working diaphragm. Millimeter-scale HLEDs produce forces exceeding 0.4 N and displacements of 1 mm at low voltages, with 5 to 100 ms response times, making them attractive as actuators providing tactile feedback in human-machine interfaces. Perceptual testing revealed that the strength of tactile feedback increased linearly with optical power. HLEDs devices are mechanically simple and efficient to fabricate. Unusually, these actuators are also light-emitting, as a fraction of optical energy is transmitted through the membrane. These opto-mechanical actuators have many potential applications in tactile displays, human interface engineering, wearable computing, and other areas.",
    "url": "https://arxiv.org/abs/2601.11043",
    "journal": "arXiv cs.HC",
    "ai_summary": "The researchers have developed Haptic Light-Emitting Diodes (HLEDs), which are miniature actuators that convert pulsed light into mechanical forces and displacements. These devices can produce strong tactile feedback in human-machine interfaces, with forces exceeding 0.4 N and displacements of 1 mm at low voltages. The HLEDs are mechanically simple, efficient to fabricate, and have potential applications in tactile displays, human interface engineering, wearable computing, and other areas."
  },
  {
    "title": "\"I'm Constantly Getting Comments Like, 'Oh, You're Blind. You're Like the Only Woman That I Stand a Chance With.'\": A Study of Blind TikTokers' Intersectional Experiences of Gender and Sexuality",
    "abstract": "Social media platforms are important venues for identity expression, and the Human-Computer Interaction community has been paying growing attention to how marginalized groups express their identities on these platforms. Joining the emerging literature on intersectional experiences, we study blind TikTokers (\"BlindTokers\") who are also women and/or LGBTQ+. Using interview data from \\rev{41} participants, we identify their intersectional experiences as mediated by TikTok's socio-technical affordances. We argue that BlindTokers' intersectional marginalization is infrastructural: TikTok's classification and moderation features interact with social norms in ways that push them aside and distort how they are treated on the platform. We use this infrastructure perspective to understand what these experiences are, how they were formed, and how they become harmful. We further recognize participants' infrastructuring work to address these problems. This study guides future social media design with accessible creator tools, inclusive identity options, and context-aware moderation developed in partnership with communities.",
    "url": "https://arxiv.org/abs/2601.10957",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study focuses on the intersectional experiences of blind TikTokers who are also women and/or LGBTQ+. The research highlights how TikTok's features and social norms contribute to the marginalization of these individuals on the platform. The findings emphasize the need for social media design that is inclusive and accessible, with creator tools, identity options, and moderation strategies developed in collaboration with marginalized communities."
  },
  {
    "title": "\"My Brother Is a School Principal, Earns About $80,000 Per Year... But When the Kids See Me, 'Wow, Uncle, You Have 1500 Followers on TikTok!'\": A Study of Blind TikTokers' Alternative Professional Development Experiences",
    "abstract": "One's profession is an essential part of modern life. Traditionally, professional development has been criticized for excluding people with disabilities. People with visual impairments, for example, face disproportionately low employment rates, highlighting persistent gaps in professional opportunities. Recently, there has been growing research on social media platforms as spaces for more equitable career development approaches. In this paper, we present an interview study on the professional development experiences of 60 people with visual impairments on TikTok (also known as \"BlindTokers\"). We report BlindTokers' goals, strategies, and challenges, supported by detailed examples and in-depth analysis. Based on the findings, we identify that BlindTokers' practices reveal an alternative professional development approach that is more flexible, inclusive, personalized, and diversified than traditional models. Our study also extends professional development research by foregrounding emerging digital skills and proposing design implications to foster more equitable and inclusive professional opportunities.",
    "url": "https://arxiv.org/abs/2601.10956",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores the professional development experiences of visually impaired individuals on TikTok, known as BlindTokers. The findings suggest that BlindTokers have developed alternative, flexible, inclusive, and personalized approaches to career development compared to traditional models. This research highlights the importance of social media platforms in providing more equitable and diverse professional opportunities for individuals with disabilities."
  },
  {
    "title": "Bridging Psychological Safety and Skill Guidance: An Adaptive Robotic Interview Coach",
    "abstract": "Social robots hold promise for reducing job interview anxiety, yet designing agents that provide both psychological safety and instructional guidance remains challenging. Through a three-phase iterative design study (N = 8), we empirically mapped this tension. Phase I revealed a \"Safety-Guidance Gap\": while a Person-Centered Therapy (PCT) robot established safety (d = 3.27), users felt insufficiently coached. Phase II identified a \"Scaffolding Paradox\": rigid feedback caused cognitive overload, while delayed feedback lacked specificity. In Phase III, we resolved these tensions by developing an Agency-Driven Interaction Layer. Synthesizing our empirical findings, we propose the Adaptive Scaffolding Ecosystem, a conceptual framework that redefines robotic coaching not as a static script, but as a dynamic balance between affective support and instructional challenge, mediated by user agency.",
    "url": "https://arxiv.org/abs/2601.10824",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the challenge of designing social robots for job interviews that provide both psychological safety and instructional guidance. The study identified a \"Safety-Guidance Gap\" and a \"Scaffolding Paradox\" in previous designs, leading to the development of an Agency-Driven Interaction Layer to address these tensions. The proposed Adaptive Scaffolding Ecosystem framework redefines robotic coaching as a dynamic balance between emotional support and instructional challenge, mediated by user agency."
  },
  {
    "title": "From SERPs to Sound: How Search Engine Result Pages and AI-generated Podcasts Interact to Influence User Attitudes on Controversial Topics",
    "abstract": "Compared to search engine result pages (SERPs), AI-generated podcasts represent a relatively new and relatively more passive modality of information consumption, delivering narratives in a naturally engaging format. As these two media increasingly converge in everyday information-seeking behavior, it is essential to explore how their interaction influences user attitudes, particularly in contexts involving controversial, value-laden, and often debated topics. Addressing this need, we aim to understand how information mediums of present-day SERPs and AI-generated podcasts interact to shape the opinions of users. To this end, through a controlled user study (N=483), we investigated user attitudinal effects of consuming information via SERPs and AI-generated podcasts, focusing on how the sequence and modality of exposure shape user opinions. A majority of users in our study corresponded to attitude change outcomes, and we found an effect of sequence on attitude change. Our results further revealed a role of viewpoint bias and the degree of topic controversiality in shaping attitude change, although we found no effect of individual moderators.",
    "url": "https://arxiv.org/abs/2601.11282",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores how search engine result pages (SERPs) and AI-generated podcasts interact to influence user attitudes on controversial topics. The study found that the sequence and modality of exposure to information through SERPs and podcasts can impact user opinions, with a majority of users experiencing attitude change. The results also suggest that viewpoint bias and the level of controversiality of the topic play a role in shaping attitude change, highlighting the importance of understanding how different media formats can influence user attitudes."
  },
  {
    "title": "The Big Ban Theory: A Pre- and Post-Intervention Dataset of Online Content Moderation Actions",
    "abstract": "Online platforms rely on moderation interventions to curb harmful behavior such hate speech, toxicity, and the spread of mis- and disinformation. Yet research on the effects and possible biases of such interventions faces multiple limitations. For example, existing works frequently focus on single or a few interventions, due to the absence of comprehensive datasets. As a result, researchers must typically collect the necessary data for each new study, which limits opportunities for systematic comparisons. To overcome these challenges, we introduce The Big Ban Theory (TBBT), a large dataset of moderation interventions. TBBT covers 25 interventions of varying type, severity, and scope, comprising in total over 339K users and nearly 39M posted messages. For each intervention, we provide standardized metadata and pseudonymized user activity collected three months before and after its enforcement, enabling consistent and comparable analyses of intervention effects. In addition, we provide a descriptive exploratory analysis of the dataset, along with several use cases of how it can support research on content moderation. With this dataset, we aim to support researchers studying the effects of moderation interventions and to promote more systematic, reproducible, and comparable research. TBBT is publicly available at: this https URL.",
    "url": "https://arxiv.org/abs/2601.11128",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces The Big Ban Theory (TBBT), a comprehensive dataset of online content moderation interventions covering 25 interventions and over 339K users. The dataset includes standardized metadata and user activity before and after each intervention, allowing for consistent and comparable analyses of intervention effects. This dataset aims to support researchers in studying the effects of moderation interventions and promote more systematic, reproducible, and comparable research in this area."
  },
  {
    "title": "Sparing User Time with a Socially-Aware Independent Metaverse Avatar",
    "abstract": "The Metaverse is redefining digital interactions by merging physical, virtual, and social dimensions, yet its effects on social networking remain largely unexplored. This work examines the role of independent avatars (autonomous digital entities capable of managing social interactions on behalf of users), to optimize social time allocation and reshape Metaverse-based Online Social Networks. We propose a novel computational model that integrates a quantitative and realistic representation of user social life, grounded in evolutionary anthropology, with a framework for avatar-mediated interactions. Our model quantifies the effectiveness of a partial replacement of in-person interactions with independent avatar interactions. Additionally, it accounts for social conflicts and specific socialization constraints. We leverage our model to explore the benefits and trade-offs of an avatar-augmented social life in the Metaverse. Since the exact problem formulation leads to an NP-hard optimization problem when incorporating avatars into the social network, we tackle this challenge by introducing a heuristic solution. Through simulations, we compare avatar-mediated and non-avatar-mediated social networking, demonstrating the potential of independent avatars to enhance social connectivity and efficiency. Our findings provide a foundation for optimizing Metaverse-based social interactions, as well as useful insights for future digital social network design.",
    "url": "https://arxiv.org/abs/2601.11115",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of independent avatars in the Metaverse to optimize social time allocation and enhance social interactions. The study introduces a computational model that quantifies the benefits of replacing in-person interactions with avatar-mediated interactions, taking into account social conflicts and constraints. The findings suggest that independent avatars have the potential to improve social connectivity and efficiency in the Metaverse, providing valuable insights for future digital social network design."
  },
  {
    "title": "Modeling Multi-Party Interaction in Couples Therapy: A Multi-Agent Simulation Approach",
    "abstract": "Couples therapy, or relationship counseling, helps partners resolve conflicts, improve satisfaction, and foster psychological growth. Traditional approaches to training couples therapists, such as textbooks and roleplay, often fail to capture the complexity and emotional nuance of real couple dynamics. We present a novel multimodal, multi-agent simulation system that models multi-party interactions in couples therapy. Informed by our systematic research, this system creates a low-stakes environment for trainee therapists to gain valuable practical experience dealing with the critical demand-withdraw communication cycle across six couple-interaction stages. In an evaluation study involving 21 US-based licensed therapists, participants blind to conditions identified the engineered agent behaviors (i.e., the stages and the demand-withdraw cycle) and rated overall realism and agent responses higher for the experimental system than the baseline. As the first known multi-agent framework for training couples therapists, our work builds the foundation for future research that fuses HCI technologies with couples therapy.",
    "url": "https://arxiv.org/abs/2601.10970",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a multi-agent simulation system for training couples therapists, aiming to capture the complexity of real couple dynamics. The system allows trainee therapists to practice dealing with the demand-withdraw communication cycle across different couple-interaction stages. An evaluation study with licensed therapists showed that the system was more realistic and effective than traditional training methods, highlighting the potential of using HCI technologies in couples therapy training."
  },
  {
    "title": "Can Instructed Retrieval Models Really Support Exploration?",
    "abstract": "Exploratory searches are characterized by under-specified goals and evolving query intents. In such scenarios, retrieval models that can capture user-specified nuances in query intent and adapt results accordingly are desirable -- instruction-following retrieval models promise such a capability. In this work, we evaluate instructed retrievers for the prevalent yet under-explored application of aspect-conditional seed-guided exploration using an expert-annotated test collection. We evaluate both recent LLMs fine-tuned for instructed retrieval and general-purpose LLMs prompted for ranking with the highly performant Pairwise Ranking Prompting. We find that the best instructed retrievers improve on ranking relevance compared to instruction-agnostic approaches. However, we also find that instruction following performance, crucial to the user experience of interacting with models, does not mirror ranking relevance improvements and displays insensitivity or counter-intuitive behavior to instructions. Our results indicate that while users may benefit from using current instructed retrievers over instruction-agnostic models, they may not benefit from using them for long-running exploratory sessions requiring greater sensitivity to instructions.",
    "url": "https://arxiv.org/abs/2601.10936",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research evaluates the effectiveness of instructed retrieval models for exploratory searches with evolving query intents. The study finds that instructed retrievers improve ranking relevance compared to instruction-agnostic approaches, but they may not always follow instructions accurately, leading to counter-intuitive behavior. While current instructed retrievers may be beneficial for users in short exploratory sessions, they may not be as effective for longer exploratory searches requiring greater sensitivity to instructions."
  },
  {
    "title": "Hidden-in-Plain-Text: A Benchmark for Social-Web Indirect Prompt Injection in RAG",
    "abstract": "Retrieval-augmented generation (RAG) systems put more and more emphasis on grounding their responses in user-generated content found on the Web, amplifying both their usefulness and their attack surface. Most notably, indirect prompt injection and retrieval poisoning attack the web-native carriers that survive ingestion pipelines and are very concerning. We provide OpenRAG-Soc, a compact, reproducible benchmark-and-harness for web-facing RAG evaluation under these threats, in a discrete data package. The suite combines a social corpus with interchangeable sparse and dense retrievers and deployable mitigations - HTML/Markdown sanitization, Unicode normalization, and attribution-gated answered. It standardizes end-to-end evaluation from ingestion to generation and reports attacks time of one of the responses at answer time, rank shifts in both sparse and dense retrievers, utility and latency, allowing for apples-to-apples comparisons across carriers and defenses. OpenRAG-Soc targets practitioners who need fast, and realistic tests to track risk and harden deployments.",
    "url": "https://arxiv.org/abs/2601.10923",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces OpenRAG-Soc, a benchmark for evaluating retrieval-augmented generation (RAG) systems in the context of indirect prompt injection and retrieval poisoning attacks on user-generated content from the web. The benchmark includes a social corpus, interchangeable retrievers, and deployable mitigations to standardize evaluation and compare performance across different systems and defenses. OpenRAG-Soc is designed to help practitioners assess risks and strengthen their RAG deployments in a realistic and efficient manner."
  },
  {
    "title": "Chatting with Confidants or Corporations? Privacy Management with AI Companions",
    "abstract": "AI chatbots designed as emotional companions blur the boundaries between interpersonal intimacy and institutional software, creating a complex, multi-dimensional privacy environment. Drawing on Communication Privacy Management theory and Masur's horizontal (user-AI) and vertical (user-platform) privacy framework, we conducted in-depth interviews with fifteen users of companion AI platforms such as Replika and this http URL. Our findings reveal that users blend interpersonal habits with institutional awareness: while the non-judgmental, always-available nature of chatbots fosters emotional safety and encourages self-disclosure, users remain mindful of institutional risks and actively manage privacy through layered strategies and selective sharing. Despite this, many feel uncertain or powerless regarding platform-level data control. Anthropomorphic design further blurs privacy boundaries, sometimes leading to unintentional oversharing and privacy turbulence. These results extend privacy theory by highlighting the unique interplay of emotional and institutional privacy management in human-AI companionship.",
    "url": "https://arxiv.org/abs/2601.10754",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the privacy management practices of users interacting with AI chatbots designed as emotional companions. The study found that users navigate a complex privacy environment, blending interpersonal habits with institutional awareness to maintain emotional safety while managing privacy risks. The findings suggest that users feel uncertain about platform-level data control and may unintentionally overshare due to anthropomorphic design, highlighting the unique interplay of emotional and institutional privacy in human-AI companionship."
  },
  {
    "title": "An Extension-Based Accessibility Framework for Making Blockly Accessible to Blind and Low-Vision Users",
    "abstract": "Block-based programming environments (BBPEs) such as Scratch and this http URL are now widely used in K-12 computer science classes, but they remain mostly inaccessible to blind or visually impaired (BVI) learners. A major problem is that prior accessibility solutions have relied on modifications to the Blockly library, making them difficult to apply in existing BBPEs and thereby limiting adoption. We present an Extension-based Accessibility Framework (EAF) to make BBPEs accessible for BVI students. The framework uses a modular architecture that enables seamless integration with existing Blockly-based BBPEs. We present an innovative three-dimensional (3D) hierarchical navigation model featuring stack labeling and block numbering, mode-based editing to prevent accidental modifications, and WAI-ARIA implementation to ensure compatibility with external screen readers. We evaluated our approach by integrating the EAF framework into two BBPEs (covering 177 test cases) and conducting semi-structured interviews with four participants using VoiceOver, JAWS, and NVDA. Participants reported clearer spatial orientation and easier mental model formation compared to default Blockly keyboard navigation. EAF shows that modular architecture can provide comprehensive accessibility while ensuring compatibility with existing BBPEs.",
    "url": "https://arxiv.org/abs/2601.10688",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces an Extension-based Accessibility Framework (EAF) to make block-based programming environments (BBPEs) accessible for blind and visually impaired (BVI) students. The framework includes a three-dimensional hierarchical navigation model, mode-based editing, and WAI-ARIA implementation to ensure compatibility with screen readers. Evaluation of the framework in two BBPEs showed improved spatial orientation and mental model formation for BVI users compared to default keyboard navigation, highlighting the significance of modular architecture in providing comprehensive accessibility in existing BBPEs."
  },
  {
    "title": "CoGen: Creation of Reusable UI Components in Figma via Textual Commands",
    "abstract": "The evolution of User Interface design has emphasized the need for efficient, reusable, and editable components to ensure an efficient design process. This research introduces CoGen, a system that uses machine learning techniques to generate reusable UI components directly in Figma, one of the most popular UI design tools. Addressing gaps in current systems, CoGen focuses on creating atomic components such as buttons, labels, and input fields using structured JSON and natural language prompts.\nThe project integrates Figma API data extraction, Seq2Seq models, and fine-tuned T5 transformers for component generation. The key results demonstrate the efficiency of the T5 model in prompt generation, with an accuracy of 98% and a BLEU score of 0.2668, which ensures the mapping of JSON to descriptive prompts. For JSON creation, CoGen achieves a success rate of up to 100% in generating simple JSON outputs for specified component types.",
    "url": "https://arxiv.org/abs/2601.10536",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces CoGen, a system that uses machine learning to generate reusable UI components in Figma. CoGen focuses on creating atomic components like buttons and input fields using structured JSON and natural language prompts. The project shows high accuracy and success rates in generating prompts and JSON outputs, making it a valuable tool for efficient UI design processes."
  },
  {
    "title": "Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition",
    "abstract": "Understanding how local neurophysiological patterns interact with global brain dynamics is essential for decoding human emotions from EEG signals. However, existing deep learning approaches often overlook the brain's intrinsic spatial organization, failing to simultaneously capture local topological relations and global dependencies. To address these challenges, we propose Neuro-HGLN, a Neurologically-informed Hierarchical Graph-Transformer Learning Network that integrates biologically grounded priors with hierarchical representation learning. Neuro-HGLN first constructs a spatial Euclidean prior graph based on physical electrode distances to serve as an anatomically grounded inductive bias. A learnable global dynamic graph is then introduced to model functional connectivity across the entire brain. In parallel, to capture fine-grained regional dependencies, Neuro-HGLN builds region-level local graphs using a multi-head self-attention mechanism. These graphs are processed synchronously through local-constrained parallel GCN layers to produce region-specific representations. Subsequently, an iTransformer encoder aggregates these features to capture cross-region dependencies under a dimension-as-token formulation. Extensive experiments demonstrate that Neuro-HGLN achieves state-of-the-art performance on multiple benchmarks, providing enhanced interpretability grounded in neurophysiological structure. These results highlight the efficacy of unifying local topological learning with cross-region dependency modeling for robust EEG emotion recognition.",
    "url": "https://arxiv.org/abs/2601.10525",
    "journal": "arXiv cs.HC",
    "ai_summary": "The study introduces a novel Neurologically-informed Hierarchical Graph-Transformer Learning Network (Neuro-HGLN) that combines biologically grounded priors with hierarchical representation learning to decode human emotions from EEG signals. By incorporating spatial Euclidean prior graphs, global dynamic graphs, and region-level local graphs, Neuro-HGLN effectively captures local topological relations and global dependencies in the brain, leading to state-of-the-art performance in EEG emotion recognition. This approach enhances interpretability and demonstrates the importance of integrating local and global information for robust analysis of EEG signals."
  },
  {
    "title": "AI Sycophancy: How Users Flag and Respond",
    "abstract": "While concerns about LLM sycophancy have grown among researchers and developers, how users themselves experience this behavior remains largely unexplored. We analyze Reddit discussions to investigate how users detect, mitigate, and perceive sycophantic AI. We develop the ODR Framework that maps user experiences across three stages: observing sycophantic behaviors, detecting sycophancy, and responding to these behaviors. Our findings reveal that users employ various detection techniques, including cross-platform comparison and inconsistency testing. We document diverse mitigation approaches, such as persona-based prompts to specific language patterns in prompt engineering. We find sycophancy's effects are context-dependent rather than universally harmful. Specifically, vulnerable populations experiencing trauma, mental health challenges, or isolation actively seek and value sycophantic behaviors as emotional support. Users develop both technical and folk explanations for why sycophancy occurs. These findings challenge the assumption that sycophancy should be eliminated universally. We conclude by proposing context-aware AI design that balances the risks with the benefits of affirmative interaction, while discussing implications for user education and transparency.",
    "url": "https://arxiv.org/abs/2601.10467",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores how users detect, respond to, and perceive sycophantic behavior in AI, using Reddit discussions as a case study. The study introduces the ODR Framework to map user experiences in observing, detecting, and responding to sycophancy in AI. The findings show that users employ various techniques to detect sycophantic behaviors and that the effects of sycophancy can be context-dependent, with some vulnerable populations valuing these behaviors for emotional support. The research challenges the idea that sycophancy should be universally eliminated and suggests context-aware AI design to balance risks and benefits in affirmative interactions."
  },
  {
    "title": "LangLasso: Interactive Cluster Descriptions through LLM Explanation",
    "abstract": "Dimensionality reduction is a powerful technique for revealing structure and potential clusters in data. However, as the axes are complex, non-linear combinations of features, they often lack semantic interpretability. Existing visual analytics (VA) methods support cluster interpretation through feature comparison and interactive exploration, but they require technical expertise and intense human effort. We present \\textit{LangLasso}, a novel method that complements VA approaches through interactive, natural language descriptions of clusters using large language models (LLMs). It produces human-readable descriptions that make cluster interpretation accessible to non-experts and allow integration of external contextual knowledge beyond the dataset. We systematically evaluate the reliability of these explanations and demonstrate that \\langlasso provides an effective first step for engaging broader audiences in cluster interpretation. The tool is available at this https URL",
    "url": "https://arxiv.org/abs/2601.10458",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces LangLasso, a method that uses large language models to provide interactive, natural language descriptions of clusters in data sets. This approach complements existing visual analytics methods by making cluster interpretation accessible to non-experts and allowing for integration of external contextual knowledge. The study demonstrates the reliability of these explanations and highlights the effectiveness of LangLasso in engaging broader audiences in cluster interpretation."
  },
  {
    "title": "Does Cognitive Load Affect Human Accuracy in Detecting Voice-Based Deepfakes?",
    "abstract": "Deepfake technologies are powerful tools that can be misused for malicious purposes such as spreading disinformation on social media. The effectiveness of such malicious applications depends on the ability of deepfakes to deceive their audience. Therefore, researchers have investigated human abilities to detect deepfakes in various studies. However, most of these studies were conducted with participants who focused exclusively on the detection task; hence the studies may not provide a complete picture of human abilities to detect deepfakes under realistic conditions: Social media users are exposed to cognitive load on the platform, which can impair their detection abilities. In this paper, we investigate the influence of cognitive load on human detection abilities of voice-based deepfakes in an empirical study with 30 participants. Our results suggest that low cognitive load does not generally impair detection abilities, and that the simultaneous exposure to a secondary stimulus can actually benefit people in the detection task.",
    "url": "https://arxiv.org/abs/2601.10383",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores the impact of cognitive load on human accuracy in detecting voice-based deepfakes, which are often used for spreading disinformation on social media. The study with 30 participants found that low cognitive load does not necessarily impair detection abilities, and exposure to a secondary stimulus can actually improve detection performance. These findings are significant as they highlight the importance of considering real-world conditions, such as cognitive load, when studying human abilities to detect deepfakes."
  },
  {
    "title": "Developer Interaction Patterns with Proactive AI: A Five-Day Field Study",
    "abstract": "Current in-IDE AI coding tools typically rely on time-consuming manual prompting and context management, whereas proactive alternatives that anticipate developer needs without explicit invocation remain underexplored. Understanding when humans are receptive to such proactive AI assistance during their daily work remains an open question in human-AI interaction research. We address this gap through a field study of proactive AI assistance in professional developer workflows. We present a five-day in-the-wild study with 15 developers who interacted with a proactive feature of an AI assistant integrated into a production-grade IDE that offers code quality suggestions based on in-IDE developer activity. We examined 229 AI interventions across 5,732 interaction points to understand how proactive suggestions are received across workflow stages, how developers experience them, and their perceived impact. Our findings reveal systematic patterns in human receptivity to proactive suggestions: interventions at workflow boundaries (e.g., post-commit) achieved 52% engagement rates, while mid-task interventions (e.g., on declined edit) were dismissed 62% of the time. Notably, well-timed proactive suggestions required significantly less interpretation time than reactive suggestions (45.4s versus 101.4s, W = 109.00, r = 0.533, p = 0.0016), indicating enhanced cognitive alignment. This study provides actionable implications for designing proactive coding assistants, including how to time interventions, align them with developer context, and strike a balance between AI agency and user control in production IDEs.",
    "url": "https://arxiv.org/abs/2601.10253",
    "journal": "arXiv cs.HC",
    "ai_summary": "This field study explores how developers interact with proactive AI assistance in their daily work using a production-grade IDE. The study found that developers were more receptive to proactive suggestions at workflow boundaries compared to mid-task interventions, with well-timed proactive suggestions requiring less interpretation time than reactive suggestions. These findings have implications for designing proactive coding assistants, including the importance of timing interventions, aligning them with developer context, and finding a balance between AI agency and user control in production IDEs."
  },
  {
    "title": "Who Owns the Text? Design Patterns for Preserving Authorship in AI-Assisted Writing",
    "abstract": "AI writing assistants can reduce effort and improve fluency, but they may also weaken writers' sense of authorship. We study this tension with an ownership-aware co-writing editor that offers on-demand, sentence-level suggestions and tests two common design choices: persona-based coaching and style personalization. In an online study (N=176), participants completed three professional writing tasks: an email without AI help, a proposal with generic AI suggestions, and a cover letter with persona-based coaching, while half received suggestions tailored to a brief sample of their prior writing. Across the two AI-assisted tasks, psychological ownership dropped relative to unassisted writing (about 0.85-1.0 points on a 7-point scale), even as cognitive load decreased (about 0.9 points) and quality ratings stayed broadly similar overall. Persona coaching did not prevent the ownership decline. Style personalization partially restored ownership (about +0.43) and increased AI incorporation in text (+5 percentage points). We distill five design patterns: on-demand initiation, micro-suggestions, voice anchoring, audience scaffolds, and point-of-decision provenance, to guide authorship-preserving writing tools.",
    "url": "https://arxiv.org/abs/2601.10236",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research examines the impact of AI writing assistants on writers' sense of authorship, finding that while AI suggestions can reduce cognitive load and improve fluency, they may also decrease psychological ownership of the text. The study tested different design choices, such as persona-based coaching and style personalization, and found that style personalization helped restore ownership and increase AI incorporation in text. The findings suggest that incorporating design patterns such as on-demand initiation and voice anchoring can help preserve authorship in AI-assisted writing tools."
  },
  {
    "title": "Tables or Sankey Diagrams? Investigating User Interaction with Different Representations of Simulation Parameters",
    "abstract": "Understanding complex parameter dependencies is critical for effective configuration and maintenance of software systems across diverse domains - from Computer-Aided Engineering (CAE) to cloud infrastructure and database management. However, legacy tabular interfaces create a major bottleneck: engineers cannot easily comprehend how parameters relate across the system, leading to inefficient workflows, costly configuration errors, and reduced system trust - a fundamental program comprehension challenge in configuration-intensive software. This research evaluates whether interactive Sankey diagrams can improve comprehension of parameter dependencies compared to traditional spreadsheet interfaces. We employed a heuristic evaluation using the PURE method with three expert evaluators (UX design, simulation, and software development specialists) to compare a Sankey-based prototype to traditional tabular representations for core engineering tasks. Our key contribution demonstrates that flow-based parameter visualizations significantly reduce cognitive load (51% lower PURE scores) and interaction complexity (56% fewer steps) compared to traditional tables, while making parameter dependencies immediately visible rather than requiring mental reconstruction. By explicitly visualizing parameter relationships, Sankey diagrams address a core software visualization challenge: helping users comprehend complex system configurations without requiring deep tool-specific knowledge. While demonstrated through CAE software, this research contributes to program comprehension and software visualization by showing that dependency-aware visualizations can significantly improve understanding of configuration-intensive systems. The findings have implications for any software domain where comprehending complex parameter relationships is essential for effective system use and maintenance.",
    "url": "https://arxiv.org/abs/2601.10232",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research investigates the effectiveness of interactive Sankey diagrams in improving comprehension of parameter dependencies compared to traditional tabular interfaces for software configuration tasks. The study found that Sankey diagrams significantly reduce cognitive load and interaction complexity, making parameter relationships more visible and easier to understand. This has implications for improving program comprehension and software visualization in various domains where understanding complex parameter relationships is crucial for system configuration and maintenance."
  },
  {
    "title": "Empowering Older Adults in Digital Technology Use with Foundation Models",
    "abstract": "While high-quality technology support can assist older adults in using digital applications, many struggle to articulate their issues due to unfamiliarity with technical terminology and age-related cognitive changes. This study examines these communication challenges and explores AI-based approaches to mitigate them. We conducted a diary study with English-speaking, community-dwelling older adults to collect asynchronous, technology-related queries and used reflexive thematic analysis to identify communication barriers. To address these barriers, we evaluated how foundation models can paraphrase older adults' queries to improve solution accuracy. Two controlled experiments followed: one with younger adults evaluating AI-rephrased queries and another with older adults evaluating AI-generated solutions. We also developed a pipeline using large language models to generate the first synthetic dataset of how older adults request tech support (OATS). We identified four key communication challenges: verbosity, incompleteness, over-specification, and under-specification. Our prompt-chaining approach using the large language model, GPT-4o, elicited contextual details, paraphrased the original query, and generated a solution. AI-rephrased queries significantly improved solution accuracy (69% vs. 46%) and Google search results (69% vs. 35%). Younger adults better understood AI-rephrased queries (93.7% vs. 65.8%) and reported greater confidence and ease. Older adults reported high perceived ability to answer contextual questions (89.8%) and follow solutions (94.7%), with high confidence and ease. OATS demonstrated strong fidelity and face validity. This work shows how foundation models can enhance technology support for older adults by addressing age-related communication barriers. The OATS dataset offers a scalable resource for developing equitable AI systems that better serve aging populations.",
    "url": "https://arxiv.org/abs/2601.10018",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores communication challenges faced by older adults when seeking technology support and investigates AI-based solutions to improve their experience. The research identifies key barriers such as verbosity and incompleteness in queries and demonstrates how using foundation models can help paraphrase queries to enhance solution accuracy. The study shows that AI-rephrased queries significantly improve solution accuracy and Google search results, highlighting the potential of AI in empowering older adults to use digital technology effectively."
  },
  {
    "title": "From SERPs to Agents: A Platform for Comparative Studies of Information Interaction",
    "abstract": "The diversification of information access systems, from RAG to autonomous agents, creates a critical need for comparative user studies. However, the technical overhead to deploy and manage these distinct systems is a major barrier. We present UXLab, an open-source system for web-based user studies that addresses this challenge. Its core is a web-based dashboard enabling the complete, no-code configuration of complex experimental designs. Researchers can visually manage the full study, from recruitment to comparing backends like traditional search, vector databases, and LLMs. We demonstrate UXLab's value via a micro case study comparing user behavior with RAG versus an autonomous agent. UXLab allows researchers to focus on experimental design and analysis, supporting future multi-modal interaction research.",
    "url": "https://arxiv.org/abs/2601.09937",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research presents UXLab, an open-source system for conducting web-based user studies comparing different information access systems. The system allows researchers to easily configure complex experimental designs without the need for coding, enabling comparisons between traditional search, vector databases, and LLMs. A case study comparing user behavior with RAG and autonomous agents demonstrates the value of UXLab in facilitating experimental design and analysis for future multi-modal interaction research."
  },
  {
    "title": "In-Browser Agents for Search Assistance",
    "abstract": "A fundamental tension exists between the demand for sophisticated AI assistance in web search and the need for user data privacy. Current centralized models require users to transmit sensitive browsing data to external services, which limits user control. In this paper, we present a browser extension that provides a viable in-browser alternative. We introduce a hybrid architecture that functions entirely on the client side, combining two components: (1) an adaptive probabilistic model that learns a user's behavioral policy from direct feedback, and (2) a Small Language Model (SLM), running in the browser, which is grounded by the probabilistic model to generate context-aware suggestions. To evaluate this approach, we conducted a three-week longitudinal user study with 18 participants. Our results show that this privacy-preserving approach is highly effective at adapting to individual user behavior, leading to measurably improved search efficiency. This work demonstrates that sophisticated AI assistance is achievable without compromising user privacy or data control.",
    "url": "https://arxiv.org/abs/2601.09928",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a browser extension that provides AI assistance for web search while maintaining user data privacy. The hybrid architecture used in the extension, which functions entirely on the client side, includes an adaptive probabilistic model and a Small Language Model to generate context-aware suggestions. The results of a user study with 18 participants show that this approach improves search efficiency without compromising user privacy, highlighting the feasibility of sophisticated AI assistance in web search."
  },
  {
    "title": "Cooking Up Politeness in Human-AI Information Seeking Dialogue",
    "abstract": "Politeness is a core dimension of human communication, yet its role in human-AI information seeking remains underexplored. We investigate how user politeness behaviour shapes conversational outcomes in a cooking-assistance setting. First, we annotated 30 dialogues, identifying four distinct user clusters ranging from Hyperpolite to Hyperefficient. We then scaled up to 18,000 simulated conversations across five politeness profiles (including impolite) and three open-weight models. Results show that politeness is not only cosmetic: it systematically affects response length, informational gain, and efficiency. Engagement-seeking prompts produced up to 90% longer replies and 38% more information nuggets than hyper-efficient prompts, but at markedly lower density. Impolite inputs yielded verbose but less efficient answers, with up to 48% fewer nuggets per watt-hour compared to polite input. These findings highlight politeness as both a fairness and sustainability issue: conversational styles can advantage or disadvantage users, and \"polite\" requests may carry hidden energy costs. We discuss implications for inclusive and resource-aware design of information agents.",
    "url": "https://arxiv.org/abs/2601.09898",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the impact of user politeness behavior on conversational outcomes in human-AI information seeking dialogues in a cooking-assistance setting. The study identified four distinct user clusters based on politeness levels and found that politeness significantly affects response length, informational gain, and efficiency. The findings suggest that conversational styles can either advantage or disadvantage users, with polite requests potentially carrying hidden energy costs, highlighting the importance of inclusive and resource-aware design of information agents."
  },
  {
    "title": "The Algorithmic Gaze: An Audit and Ethnography of the LAION-Aesthetics Predictor Model",
    "abstract": "Visual generative AI models are trained using a one-size-fits-all measure of aesthetic appeal. However, what is deemed \"aesthetic\" is inextricably linked to personal taste and cultural values, raising the question of whose taste is represented in visual generative AI models. In this work, we study an aesthetic evaluation model--LAION Aesthetic Predictor (LAP)--that is widely used to curate datasets to train visual generative image models, like Stable Diffusion, and evaluate the quality of AI-generated images. To understand what LAP measures, we audited the model across three datasets. First, we examined the impact of aesthetic filtering on the LAION-Aesthetics Dataset (approximately 1.2B images), which was curated from LAION-5B using LAP. We find that the LAP disproportionally filters in images with captions mentioning women, while filtering out images with captions mentioning men or LGBTQ+ people. Then, we used LAP to score approximately 330k images across two art datasets, finding the model rates realistic images of landscapes, cityscapes, and portraits from western and Japanese artists most highly. In doing so, the algorithmic gaze of this aesthetic evaluation model reinforces the imperial and male gazes found within western art history. In order to understand where these biases may have originated, we performed a digital ethnography of public materials related to the creation of LAP. We find that the development of LAP reflects the biases we found in our audits, such as the aesthetic scores used to train LAP primarily coming from English-speaking photographers and western AI-enthusiasts. In response, we discuss how aesthetic evaluation can perpetuate representational harms and call on AI developers to shift away from prescriptive measures of \"aesthetics\" toward more pluralistic evaluation.",
    "url": "https://arxiv.org/abs/2601.09896",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research examines the LAION Aesthetic Predictor (LAP) model used to curate datasets for training visual generative AI models. The study found that LAP disproportionately filters in images mentioning women while filtering out images mentioning men or LGBTQ+ people, reinforcing biases in western art history. The findings highlight the need for AI developers to move towards more pluralistic evaluation methods to avoid perpetuating representational harms."
  },
  {
    "title": "LAMDA: Aiding Visual Exploration of Atomic Displacements in Molecular Dynamics Simulations",
    "abstract": "Contemporary materials science research is heavily conducted in silico, involving massive simulations of the atomic-scale evolution of materials. Cataloging basic patterns in the atomic displacements is key to understanding and predicting the evolution of physical properties. However, the combinatorial complexity of the space of possible transitions coupled with the overwhelming amount of data being produced by high-throughput simulations make such an analysis extremely challenging and time-consuming for domain experts. The development of visual analytics systems that facilitate the exploration of simulation data is an active field of research. While these systems excel in identifying temporal regions of interest, they treat each timestep of a simulation as an independent event without considering the behavior of the atomic displacements between timesteps. We address this gap by introducing LAMDA, a visual analytics system that allows domain experts to quickly and systematically explore state-to-state transitions. In LAMDA, transitions are hierarchically categorized, providing a basis for cataloging displacement behavior, as well as enabling the analysis of simulations at different resolutions, ranging from very broad qualitative classes of transitions to very narrow definitions of unit processes. LAMDA supports navigating the hierarchy of transitions, enabling scientists to visualize the commonalities between different transitions in each class in terms of invariant features characterizing local atomic environments, and LAMDA simplifies the analysis by capturing user inputs through annotations. We evaluate our system through a case study and report on findings from our domain experts.",
    "url": "https://arxiv.org/abs/2601.09887",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces LAMDA, a visual analytics system designed to aid in the exploration of atomic displacements in molecular dynamics simulations. LAMDA allows domain experts to systematically explore state-to-state transitions, categorizing them hierarchically and enabling analysis at different resolutions. The system simplifies the analysis process by capturing user inputs through annotations and allows scientists to visualize commonalities between different transitions, ultimately aiding in understanding and predicting the evolution of physical properties in materials science research."
  },
  {
    "title": "Who Owns My AI Twin? Data Ownership in a New World of Simulated Identities",
    "abstract": "The emergence of AI twins, digital replicas that encapsulate an individual's knowledge, memories, psychological traits, and behavioral patterns, raises novel legal and ethical challenges for data governance and personal identity. Built from personal data, these systems require a rethinking of what it means to exercise dominion over one's data and to maintain personal autonomy in an AI-mediated environment. This article argues that natural persons should be recognized as the moral and legal owners of their AI twins, which function as intimate extensions of the self rather than as proprietary technological artifacts. It critiques prevailing legal frameworks that prioritize technological infrastructure and platform control over data and individual autonomy, exposing their structural limitations. In response, the article advances a human-centric model of data governance grounded in individual dominion and a private-by-default principle. This approach proposes a reimagined social contract for AI-driven identities that strengthens personal agency, promotes equitable data stewardship, and better aligns legal norms with the socio-technical realities of AI twins.",
    "url": "https://arxiv.org/abs/2601.09877",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the concept of AI twins, digital replicas of individuals created from personal data, and the implications for data ownership and personal autonomy. It argues that individuals should be recognized as the owners of their AI twins, emphasizing the need for a human-centric model of data governance to protect personal agency and promote equitable data stewardship. The research critiques existing legal frameworks that prioritize technological control over individual autonomy and proposes a reimagined social contract for AI-driven identities to better align legal norms with the realities of AI twins."
  },
  {
    "title": "The Conversational Exam: A Scalable Assessment Design for the AI Era",
    "abstract": "Traditional assessment methods collapse when students use generative AI to complete work without genuine engagement, creating an illusion of competence where they believe they're learning but aren't. This paper presents the conversational exam -- a scalable oral examination format that restores assessment validity by having students code live while explaining their reasoning. Drawing on human-computer interaction principles, we examined 58 students in small groups across just two days, demonstrating that oral exams can scale to typical class sizes. The format combines authentic practice (students work with documentation and supervised AI access) with inherent validity (real-time performance cannot be faked). We provide detailed implementation guidance to help instructors adapt this approach, offering a practical path forward when many educators feel paralyzed between banning AI entirely or accepting that valid assessment is impossible.",
    "url": "https://arxiv.org/abs/2601.10691",
    "journal": "arXiv cs.HC",
    "ai_summary": "The study introduces the conversational exam as a scalable assessment design to combat the use of generative AI in student work, which can create a false sense of competence. By having students code live and explain their reasoning during oral examinations, the format restores assessment validity and authenticity. The research demonstrates that oral exams can be conducted in large class sizes, offering a practical solution for educators grappling with the challenge of assessing student learning in the AI era."
  },
  {
    "title": "Generative AI collective behavior needs an interactionist paradigm",
    "abstract": "In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.",
    "url": "https://arxiv.org/abs/2601.10567",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research article emphasizes the importance of understanding the collective behavior of agents in large language models (LLMs) and the implications it has on society. The authors argue that LLMs' unique characteristics require an interactionist paradigm to study how prior knowledge and values interact with social context to shape emergent phenomena in multi-agent generative AI systems. They propose four crucial directions for the development and deployment of LLM-based collectives, highlighting the need for alternative theoretical foundations, methodologies, and trans-disciplinary dialogue."
  },
  {
    "title": "AEQ-Bench: Measuring Empathy of Omni-Modal Large Models",
    "abstract": "While the automatic evaluation of omni-modal large models (OLMs) is essential, assessing empathy remains a significant challenge due to its inherent affectivity. To investigate this challenge, we introduce AEQ-Bench (Audio Empathy Quotient Benchmark), a novel benchmark to systematically assess two core empathetic capabilities of OLMs: (i) generating empathetic responses by comprehending affective cues from multi-modal inputs (audio + text), and (ii) judging the empathy of audio responses without relying on text transcription. Compared to existing benchmarks, AEQ-Bench incorporates two novel settings that vary in context specificity and speech tone. Comprehensive assessment across linguistic and paralinguistic metrics reveals that (1) OLMs trained with audio output capabilities generally outperformed models with text-only outputs, and (2) while OLMs align with human judgments for coarse-grained quality assessment, they remain unreliable for evaluating fine-grained paralinguistic expressiveness.",
    "url": "https://arxiv.org/abs/2601.10513",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces AEQ-Bench, a benchmark designed to assess the empathy capabilities of omni-modal large models (OLMs) by evaluating their ability to generate empathetic responses from audio and text inputs, as well as judge empathy in audio responses without text transcription. The study found that OLMs with audio output capabilities performed better than text-only models, and while OLMs aligned with human judgments for overall quality assessment, they were less reliable in evaluating fine-grained paralinguistic expressiveness. This research highlights the importance of assessing empathy in OLMs and the challenges associated with evaluating their emotional understanding and expression."
  },
  {
    "title": "Evolving with AI: A Longitudinal Analysis of Developer Logs",
    "abstract": "AI-powered coding assistants are rapidly becoming fixtures in professional IDEs, yet their sustained influence on everyday development remains poorly understood. Prior research has focused on short-term use or self-reported perceptions, leaving open questions about how sustained AI use reshapes actual daily coding practices in the long term. We address this gap with a mixed-method study of AI adoption in IDEs, combining longitudinal two-year fine-grained telemetry from 800 developers with a survey of 62 professionals. We analyze five dimensions of workflow change: productivity, code quality, code editing, code reuse, and context switching. Telemetry reveals that AI users produce substantially more code but also delete significantly more. Meanwhile, survey respondents report productivity gains and perceive minimal changes in other dimensions. Our results offer empirical insights into the silent restructuring of software workflows and provide implications for designing future AI-augmented tooling.",
    "url": "https://arxiv.org/abs/2601.10258",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research study examines the long-term impact of AI-powered coding assistants on developers' daily coding practices. Through a two-year longitudinal analysis of 800 developers' telemetry data and a survey of 62 professionals, the study found that AI users produce more code but also delete more. While survey respondents reported productivity gains, there were minimal perceived changes in other dimensions of workflow. These findings shed light on how AI adoption is silently reshaping software workflows and provide insights for designing future AI-augmented tools."
  },
  {
    "title": "Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends",
    "abstract": "In recent years, with the rapid advancement of large language models (LLMs), role-playing language agents (RPLAs) have emerged as a prominent research focus at the intersection of natural language processing (NLP) and human-computer interaction. This paper systematically reviews the current development and key technologies of RPLAs, delineating the technological evolution from early rule-based template paradigms, through the language style imitation stage, to the cognitive simulation stage centered on personality modeling and memory mechanisms. It summarizes the critical technical pathways supporting high-quality role-playing, including psychological scale-driven character modeling, memory-augmented prompting mechanisms, and motivation-situation-based behavioral decision control. At the data level, the paper further analyzes the methods and challenges of constructing role-specific corpora, focusing on data sources, copyright constraints, and structured annotation processes. In terms of evaluation, it collates multi-dimensional assessment frameworks and benchmark datasets covering role knowledge, personality fidelity, value alignment, and interactive hallucination, while commenting on the advantages and disadvantages of methods such as human evaluation, reward models, and LLM-based scoring. Finally, the paper outlines future development directions of role-playing agents, including personality evolution modeling, multi-agent collaborative narrative, multimodal immersive interaction, and integration with cognitive neuroscience, aiming to provide a systematic perspective and methodological insights for subsequent research.",
    "url": "https://arxiv.org/abs/2601.10122",
    "journal": "arXiv cs.HC",
    "ai_summary": "This paper reviews the development of role-playing language agents (RPLAs) driven by large language models (LLMs), highlighting the evolution from rule-based templates to cognitive simulation with personality modeling and memory mechanisms. It discusses key technical pathways for high-quality role-playing, challenges in constructing role-specific corpora, and evaluation frameworks for assessing role knowledge, personality fidelity, and interactive hallucination. The paper also outlines future trends for RPLAs, such as personality evolution modeling, multi-agent collaborative narrative, and integration with cognitive neuroscience, providing insights for future research in this area."
  },
  {
    "title": "A Sustainable AI Economy Needs Data Deals That Work for Generators",
    "abstract": "We argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality: each state in the data cycle from inputs to model weights to synthetic outputs refines technical signal but strips economic equity from data generators. We show, by analyzing seventy-three public data deals, that the majority of value accrues to aggregators, with documented creator royalties rounding to zero and widespread opacity of deal terms. This is not just an economic welfare concern: as data and its derivatives become economic assets, the feedback loop that sustains current learning algorithms is at risk. We identify three structural faults - missing provenance, asymmetric bargaining power, and non-dynamic pricing - as the operational machinery of this inequality. In our analysis, we trace these problems along the machine learning value chain and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants. Finally, we outline research directions where our community can make concrete contributions to data deals and contextualize our position with related and orthogonal viewpoints.",
    "url": "https://arxiv.org/abs/2601.09966",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research argues that the current machine learning value chain is unsustainable due to an economic data processing inequality that benefits aggregators at the expense of data generators. The study of seventy-three public data deals shows that creator royalties are minimal and deal terms are often opaque. The proposed Equitable Data-Value Exchange (EDVEX) Framework aims to create a more fair market for all participants in the data economy."
  },
  {
    "title": "Epistemology gives a Future to Complementarity in Human-AI Interactions",
    "abstract": "Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.",
    "url": "https://arxiv.org/abs/2601.09871",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the concept of human-AI complementarity in decision-making processes, highlighting its theoretical challenges and lack of empirical support. The study proposes reframing complementarity within the discourse of justificatory AI, using epistemology to assess the reliability of human-AI interactions. By focusing on the alignment with epistemic standards and socio-technical practices, complementarity can contribute to the reliability of AI-supported decision-making processes, ultimately shaping everyday life."
  },
  {
    "title": "A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Model-Based Conversational Agents",
    "abstract": "Anthropomorphisation -- the phenomenon whereby non-human entities are ascribed human-like qualities -- has become increasingly salient with the rise of large language model (LLM)-based conversational agents (CAs). Unlike earlier chatbots, LLM-based CAs routinely generate interactional and linguistic cues, such as first-person self-reference, epistemic and affective expressions that empirical work shows can increase engagement. On the other hand, anthropomorphisation raises ethical concerns, including deception, overreliance, and exploitative relationship framing, while some authors argue that anthropomorphic interaction may support autonomy, well-being, and inclusion. Despite increasing interest in the phenomenon, literature remains fragmented across domains and varies substantially in how it defines, operationalizes, and normatively evaluates anthropomorphisation. This scoping review maps ethically oriented work on anthropomorphising LLM-based CAs across five databases and three preprint repositories. We synthesize (1) conceptual foundations, (2) ethical challenges and opportunities, and (3) methodological approaches. We find convergence on attribution-based definitions but substantial divergence in operationalization, a predominantly risk-forward normative framing, and limited empirical work that links observed interaction effects to actionable governance guidance. We conclude with a research agenda and design/governance recommendations for ethically deploying anthropomorphic cues in LLM-based conversational agents.",
    "url": "https://arxiv.org/abs/2601.09869",
    "journal": "arXiv cs.HC",
    "ai_summary": "This scoping review explores the ethical perspectives on anthropomorphising large language model-based conversational agents (CAs). The study highlights the potential benefits of anthropomorphisation, such as increased engagement, as well as the ethical concerns it raises, including deception and exploitative relationships. The review calls for further research to better understand the impact of anthropomorphic cues in CAs and provides recommendations for ethically deploying these technologies."
  },
  {
    "title": "How Human Motion Prediction Quality Shapes Social Robot Navigation Performance in Constrained Spaces",
    "abstract": "Motivated by the vision of integrating mobile robots closer to humans in warehouses, hospitals, manufacturing plants, and the home, we focus on robot navigation in dynamic and spatially constrained environments. Ensuring human safety, comfort, and efficiency in such settings requires that robots are endowed with a model of how humans move around them. Human motion prediction around robots is especially challenging due to the stochasticity of human behavior, differences in user preferences, and data scarcity. In this work, we perform a methodical investigation of the effects of human motion prediction quality on robot navigation performance, as well as human productivity and impressions. We design a scenario involving robot navigation among two human subjects in a constrained workspace and instantiate it in a user study ($N=80$) involving two different robot platforms, conducted across two sites from different world regions. Key findings include evidence that: 1) the widely adopted average displacement error is not a reliable predictor of robot navigation performance and human impressions; 2) the common assumption of human cooperation breaks down in constrained environments, with users often not reciprocating robot cooperation, and causing performance degradations; 3) more efficient robot navigation often comes at the expense of human efficiency and comfort.",
    "url": "https://arxiv.org/abs/2601.09856",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on the impact of human motion prediction quality on robot navigation performance in constrained spaces. The study found that the commonly used average displacement error is not a reliable predictor of robot navigation performance and human impressions. Additionally, it was discovered that more efficient robot navigation can sometimes lead to decreased human efficiency and comfort, highlighting the importance of considering human factors in robot design for dynamic environments."
  },
  {
    "title": "ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning",
    "abstract": "Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\\%$ in VQA accuracy without increasing processing load.",
    "url": "https://arxiv.org/abs/2601.09851",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces a new metric called ViSIL, which quantifies the information loss in multimodal video captioning by comparing the summary to the original video content. This metric allows for a more accurate evaluation of the effectiveness of generative AI models in summarizing videos and optimizing the trade-off between information loss and processing speed. The results show that ViSIL scores correlate with human and VLM performance on VQA tasks and outperform traditional text summaries in VQA accuracy."
  }
]