[
  {
    "title": "VR as a \"Drop-In\" Well-being Tool for Knowledge Workers",
    "abstract": "Virtual Reality (VR) is increasingly being used to support workplace well-being, but many interventions focus narrowly on a single activity or goal. Our work explores how VR can meet the diverse physical and mental needs of knowledge workers. We developed Tranquil Loom, a VR app offering stretching, guided meditation, and open exploration across four environments. The app includes an AI assistant that suggests activities based on users' emotional states. We conducted a two-phase mixed-methods study: (1) interviews with 10 knowledge workers to guide the app's design, and (2) deployment with 35 participants gathering usage data, well-being measures, and interviews. Results showed increases in mindfulness and reductions in anxiety. Participants enjoyed both structured and open-ended activities, often using the app playfully. While AI suggestions were used infrequently, they prompted ideas for future personalization. Overall, participants viewed VR as a flexible, ``drop-in'' tool, highlighting its value for situational rather than prescriptive well-being support.",
    "url": "https://arxiv.org/abs/2510.02836",
    "journal": "arXiv cs.HC",
    "ai_summary": "The study explores how VR can support the well-being of knowledge workers by offering a variety of activities through the Tranquil Loom app, including stretching, guided meditation, and open exploration. Results showed increases in mindfulness and reductions in anxiety among participants, who found the app to be a flexible and enjoyable tool for well-being support. While AI suggestions were used infrequently, they inspired ideas for future personalization, highlighting the potential of VR as a \"drop-in\" tool for situational well-being support."
  },
  {
    "title": "PromptMap: Supporting Exploratory Text-to-Image Generation",
    "abstract": "Text-to-image generative models can be tremendously valuable in supporting creative tasks by providing inspirations and enabling quick exploration of different design ideas. However, one common challenge is that users may still not be able to find anything useful after many hours and hundreds of images. Without effective help, users can easily get lost in the vast design space, forgetting what has been tried and what has not. In this work, we first propose the Design-Exploration model to formalize the exploration process. Based on this model, we create an interactive visualization system, PromptMap, to support exploratory text-to-image generation. Our system provides a new visual representation that better matches the non-linear nature of such processes, making them easier to understand and follow. It utilizes novel visual representations and intuitive interactions to help users structure the many possibilities that they can explore. We evaluated the system through in-depth interviews with users.",
    "url": "https://arxiv.org/abs/2510.02814",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces PromptMap, an interactive visualization system designed to support exploratory text-to-image generation. By formalizing the exploration process with the Design-Exploration model, the system helps users navigate the vast design space more effectively. Through in-depth interviews with users, the system was found to provide a new visual representation that matches the non-linear nature of the process, making it easier for users to structure and explore different design possibilities."
  },
  {
    "title": "Fostering Collective Discourse: A Distributed Role-Based Approach to Online News Commenting",
    "abstract": "Current news commenting systems are designed based on implicitly individualistic assumptions, where discussion is the result of a series of disconnected opinions. This often results in fragmented and polarized conversations that fail to represent the spectrum of public discourse. In this work, we develop a news commenting system where users take on distributed roles to collaboratively structure the comments to encourage a connected, balanced discussion space. Through a within-subject, mixed-methods evaluation (N=38), we find that the system supported three stages of participation: understanding issues, collaboratively structuring comments, and building a discussion. With our system, users' comments displayed more balanced perspectives and a more emotionally neutral argumentation. Simultaneously, we observed reduced argument strength compared to a traditional commenting system, indicating a trade-off between inclusivity and depth. We conclude with design considerations and trade-offs for introducing distributed roles in news commenting system design.",
    "url": "https://arxiv.org/abs/2510.02766",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research proposes a new approach to online news commenting that involves users taking on distributed roles to structure comments collaboratively. The study found that this system led to more balanced perspectives and emotionally neutral arguments, but also resulted in reduced argument strength compared to traditional systems. The findings suggest that incorporating distributed roles in news commenting systems can foster more connected and inclusive discussions, with potential trade-offs in depth of argumentation."
  },
  {
    "title": "Prototyping Digital Social Spaces through Metaphor-Driven Design: Translating Spatial Concepts into an Interactive Social Simulation",
    "abstract": "Social media platforms are central to communication, yet their designs remain narrowly focused on engagement and scale. While researchers have proposed alternative visions for online spaces, these ideas are difficult to prototype within platform constraints. In this paper, we introduce a metaphor-driven system to help users imagine and explore new social media environments. The system translates users' metaphors into structured sets of platform features and generates interactive simulations populated with LLM-driven agents. To evaluate this approach, we conducted a study where participants created and interacted with simulated social media spaces. Our findings show that metaphors allow users to express distinct social expectations, and that perceived authenticity of the simulation depended on how well it captured dynamics like intimacy, participation, and temporal engagement. We conclude by discussing how metaphor-driven simulation can be a powerful design tool for prototyping alternative social architectures and expanding the design space for future social platforms.",
    "url": "https://arxiv.org/abs/2510.02759",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a metaphor-driven system to prototype new social media environments, allowing users to express distinct social expectations through metaphors. The study found that the perceived authenticity of the simulation depended on how well it captured dynamics like intimacy, participation, and temporal engagement. This approach has the potential to expand the design space for future social platforms by allowing for the exploration of alternative social architectures."
  },
  {
    "title": "\"It Felt Real\" Victim Perspectives on Platform Design and Longer-Running Scams",
    "abstract": "Longer-running scams, such as romance fraud and \"pig-butchering\" scams, exploit not only victims' emotions but also the design of digital platforms. Scammers commonly leverage features such as professional-looking profile verification, algorithmic recommendations that reinforce contact, integrated payment systems, and private chat affordances to gradually establish trust and dependency with victims. Prior work in HCI and criminology has examined online scams through the lenses of detection mechanisms, threat modeling, and user-level vulnerabilities. However, less attention has been paid to how platform design itself enables longer-running scams. To address this gap, we conducted in-depth interviews with 25 longer-running scam victims in China. Our findings show how scammers strategically use platform affordances to stage credibility, orchestrate intimacy, and sustain coercion with victims. By analyzing scams as socio-technical projects, we highlight how platform design can be exploited in longer-running scams, and point to redesigning future platforms to better protect users.",
    "url": "https://arxiv.org/abs/2510.02680",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores how longer-running scams exploit victims' emotions and platform design features to establish trust and dependency. By conducting interviews with scam victims in China, the study reveals how scammers strategically use platform affordances to stage credibility, orchestrate intimacy, and sustain coercion with victims. The findings emphasize the importance of redesigning digital platforms to better protect users from falling victim to such scams."
  },
  {
    "title": "When Researchers Say Mental Model/Theory of Mind of AI, What Are They Really Talking About?",
    "abstract": "When researchers claim AI systems possess ToM or mental models, they are fundamentally dis- cussing behavioral predictions and bias corrections rather than genuine mental states. This position paper argues that the current discourse conflates sophisticated pattern matching with authentic cog- nition, missing a crucial distinction between simulation and experience. While recent studies show LLMs achieving human-level performance on ToM laboratory tasks, these results are based only on behavioral mimicry. More importantly, the entire testing paradigm may be flawed in applying individual human cognitive tests to AI systems, but assessing human cognition directly in the moment of human-AI interaction. I suggest shifting focus toward mutual ToM frameworks that acknowledge the simultaneous contributions of human cognition and AI algorithms, emphasizing the interaction dynamics, instead of testing AI in isolation.",
    "url": "https://arxiv.org/abs/2510.02660",
    "journal": "arXiv cs.HC",
    "ai_summary": "This position paper argues that when researchers claim AI systems possess Theory of Mind (ToM) or mental models, they are actually referring to behavioral predictions and bias corrections rather than genuine mental states. While recent studies show AI systems achieving human-level performance on ToM tasks, this is based on behavioral mimicry rather than true understanding. The paper suggests shifting focus towards mutual ToM frameworks that consider the interaction dynamics between human cognition and AI algorithms, rather than testing AI in isolation."
  },
  {
    "title": "Open WebUI: An Open, Extensible, and Usable Interface for AI Interaction",
    "abstract": "While LLMs enable a range of AI applications, interacting with multiple models and customizing workflows can be challenging, and existing LLM interfaces offer limited support for collaborative extension or real-world evaluation. In this work, we present an interface toolkit for LLMs designed to be open (open-source and local), extensible (plugin support and users can interact with multiple models), and usable. The extensibility is enabled through a two-pronged plugin architecture and a community platform for sharing, importing, and adapting extensions. To evaluate the system, we analyzed organic engagement through social platforms, conducted a user survey, and provided notable examples of the toolkit in the wild. Through studying how users engage with and extend the toolkit, we show how extensible, open LLM interfaces provide both functional and social value, and highlight opportunities for future HCI work on designing LLM toolkit platforms and shaping local LLM-user interaction.",
    "url": "https://arxiv.org/abs/2510.02546",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces an open and extensible interface toolkit for Large Language Models (LLMs) to facilitate interaction with multiple models and customize workflows. The toolkit allows for plugin support and user collaboration, with a focus on usability and community sharing. Through social platform engagement and user surveys, the study demonstrates the value of extensible LLM interfaces in both functionality and social interaction, suggesting opportunities for future work in designing LLM toolkit platforms."
  },
  {
    "title": "Vector Autoregression (VAR) of Longitudinal Sleep and Self-report Mood Data",
    "abstract": "Self-tracking is one of many behaviors involved in the long-term self-management of chronic illnesses. As consumer-grade wearable sensors have made the collection of health-related behaviors commonplace, the quality, volume, and availability of such data has dramatically improved. This exploratory longitudinal N-of-1 study quantitatively assesses four years of sleep data captured via the Oura Ring, a consumer-grade sleep tracking device, along with self-reported mood data logged using eMood Tracker for iOS. After assessing the data for stationarity and computing the appropriate lag-length selection, a vector autoregressive (VAR) model was fit along with Granger causality tests to assess causal mechanisms within this multivariate time series. Oura's nightly sleep quality score was shown to Granger-cause the presence of depressed and anxious moods using a VAR(2) model.",
    "url": "https://arxiv.org/abs/2510.02511",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study analyzed four years of sleep data collected from the Oura Ring and self-reported mood data from the eMood Tracker app to explore the relationship between sleep quality and mood. The findings revealed that the quality of sleep, as measured by the Oura Ring, was found to have a causal effect on the presence of depressed and anxious moods. This research highlights the potential of using wearable sensors and self-tracking apps to better understand and manage chronic illnesses through personalized data analysis."
  },
  {
    "title": "AI Generated Child Sexual Abuse Material - What's the Harm?",
    "abstract": "The development of generative artificial intelligence (AI) tools capable of producing wholly or partially synthetic child sexual abuse material (AI CSAM) presents profound challenges for child protection, law enforcement, and societal responses to child exploitation. While some argue that the harmfulness of AI CSAM differs fundamentally from other CSAM due to a perceived absence of direct victimization, this perspective fails to account for the range of risks associated with its production and consumption. AI has been implicated in the creation of synthetic CSAM of children who have not previously been abused, the revictimization of known survivors of abuse, the facilitation of grooming, coercion and sexual extortion, and the normalization of child sexual exploitation. Additionally, AI CSAM may serve as a new or enhanced pathway into offending by lowering barriers to engagement, desensitizing users to progressively extreme content, and undermining protective factors for individuals with a sexual interest in children. This paper provides a primer on some key technologies, critically examines the harms associated with AI CSAM, and cautions against claims that it may function as a harm reduction tool, emphasizing how some appeals to harmlessness obscure its real risks and may contribute to inertia in ecosystem responses.",
    "url": "https://arxiv.org/abs/2510.02978",
    "journal": "arXiv cs.HC",
    "ai_summary": "The development of AI tools capable of generating synthetic child sexual abuse material presents significant challenges for child protection and law enforcement. This AI-generated material can lead to revictimization, facilitation of grooming and coercion, and normalization of child sexual exploitation. Claims that AI CSAM may reduce harm are cautioned against, as they may obscure the real risks and hinder effective responses to this issue."
  },
  {
    "title": "AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models",
    "abstract": "Multi-agent systems powered by large language models have demonstrated remarkable capabilities across diverse domains, yet existing automated design approaches seek monolithic solutions that fail to adapt resource allocation based on query complexity and domain requirements. This paper introduces AutoMaAS, a self-evolving multi-agent architecture search framework that leverages neural architecture search principles to automatically discover optimal agent configurations through dynamic operator lifecycle management and automated machine learning techniques. Our approach incorporates four key innovations: (1) automatic operator generation, fusion, and elimination based on performance-cost analysis, (2) dynamic cost-aware optimization with real-time parameter adjustment, (3) online feedback integration for continuous architecture refinement, and (4) enhanced interpretability through decision tracing mechanisms. Extensive experiments across six benchmarks demonstrate that AutoMaAS achieves 1.0-7.1\\% performance improvement while reducing inference costs by 3-5\\% compared to state-of-the-art methods. The framework shows superior transferability across datasets and LLM backbones, establishing a new paradigm for automated multi-agent system design in the era of large language models.",
    "url": "https://arxiv.org/abs/2510.02669",
    "journal": "arXiv cs.HC",
    "ai_summary": "The paper introduces AutoMaAS, a self-evolving multi-agent architecture search framework that uses neural architecture search principles to optimize agent configurations based on query complexity and domain requirements. The framework incorporates automatic operator generation, dynamic cost-aware optimization, online feedback integration, and enhanced interpretability, resulting in 1.0-7.1% performance improvement and 3-5% reduction in inference costs compared to existing methods. This research establishes a new paradigm for automated multi-agent system design in the context of large language models, demonstrating superior transferability across datasets and LLM backbones."
  },
  {
    "title": "Who's Wearing? Ear Canal Biometric Key Extraction for User Authentication on Wireless Earbuds",
    "abstract": "Ear canal scanning/sensing (ECS) has emerged as a novel biometric authentication method for mobile devices paired with wireless earbuds. Existing studies have demonstrated the uniqueness of ear canals by training and testing machine learning classifiers on ECS data. However, implementing practical ECS-based authentication requires preventing raw biometric data leakage and designing computationally efficient protocols suitable for resource-constrained earbuds. To address these challenges, we propose an ear canal key extraction protocol, \\textbf{EarID}. Without relying on classifiers, EarID extracts unique binary keys directly on the earbuds during authentication. These keys further allow the use of privacy-preserving fuzzy commitment scheme that verifies the wearer's key on mobile devices. Our evaluation results demonstrate that EarID achieves a 98.7\\% authentication accuracy, comparable to machine learning classifiers. The mobile enrollment time (160~ms) and earbuds processing time (226~ms) are negligible in terms of wearer's experience. Moreover, our approach is robust and attack-resistant, maintaining a false acceptance rate below 1\\% across all adversarial scenarios. We believe the proposed EarID offers a practical and secure solution for next-generation wireless earbuds.",
    "url": "https://arxiv.org/abs/2510.02563",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research focuses on using ear canal scanning/sensing as a biometric authentication method for wireless earbuds. The proposed protocol, EarID, extracts unique binary keys directly on the earbuds during authentication, eliminating the need for machine learning classifiers. The results show that EarID achieves high authentication accuracy, low processing times, and robustness against attacks, making it a practical and secure solution for next-generation wireless earbuds."
  },
  {
    "title": "ERUPT: An Open Toolkit for Interfacing with Robot Motion Planners in Extended Reality",
    "abstract": "We propose the Extended Reality Universal Planning Toolkit (ERUPT), an extended reality (XR) system for interactive motion planning. Our system allows users to create and dy- namically reconfigure environments while they plan robot paths. In immersive three-dimensional XR environments, users gain a greater spatial understanding. XR also unlocks a broader range of natural interaction capabilities, allowing users to grab and adjust objects in the environment similarly to the real world, rather than using a mouse and keyboard with the scene projected onto a two-dimensional computer screen. Our system integrates with MoveIt, a manipulation planning framework, allowing users to send motion planning requests and visualize the resulting robot paths in virtual or augmented reality. We provide a broad range of interaction modalities, allowing users to modify objects in the environment and interact with a virtual robot. Our system allows operators to visualize robot motions, ensuring desired behavior as it moves throughout the environment, without risk of collisions within a virtual space, and to then deploy planned paths on physical robots in the real world.",
    "url": "https://arxiv.org/abs/2510.02464",
    "journal": "arXiv cs.HC",
    "ai_summary": "The ERUPT toolkit is introduced as an extended reality system for interactive motion planning, allowing users to create and modify environments while planning robot paths in immersive XR environments. The system integrates with MoveIt, enabling users to visualize robot paths in virtual or augmented reality and interact with virtual robots using a variety of natural interaction capabilities. This tool allows operators to visualize and test robot motions in a virtual space before deploying planned paths on physical robots in the real world, ultimately improving spatial understanding and reducing the risk of collisions."
  },
  {
    "title": "EvolveCaptions: Empowering DHH Users Through Real-Time Collaborative Captioning",
    "abstract": "Automatic Speech Recognition (ASR) systems often fail to accurately transcribe speech from Deaf and Hard of Hearing (DHH) individuals, especially during real-time conversations. Existing personalization approaches typically require extensive pre-recorded data and place the burden of adaptation on the DHH speaker. We present EvolveCaptions, a real-time, collaborative ASR adaptation system that supports in-situ personalization with minimal effort. Hearing participants correct ASR errors during live conversations. Based on these corrections, the system generates short, phonetically targeted prompts for the DHH speaker to record, which are then used to fine-tune the ASR model. In a study with 12 DHH and six hearing participants, EvolveCaptions reduced Word Error Rate (WER) across all DHH users within one hour of use, using only five minutes of recording time on average. Participants described the system as intuitive, low-effort, and well-integrated into communication. These findings demonstrate the promise of collaborative, real-time ASR adaptation for more equitable communication.",
    "url": "https://arxiv.org/abs/2510.02181",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research presents EvolveCaptions, a real-time, collaborative ASR adaptation system that improves transcription accuracy for Deaf and Hard of Hearing (DHH) individuals during live conversations. By having hearing participants correct ASR errors and generating prompts for DHH speakers to record, the system significantly reduced Word Error Rate (WER) within one hour of use. This approach offers a promising solution for more equitable communication by providing in-situ personalization with minimal effort for DHH users."
  },
  {
    "title": "Agentic Reasoning and Refinement through Semantic Interaction",
    "abstract": "Sensemaking report writing often requires multiple refinements in the iterative process. While Large Language Models (LLMs) have shown promise in generating initial reports based on human visual workspace representations, they struggle to precisely incorporate sequential semantic interactions during the refinement process. We introduce VIS-ReAct, a framework that reasons about newly-added semantic interactions in visual workspaces to steer the LLM for report refinement.\nVIS-ReAct is a two-agent framework: a primary LLM analysis agent interprets new semantic interactions to infer user intentions and generate refinement planning, followed by an LLM refinement agent that updates reports accordingly. Through case study, VIS-ReAct outperforms baseline and VIS-ReAct (without LLM analysis) on targeted refinement, semantic fidelity, and transparent inference. Results demonstrate that VIS-ReAct better handles various interaction types and granularities while enhancing the transparency of human-LLM collaboration.",
    "url": "https://arxiv.org/abs/2510.02157",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces VIS-ReAct, a framework that utilizes two agents to incorporate sequential semantic interactions during the refinement process of sensemaking report writing. The primary LLM analysis agent interprets new semantic interactions to infer user intentions and generate refinement planning, followed by the LLM refinement agent that updates reports accordingly. Results show that VIS-ReAct outperforms baseline models in targeted refinement, semantic fidelity, and transparent inference, demonstrating its ability to handle various interaction types and improve human-LLM collaboration transparency."
  },
  {
    "title": "Human-Robo-advisor collaboration in decision-making: Evidence from a multiphase mixed methods experimental study",
    "abstract": "Robo-advisors (RAs) are cost-effective, bias-resistant alternatives to human financial advisors, yet adoption remains limited. While prior research has examined user interactions with RAs, less is known about how individuals interpret RA roles and integrate their advice into decision-making. To address this gap, this study employs a multiphase mixed methods design integrating a behavioral experiment (N = 334), thematic analysis, and follow-up quantitative testing. Findings suggest that people tend to rely on RAs, with reliance shaped by information about RA performance and the framing of advice as gains or losses. Thematic analysis reveals three RA roles in decision-making and four user types, each reflecting distinct patterns of advice integration. In addition, a 2 x 2 typology categorizes antecedents of acceptance into enablers and inhibitors at both the individual and algorithmic levels. By combining behavioral, interpretive, and confirmatory evidence, this study advances understanding of human-RA collaboration and provides actionable insights for designing more trustworthy and adaptive RA systems.",
    "url": "https://arxiv.org/abs/2510.02153",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores how individuals interpret and integrate advice from robo-advisors (RAs) in decision-making processes. Findings suggest that people tend to rely on RAs, with reliance influenced by factors such as RA performance and advice framing. The study identifies three RA roles in decision-making, four user types, and categorizes antecedents of acceptance into enablers and inhibitors, providing insights for designing more trustworthy and adaptive RA systems."
  },
  {
    "title": "Komitee Equal Shares: Choosing Together as Voters and as Groups with a Co-designed Virtual Budget Algorithm",
    "abstract": "Public funding processes demand fairness, learning, and outcomes that participants can understand. We introduce Komitee Equal Shares, a priceable virtual-budget allocation framework that integrates two signals: in voter mode, participants cast point votes; in evaluator mode, small groups assess proposals against collectively defined impact fields. The framework extends the Method of Equal Shares by translating both signals into virtual spending power and producing voting receipts. We deployed the framework in the 2025 Kultur Komitee in Winterthur, Switzerland. Our contributions are: (1) a clear separation of decision modes, addressing a gap in social choice that typically treats participatory budgeting as preference aggregation while citizens also see themselves as evaluators; and (2) the design of voting receipts that operationalise priceability into participant-facing explanations, making proportional allocations legible and traceable. The framework generalises to participatory grant-making and budgeting, offering a model where citizens act as voters and evaluators within one proportional, explainable allocation.",
    "url": "https://arxiv.org/abs/2510.02040",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Komitee Equal Shares, a virtual-budget allocation framework that allows participants to vote and evaluate proposals in a fair and understandable manner. By translating votes and evaluations into virtual spending power and providing voting receipts, the framework addresses the gap in social choice by allowing citizens to act as both voters and evaluators. The framework was deployed in the 2025 Kultur Komitee in Winterthur, Switzerland, demonstrating its applicability to participatory grant-making and budgeting processes."
  },
  {
    "title": "Who is responsible? Social Identity, Robot Errors and Blame Attribution",
    "abstract": "This paper argues that conventional blame practices fall short of capturing the complexity of moral experiences, neglecting power dynamics and discriminatory social practices. It is evident that robots, embodying roles linked to specific social groups, pose a risk of reinforcing stereotypes of how these groups behave or should behave, so they set a normative and descriptive standard. In addition, we argue that faulty robots might create expectations of who is supposed to compensate and repair after their errors, where social groups that are already disadvantaged might be blamed disproportionately if they do not act according to their ascribed roles. This theoretical and empirical gap becomes even more urgent to address as there have been indications of potential carryover effects from Human-Robot Interactions (HRI) to Human-Human Interactions (HHI). We therefore urge roboticists and designers to stay in an ongoing conversation about how social traits are conceptualised and implemented in this technology. We also argue that one solution could be to 'embrace the glitch' and to focus on constructively disrupting practices instead of prioritizing efficiency and smoothness of interaction above everything else. Apart from considering ethical aspects in the design phase of social robots, we see our analysis as a call for more research on the consequences of robot stereotyping and blame attribution.",
    "url": "https://arxiv.org/abs/2510.01862",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper highlights the importance of considering social identity and power dynamics in attributing blame for robot errors. It suggests that robots can reinforce stereotypes and create expectations for certain social groups to compensate for their mistakes. The study calls for ongoing conversation among roboticists and designers to address these issues and emphasizes the need for more research on the consequences of robot stereotyping and blame attribution."
  },
  {
    "title": "Towards Human-Centered RegTech: Unpacking Professionals' Strategies and Needs for Using LLMs Safely",
    "abstract": "Large Language Models are profoundly changing work patterns in high-risk professional domains, yet their application also introduces severe and underexplored compliance risks. To investigate this issue, we conducted semi-structured interviews with 24 highly-skilled knowledge workers from industries such as law, healthcare, and finance. The study found that these experts are commonly concerned about sensitive information leakage, intellectual property infringement, and uncertainty regarding the quality of model outputs. In response, they spontaneously adopt various mitigation strategies, such as actively distorting input data and limiting the details in their prompts. However, the effectiveness of these spontaneous efforts is limited due to a lack of specific compliance guidance and training for Large Language Models. Our research reveals a significant gap between current NLP tools and the actual compliance needs of experts. This paper positions these valuable empirical findings as foundational work for building the next generation of Human-Centered, Compliance-Driven Natural Language Processing for Regulatory Technology (RegTech), providing a critical human-centered perspective and design requirements for engineering NLP systems that can proactively support expert compliance workflows.",
    "url": "https://arxiv.org/abs/2510.01638",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the impact of Large Language Models (LLMs) on high-risk professional domains and the compliance risks they pose. Through interviews with knowledge workers, it was found that experts are concerned about information leakage, intellectual property infringement, and uncertainty about model outputs. The study highlights the need for specific compliance guidance and training for LLMs to bridge the gap between current NLP tools and the compliance needs of professionals, laying the groundwork for Human-Centered, Compliance-Driven Natural Language Processing for Regulatory Technology (RegTech)."
  },
  {
    "title": "TimeGazer: Temporal Modeling of Predictive Gaze Stabilization for AR Interaction",
    "abstract": "Gaze stabilization is critical for enabling fluid, accurate, and efficient interaction in immersive augmented reality (AR) environments, particularly during task-oriented visual behaviors. However, fixation sequences captured in active gaze tasks often exhibit irregular dispersion and systematic deviations from target locations, a variability primarily caused by the combined effects of human oculomotor physiology, insufficient AR headset tracking and calibration accuracy, and environmental disturbances, undermining interaction performance and visual engagement. To address this issue, we propose TimeGazer, which reformulates gaze stabilization as a sequence-to-sequence temporal regression problem, predicting idealized fixation trajectories for the target-fixation phase from historical gaze dynamics in the search phase. We present a synthetic data generation and blending strategy that produces spatially concentrated, target-centered fixation references aligned with task objectives, substantially enriching the training space and enhancing model generalization. We train and evaluate TimeGazer on a hybrid dataset of real and augmented gaze sequences collected via Microsoft HoloLens 2 from 54 participants across multiple prediction horizons. Through the user study, statistical results demonstrate that TimeGazer significantly improves interaction accuracy and reduces completion time, confirming that temporal modeling of predictive gaze stabilization can strengthen attentional consistency and responsiveness in task-driven AR interaction. These findings highlight the broader potential of TimeGazer for advancing adaptive gaze-based interfaces and temporal modeling research in immersive systems.",
    "url": "https://arxiv.org/abs/2510.01561",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces TimeGazer, a model that predicts idealized fixation trajectories for gaze stabilization in augmented reality environments by analyzing historical gaze dynamics. The model significantly improves interaction accuracy and reduces completion time, demonstrating the potential of temporal modeling of predictive gaze stabilization for enhancing attentional consistency and responsiveness in task-driven AR interaction. This research contributes to advancing adaptive gaze-based interfaces and temporal modeling in immersive systems."
  },
  {
    "title": "Dialogues with AI Reduce Beliefs in Misinformation but Build No Lasting Discernment Skills",
    "abstract": "Given the growing prevalence of fake information, including increasingly realistic AI-generated news, there is an urgent need to train people to better evaluate and detect misinformation. While interactions with AI have been shown to durably reduce people's beliefs in false information, it is unclear whether these interactions also teach people the skills to discern false information themselves. We conducted a month-long study where 67 participants classified news headline-image pairs as real or fake, discussed their assessments with an AI system, followed by an unassisted evaluation of unseen news items to measure accuracy before, during, and after AI assistance. While AI assistance produced immediate improvements during AI-assisted sessions (+21\\% average), participants' unassisted performance on new items declined significantly by week 4 (-15.3\\%). These results indicate that while AI may help immediately, it ultimately degrades long-term misinformation detection abilities.",
    "url": "https://arxiv.org/abs/2510.01537",
    "journal": "arXiv cs.HC",
    "ai_summary": "The study found that interacting with AI can reduce people's beliefs in misinformation in the short term, but does not improve their ability to discern false information on their own in the long term. While participants showed immediate improvements in identifying fake news with AI assistance, their unassisted performance declined significantly over time. This suggests that while AI can be helpful initially, it may not effectively teach individuals lasting skills to detect misinformation."
  },
  {
    "title": "From keywords to semantics: Perceptions of large language models in data discovery",
    "abstract": "Current approaches to data discovery match keywords between metadata and queries. This matching requires researchers to know the exact wording that other researchers previously used, creating a challenging process that could lead to missing relevant data. Large Language Models (LLMs) could enhance data discovery by removing this requirement and allowing researchers to ask questions with natural language. However, we do not currently know if researchers would accept LLMs for data discovery. Using a human-centered artificial intelligence (HCAI) focus, we ran focus groups (N = 27) to understand researchers' perspectives towards LLMs for data discovery. Our conceptual model shows that the potential benefits are not enough for researchers to use LLMs instead of current technology. Barriers prevent researchers from fully accepting LLMs, but features around transparency could overcome them. Using our model will allow developers to incorporate features that result in an increased acceptance of LLMs for data discovery.",
    "url": "https://arxiv.org/abs/2510.01473",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the potential of Large Language Models (LLMs) to improve data discovery by allowing researchers to ask questions in natural language instead of relying on specific keywords. The study found that while researchers recognize the benefits of LLMs, there are barriers preventing full acceptance, such as concerns around transparency. By understanding researchers' perspectives and incorporating features that address these barriers, developers can increase the acceptance and use of LLMs for data discovery."
  },
  {
    "title": "The Command Line GUIde: Graphical Interfaces from Man Pages via AI",
    "abstract": "Although birthed in the era of teletypes, the command line shell survived the graphical interface revolution of the 1980's and lives on in modern desktop operating systems. The command line provides access to powerful functionality not otherwise exposed on the computer, but requires users to recall textual syntax and carefully scour documentation. In contrast, graphical interfaces let users organically discover and invoke possible actions through widgets and menus. To better expose the power of the command line, we demonstrate a mechanism for automatically creating graphical interfaces for command line tools by translating their documentation (in the form of man pages) into interface specifications via AI. Using these specifications, our user-facing system, called GUIde, presents the command options to the user graphically. We evaluate the generated interfaces on a corpus of commands to show to what degree GUIde offers thorough graphical interfaces for users' real-world command line tasks.",
    "url": "https://arxiv.org/abs/2510.01453",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the use of AI to automatically create graphical interfaces for command line tools by translating their documentation into interface specifications. The system, called GUIde, aims to make the power of the command line more accessible to users by presenting command options graphically. The study evaluates the effectiveness of GUIde in providing comprehensive graphical interfaces for users' real-world command line tasks."
  },
  {
    "title": "Theory is Shapes",
    "abstract": "\"Theory figures\" are a staple of theoretical visualization research. Common shapes such as Cartesian planes and flowcharts can be used not only to explain conceptual contributions, but to think through and refine the contribution itself. Yet, theory figures tend to be limited to a set of standard shapes, limiting the creative and expressive potential of visualization theory. In this work, we explore how the shapes used in theory figures afford different understandings and explanations of their underlying phenomena. We speculate on the value of visualizing theories using more expressive configurations, such as icebergs, horseshoes, MÃ¶bius strips, and BLT sandwiches. By reflecting on figure-making's generative role in the practice of theorizing, we conclude that theory is, in fact, shapes.",
    "url": "https://arxiv.org/abs/2510.01382",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of different shapes in theory figures to enhance understanding and explanation of underlying phenomena. The study suggests that using more expressive shapes beyond standard ones like Cartesian planes can lead to more creative and insightful visualizations of theories. The findings highlight the importance of visualizing theories in diverse configurations to enrich the practice of theorizing."
  },
  {
    "title": "LegiScout: A Visual Tool for Understanding Complex Legislation",
    "abstract": "Modern legislative frameworks, such as the Affordable Care Act (ACA), often involve complex webs of agencies, mandates, and interdependencies. Government issued charts attempt to depict these structures but are typically static, dense, and difficult to interpret - even for experts. We introduce LegiScout, an interactive visualization system that transforms static policy diagrams into dynamic, force-directed graphs, enhancing comprehension while preserving essential relationships. By integrating data extraction, natural language processing, and computer vision techniques, LegiScout supports deeper exploration of not only the ACA but also a wide range of legislative and regulatory frameworks. Our approach enables stakeholders - policymakers, analysts, and the public - to navigate and understand the complexity inherent in modern law.",
    "url": "https://arxiv.org/abs/2510.01195",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces LegiScout, a visual tool that transforms static policy diagrams into dynamic, force-directed graphs, making complex legislative frameworks easier to understand. By integrating data extraction, natural language processing, and computer vision techniques, LegiScout allows stakeholders to navigate and comprehend not only the Affordable Care Act but also a variety of legislative and regulatory frameworks. This approach enhances comprehension and supports deeper exploration of modern law for policymakers, analysts, and the public."
  },
  {
    "title": "Development and Evaluation of an AI-Driven Telemedicine System for Prenatal Healthcare",
    "abstract": "Access to obstetric ultrasound is often limited in low-resource settings, particularly in rural areas of low- and middle-income countries. This work proposes a human-in-the-loop artificial intelligence (AI) system designed to assist midwives in acquiring diagnostically relevant fetal images using blind sweep protocols. The system incorporates a classification model along with a web-based platform for asynchronous specialist reviews. By identifying key frames in blind sweep studies, the AI system allows specialists to concentrate on interpretation rather than having to review entire videos. To evaluate its performance, blind sweep videos captured by a small group of soft-trained midwives using a low-cost Point-of-Care Ultrasound (POCUS) device were analyzed. The system demonstrated promising results in identifying standard fetal planes from sweeps made by non-experts. A field evaluation indicated good usability and a low cognitive workload, suggesting that it has the potential to expand access to prenatal imaging in underserved regions.",
    "url": "https://arxiv.org/abs/2510.01194",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research presents an AI-driven telemedicine system that assists midwives in acquiring diagnostically relevant fetal images using blind sweep protocols, particularly in low-resource settings. The system effectively identifies key frames in blind sweep studies, allowing specialists to focus on interpretation rather than reviewing entire videos. The system showed promising results in identifying standard fetal planes from sweeps made by non-experts, indicating its potential to expand access to prenatal imaging in underserved regions."
  },
  {
    "title": "How can AI agents support journalists' work? An experiment with designing an LLM-driven intelligent reporting system",
    "abstract": "The integration of artificial intelligence into journalistic practices represents a transformative shift in how news is gathered, analyzed, and disseminated. Large language models (LLMs), particularly those with agentic capabilities, offer unprecedented opportunities for enhancing journalistic workflows while simultaneously presenting complex challenges for newsroom integration. This research explores how agentic LLMs can support journalists' workflows, based on insights from journalist interviews and from the development of an LLM-based automation tool performing information filtering, summarization, and reporting. The paper details automated aggregation and summarization systems for journalists, presents a technical overview and evaluation of a user-centric LLM-driven reporting system (TeleFlash), and discusses both addressed and unmet journalist needs, with an outlook on future directions for AI-driven tools in journalism.",
    "url": "https://arxiv.org/abs/2510.01193",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the potential of agentic large language models (LLMs) in supporting journalists' workflows by developing an LLM-driven intelligent reporting system called TeleFlash. The study highlights the benefits of using LLMs for information filtering, summarization, and reporting in journalism, while also discussing the challenges and unmet needs of journalists in integrating AI tools into their work. The findings suggest that AI-driven tools like TeleFlash have the potential to revolutionize journalistic practices, offering both opportunities and complexities for newsroom integration."
  },
  {
    "title": "Better Than \"Better Than Nothing\": Design Strategies for Enculturated Empathetic AI Robot Companions for Older Adults",
    "abstract": "The paper asserts that emulating empathy in human-robot interaction is a key component to achieve satisfying social, trustworthy, and ethical robot interaction with older people. Following comments from older adult study participants, the paper identifies a gap. Despite the acceptance of robot care scenarios, participants expressed the poor quality of the social aspect. Current human-robot designs, to a certain extent, neglect to include empathy as a theorized design pathway. Using rhetorical theory, this paper defines the socio-cultural expectations for convincing empathetic relationships. It analyzes and then summarizes how society understands, values, and negotiates empathic interaction between human companions in discursive exchanges, wherein empathy acts as a societal value system. Using two public research collections on robots, with one geared specifically to gerontechnology for older people, it substantiates the lack of attention to empathy in public materials produced by robot companies. This paper contends that using an empathetic care vocabulary as a design pathway is a productive underlying foundation for designing humanoid social robots that aim to support older people's goals of aging-in-place. It argues that the integration of affective AI into the sociotechnical assemblages of human-socially assistive robot interaction ought to be scrutinized to ensure it is based on genuine cultural values involving empathetic qualities.",
    "url": "https://arxiv.org/abs/2510.01192",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper highlights the importance of incorporating empathy into AI robot companions for older adults to enhance social interaction and trust. It identifies a gap in current designs that neglect to include empathy as a key component. The paper argues that using an empathetic care vocabulary as a design pathway can lead to more satisfying and ethical relationships between older adults and AI robots aiming to support aging-in-place goals."
  },
  {
    "title": "An Optical Measurement System for Open-Source Tracking of Jaw Motions",
    "abstract": "Precise tracking of the jaw kinematics is crucial for diagnosing various musculoskeletal and neuromuscular diseases affecting the masticatory system and for advancing rehabilitative devices such as jaw exoskeletons, a hardly explored research field, to treat these disorders. We introduce an open-source, low-cost, precise, non-invasive, and biocompatible jaw tracking system based on optical motion capture technology to address the need for accessible and adaptable research tools. The system encompasses a complete pipeline from data acquisition, processing, and kinematic analysis to filtering, visualization, and data storage. We evaluated its performance and feasibility in experiments with four participants executing various jaw movements. The system demonstrated reliable kinematic tracking with an estimated precision of $(182 \\pm 47) {\\mu}m$ and $(0.126 \\pm 0.034) Â°$. Therefore, the open-source nature of the system and its utility comparable to commercial systems make it suitable for many research and development contexts, especially for applications such as the integration and design of jaw exoskeletons and customized diagnostic protocols. The complete system is available at GitHub with the aim of promoting innovation in temporomandibular disorders research and jaw assistive technology.",
    "url": "https://arxiv.org/abs/2510.01191",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces an open-source, low-cost, and precise optical measurement system for tracking jaw movements, which is essential for diagnosing and treating musculoskeletal and neuromuscular diseases affecting the masticatory system. The system demonstrated reliable kinematic tracking with high precision, making it suitable for various research and development contexts, particularly for applications like jaw exoskeletons and customized diagnostic protocols. The availability of the system on GitHub aims to promote innovation in temporomandibular disorders research and jaw assistive technology."
  },
  {
    "title": "An Anthropologist LLM to Elicit Users' Moral Preferences through Role-Play",
    "abstract": "This study investigates a novel approach to eliciting users' moral decision-making by combining immersive roleplaying games with LLM analysis capabilities. Building on the distinction introduced by Floridi between hard ethics inspiring and shaping laws-and soft ethics-moral preferences guiding individual behavior within the free space of decisions compliant to laws-we focus on capturing the latter through contextrich, narrative-driven interactions. Grounded in anthropological methods, the role-playing game exposes participants to ethically charged scenarios in the domain of digital privacy. Data collected during the sessions were interpreted by a customized LLM (\"GPT Anthropologist\"). Evaluation through a cross-validation process shows that both the richness of the data and the interpretive framing significantly enhance the model's ability to predict user behavior. Results show that LLMs can be effectively employed to automate and enhance the understanding of user moral preferences and decision-making process in the early stages of software development.",
    "url": "https://arxiv.org/abs/2510.01189",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores a new method of understanding users' moral decision-making by using immersive role-playing games and LLM analysis. By focusing on capturing users' moral preferences within the context of ethical scenarios, the research shows that LLMs can effectively predict user behavior and enhance the understanding of moral decision-making in software development. This approach can help automate and improve the early stages of software development by incorporating users' moral preferences."
  },
  {
    "title": "Beyond Divergence: Characterizing Co-exploration Patterns in Collaborative Design Processes",
    "abstract": "Exploration is crucial in the design process and is known for its essential role in fostering creativity and enhancing design outcomes. Within design teams, exploration evolves into co-exploration, a collaborative and dynamic practice that this study aims to unpack. To investigate this experience, we conducted a longitudinal observational study with 61 students across 16 design teams. Over five months of weekly diary-interviews, we uncovered the intricate dynamics of co-exploration. Our main contribution is a four-dimensional framework that identifies five distinct patterns of co-exploration activities. Our findings reveal how co-exploration emerges across various activities throughout the design process, demonstrating its role in different team interactions. It fosters a sense of togetherness, keeping design teams open-minded and engaged. This engagement cultivates collective intelligence, enabling teams to actively share knowledge, build upon each other's ideas, and achieve outcomes beyond individual contributions. Our study underscores the value of co-exploration, suggesting that it reflects the trajectory of design success and warrants further research. We also provide actionable insights, equipping future practitioners with strategies to enhance co-exploration in design collaborations.",
    "url": "https://arxiv.org/abs/2510.01188",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores the concept of co-exploration in design teams, identifying five distinct patterns of collaborative exploration activities. The findings highlight the importance of co-exploration in fostering creativity, enhancing design outcomes, and promoting collective intelligence within teams. The study provides a framework for understanding and improving co-exploration in design collaborations, suggesting that it is essential for achieving successful design outcomes."
  },
  {
    "title": "Manim for STEM Education: Visualizing Complex Problems Through Animation",
    "abstract": "Many STEM concepts pose significant learning challenges to students due to their inherent complexity and abstract nature. Visualizing complex problems through animations can significantly enhance learning outcomes. However, the creation of animations can be time-consuming and inconvenient. Hence, many educators illustrate complex concepts by hand on a board or a digital device. Although static graphics are helpful for understanding, they are less effective than animations. The free and open-source Python package Manim enables educators to create visually compelling animations easily. Python's straightforward syntax, combined with Manim's comprehensive set of built-in classes and methods, greatly simplifies implementation. This article presents a series of examples that demonstrate how Manim can be used to create animated video lessons for a variety of topics in computer science and mathematics. In addition, it analyzes viewer feedback collected across multiple social media platforms to evaluate the effectiveness and accessibility of these visualizations. The article further explores broader potentials of the Manim Python library by showcasing demonstrations that extend its applications to subject areas beyond computer science and mathematics.",
    "url": "https://arxiv.org/abs/2510.01187",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research discusses the use of the Manim Python package to create visually compelling animations for STEM education, which can enhance learning outcomes for complex concepts. The study demonstrates how Manim can be used to create animated video lessons in computer science and mathematics, and analyzes viewer feedback to evaluate the effectiveness of these visualizations. The research also explores the potential of the Manim library to extend its applications to other subject areas beyond computer science and mathematics."
  },
  {
    "title": "NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes",
    "abstract": "Reconstructing visual information from brain activity via computer vision technology provides an intuitive understanding of visual neural mechanisms. Despite progress in decoding fMRI data with generative models, achieving accurate cross-subject reconstruction of visual stimuli remains challenging and computationally demanding. This difficulty arises from inter-subject variability in neural representations and the brain's abstract encoding of core semantic features in complex visual inputs. To address these challenges, we propose NeuroSwift, which integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter is trained on Stable Diffusion generated images paired with COCO captions to emulate higher visual cortex encoding. For cross-subject generalization, we pretrain on one subject and then fine-tune only 17 percent of parameters (fully connected layers) for new subjects, while freezing other components. This enables state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), and it outperforms existing methods.",
    "url": "https://arxiv.org/abs/2510.02266",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces NeuroSwift, a lightweight framework for fMRI visual reconstruction of complex scenes that addresses challenges in accurate cross-subject reconstruction. The framework integrates AutoKL for low-level features and CLIP for semantics, achieving state-of-the-art performance with minimal training time on lightweight GPUs. By pretraining on one subject and fine-tuning only a small percentage of parameters for new subjects, NeuroSwift outperforms existing methods in decoding visual information from brain activity."
  },
  {
    "title": "Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers",
    "abstract": "Pose estimation refers to tracking a human's full body posture, including their head, torso, arms, and legs. The problem is challenging in practical settings where the number of body sensors are limited. Past work has shown promising results using conditional diffusion models, where the pose prediction is conditioned on both <location, rotation> measurements from the sensors. Unfortunately, nearly all these approaches generalize poorly across users, primarly because location measurements are highly influenced by the body size of the user. In this paper, we formulate pose estimation as an inverse problem and design an algorithm capable of zero-shot generalization. Our idea utilizes a pre-trained diffusion model and conditions it on rotational measurements alone; the priors from this model are then guided by a likelihood term, derived from the measured locations. Thus, given any user, our proposed InPose method generatively estimates the highly likely sequence of poses that best explains the sparse on-body measurements.",
    "url": "https://arxiv.org/abs/2510.02043",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on zero-shot human pose estimation using diffusion-based inverse solvers. The study addresses the challenge of generalizing pose estimation across users by conditioning the pose prediction on rotational measurements alone, rather than location measurements which are influenced by body size. The proposed InPose method utilizes a pre-trained diffusion model and a likelihood term derived from measured locations to generatively estimate the most likely sequence of poses based on sparse on-body measurements."
  },
  {
    "title": "Reducing Discomfort in Driving Simulators: Motion Cueing for Motion Sickness Mitigation",
    "abstract": "Driving simulators are increasingly used in research and development. However, simulators often cause motion sickness due to downscaled motion and unscaled veridical visuals. In this paper, a motion cueing algorithm is proposed that reduces motion sickness as predicted by the subjective vertical conflict (SVC) model using model predictive control (MPC). Both sensory conflict and specific force errors are penalised in the cost function, allowing the algorithm to jointly optimise fidelity and comfort.\nHuman-in-the-loop experiments were conducted to compare four simulator motion settings: two variations of our MPC-based algorithm, one focused on pure specific force tracking and the second compromising specific force tracking and motion sickness minimisation, as well as reference adaptive washout and no motion cases. The experiments were performed on a hexapod driving simulator with participants exposed to passive driving.\nExperimental motion sickness results closely matched the sickness model predictions. As predicted by the model, the no motion condition yielded the lowest sickness levels. However, it was rated lowest in terms of fidelity. The compromise solution reduced sickness by over 50% (average MISC level 3 to 1.5) compared to adaptive washout and the algorithm focusing on specific force tracking, without any significant reduction in fidelity rating.\nThe proposed approach for developing MCA that takes into account both the simulator dynamics and time evolution of motion sickness offers a significant advancement in achieving an optimal control of motion sickness and specific force recreation in driving simulators, supporting broader simulator use.",
    "url": "https://arxiv.org/abs/2510.01986",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper proposes a motion cueing algorithm for driving simulators that reduces motion sickness by optimizing fidelity and comfort. Human-in-the-loop experiments showed that the algorithm significantly reduced motion sickness levels without compromising fidelity, compared to other motion settings. This approach represents a significant advancement in achieving optimal control of motion sickness and specific force recreation in driving simulators, supporting broader use of simulators in research and development."
  },
  {
    "title": "Multimodal Foundation Models for Early Disease Detection",
    "abstract": "Healthcare generates diverse streams of data, including electronic health records (EHR), medical imaging, genetics, and ongoing monitoring from wearable devices. Traditional diagnostic models frequently analyze these sources in isolation, which constrains their capacity to identify cross-modal correlations essential for early disease diagnosis. Our research presents a multimodal foundation model that consolidates diverse patient data through an attention-based transformer framework. At first, dedicated encoders put each modality into a shared latent space. Then, they combine them using multi-head attention and residual normalization. The architecture is made for pretraining on many tasks, which makes it easy to adapt to new diseases and datasets with little extra work. We provide an experimental strategy that uses benchmark datasets in oncology, cardiology, and neurology, with the goal of testing early detection tasks. The framework includes data governance and model management tools in addition to technological performance to improve transparency, reliability, and clinical interpretability. The suggested method works toward a single foundation model for precision diagnostics, which could improve the accuracy of predictions and help doctors make decisions.",
    "url": "https://arxiv.org/abs/2510.01899",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a multimodal foundation model that combines various types of patient data, such as electronic health records and medical imaging, to improve early disease detection. The model utilizes an attention-based transformer framework to identify cross-modal correlations and is designed for pretraining on multiple tasks, making it adaptable to new diseases and datasets. The proposed method aims to enhance the accuracy of predictions and assist doctors in making informed decisions by providing a single foundation model for precision diagnostics."
  },
  {
    "title": "Multimodal Feedback for Task Guidance in Augmented Reality",
    "abstract": "Optical see-through augmented reality (OST-AR) overlays digital targets and annotations on the physical world, offering promising guidance for hands-on tasks such as medical needle insertion or assembly. Recent work on OST-AR depth perception shows that target opacity and tool visualization significantly affect accuracy and usability; opaque targets and rendering the real instrument reduce depth errors, whereas transparent targets and absent tools impair performance. However, reliance on visual overlays may overload attention and leaves little room for depth cues when occlusion or lighting hampers perception. To address these limitations, we explore multimodal feedback that combines OST-AR with wrist-based vibrotactile haptics. The past two years have seen rapid advances in haptic technology. Researchers have investigated skin-stretch and vibrotactile cues for conveying spatial information to blind users, wearable ring actuators that support precise pinching in AR, cross-modal audio-haptic cursors that enable eyes-free object selection, and wrist-worn feedback for teleoperated surgery that improves force awareness at the cost of longer task times. Studies comparing pull versus push vibrotactile metaphors found that pull cues yield faster gesture completion and lower cognitive load. These findings motivate revisiting OST-AR guidance with a fresh perspective on wrist-based haptics. We design a custom wristband with six vibromotors delivering directional and state cues, integrate it with a handheld tool and OST-AR, and assess its impact on cue recognition and depth guidance. Through a formative study and two experiments (N=21 and N=27), we show that participants accurately identify haptic patterns under cognitive load and that multimodal feedback improves spatial precision and usability compared with visual-only or haptic-only conditions.",
    "url": "https://arxiv.org/abs/2510.01690",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of multimodal feedback, combining optical see-through augmented reality (OST-AR) with wrist-based vibrotactile haptics, to improve task guidance in hands-on activities like medical procedures or assembly tasks. The study finds that combining visual overlays with haptic feedback enhances spatial precision and usability, compared to relying solely on visual or haptic cues. The findings suggest that integrating wrist-based haptics with OST-AR technology can improve depth perception and task performance in various applications."
  },
  {
    "title": "A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation",
    "abstract": "Patients awaiting invasive procedures often have unanswered pre-procedural questions; however, time-pressured workflows and privacy constraints limit personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave No One Behind Architecture), a safety-first, local-first system that routes inputs with a high-precision sentence-transformer classifier and returns verbatim answers from a clinician-curated FAQ for clinical queries, eliminating free-text generation in the clinical path. We evaluated two domains (tooth extraction and gastroscopy) using expert-reviewed validation sets (n=400/domain) for thresholding and independent test sets (n=200/domain). Among the four encoders, E5-large-instruct (560M) achieved an overall accuracy of 0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were statistically indistinguishable from GPT-4o on this task; Gemini made no errors on this test set. Energy logging shows that the non-generative clinical path consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local 8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single on-prem GPU. These results indicate that near-frontier discrimination and generation-induced errors are structurally avoided in the clinical path by returning vetted FAQ answers verbatim, supporting privacy, sustainability, and equitable deployment in bandwidth-limited environments.",
    "url": "https://arxiv.org/abs/2510.01671",
    "journal": "arXiv cs.HC",
    "ai_summary": "The study introduces LENOHA, a locally executable AI system designed to improve preoperative patient communication by providing verbatim answers from a clinician-curated FAQ for clinical queries. The system achieved high accuracy and low error rates in two evaluated domains, tooth extraction, and gastroscopy, with energy consumption significantly lower than generative models. This research highlights the potential of AI systems like LENOHA to provide personalized patient counseling while addressing privacy, sustainability, and deployment challenges in healthcare settings."
  },
  {
    "title": "Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations",
    "abstract": "Multimodal large language models (MLLMs) have been integrated into visual interpretation applications to support Blind and Low Vision (BLV) users because of their accuracy and ability to provide rich, human-like interpretations. However, these applications often default to comprehensive, lengthy descriptions regardless of context. This leads to inefficient exchanges, as users must go through irrelevant details rather than receiving the specific information they are likely to seek. To deliver more contextually-relevant information, we developed a system that draws on historical BLV users questions. When given an image, our system identifies similar past visual contexts from the VizWiz-LF dataset and uses the associated questions to guide the MLLM generate descriptions more relevant to BLV users. An evaluation with three human labelers who revised 92 context-aware and context-free descriptions showed that context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of comparisons (50 out of 92). Our paper reviews, and data analysis are publicly available in a Github repository at this https URL .",
    "url": "https://arxiv.org/abs/2510.01576",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on improving visual interpretation applications for Blind and Low Vision users by developing a system that uses historical user questions to guide multimodal large language models in generating more contextually-relevant descriptions. The system was found to anticipate and answer users' questions in 76.1% of cases and was preferred in 54.4% of comparisons. This work provides a more efficient and user-friendly approach for BLV individuals interacting with visual interpretation applications."
  },
  {
    "title": "Longitudinal Monitoring of LLM Content Moderation of Social Issues",
    "abstract": "Large language models' (LLMs') outputs are shaped by opaque and frequently-changing company content moderation policies and practices. LLM moderation often takes the form of refusal; models' refusal to produce text about certain topics both reflects company policy and subtly shapes public discourse. We introduce AI Watchman, a longitudinal auditing system to publicly measure and track LLM refusals over time, to provide transparency into an important and black-box aspect of LLMs. Using a dataset of over 400 social issues, we audit Open AI's moderation endpoint, GPT-4.1, and GPT-5, and DeepSeek (both in English and Chinese). We find evidence that changes in company policies, even those not publicly announced, can be detected by AI Watchman, and identify company- and model-specific differences in content moderation. We also qualitatively analyze and categorize different forms of refusal. This work contributes evidence for the value of longitudinal auditing of LLMs, and AI Watchman, one system for doing so.",
    "url": "https://arxiv.org/abs/2510.01255",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces AI Watchman, a system for tracking and measuring the refusal of large language models (LLMs) to produce text on certain topics, reflecting company content moderation policies. The study found that changes in company policies can be detected by AI Watchman and identified differences in content moderation among different LLM models. The findings highlight the importance of longitudinal auditing of LLMs to provide transparency and insight into how these models shape public discourse."
  },
  {
    "title": "JaneEye: A 12-nm 2K-FPS 18.9-$Î¼$J/Frame Event-based Eye Tracking Accelerator",
    "abstract": "Eye tracking has become a key technology for gaze-based interactions in Extended Reality (XR). However, conventional frame-based eye-tracking systems often fall short of XR's stringent requirements for high accuracy, low latency, and energy efficiency. Event cameras present a compelling alternative, offering ultra-high temporal resolution and low power consumption. In this paper, we present JaneEye, an energy-efficient event-based eye-tracking hardware accelerator designed specifically for wearable devices, leveraging sparse, high-temporal-resolution event data. We introduce an ultra-lightweight neural network architecture featuring a novel ConvJANET layer, which simplifies the traditional ConvLSTM by retaining only the forget gate, thereby halving computational complexity without sacrificing temporal modeling capability. Our proposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+ dataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. To further enhance hardware efficiency, we employ custom linear approximations of activation functions (hardsigmoid and hardtanh) and fixed-point quantization. Through software-hardware co-design, our 12-nm ASIC implementation operates at 400 MHz, delivering an end-to-end latency of 0.5 ms (equivalent to 2000 Frames Per Second (FPS)) at an energy efficiency of 18.9 $\\mu$J/frame. JaneEye sets a new benchmark in low-power, high-performance eye-tracking solutions suitable for integration into next-generation XR wearables.",
    "url": "https://arxiv.org/abs/2510.01213",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research presents JaneEye, an energy-efficient event-based eye-tracking hardware accelerator designed for wearable devices, offering high accuracy, low latency, and energy efficiency. The proposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+ dataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. The 12-nm ASIC implementation operates at 400 MHz, delivering an end-to-end latency of 0.5 ms at an energy efficiency of 18.9 $\\mu$J/frame, setting a new benchmark in low-power, high-performance eye-tracking solutions for next-generation XR wearables."
  },
  {
    "title": "Efficient Probabilistic Visualization of Local Divergence of 2D Vector Fields with Independent Gaussian Uncertainty",
    "abstract": "This work focuses on visualizing uncertainty of local divergence of two-dimensional vector fields. Divergence is one of the fundamental attributes of fluid flows, as it can help domain scientists analyze potential positions of sources (positive divergence) and sinks (negative divergence) in the flow. However, uncertainty inherent in vector field data can lead to erroneous divergence computations, adversely impacting downstream analysis. While Monte Carlo (MC) sampling is a classical approach for estimating divergence uncertainty, it suffers from slow convergence and poor scalability with increasing data size and sample counts. Thus, we present a two-fold contribution that tackles the challenges of slow convergence and limited scalability of the MC approach. (1) We derive a closed-form approach for highly efficient and accurate uncertainty visualization of local divergence, assuming independently Gaussian-distributed vector uncertainties. (2) We further integrate our approach into Viskores, a platform-portable parallel library, to accelerate uncertainty visualization. In our results, we demonstrate significantly enhanced efficiency and accuracy of our serial analytical (speed-up up to 1946X) and parallel Viskores (speed-up up to 19698X) algorithms over the classical serial MC approach. We also demonstrate qualitative improvements of our probabilistic divergence visualizations over traditional mean-field visualization, which disregards uncertainty. We validate the accuracy and efficiency of our methods on wind forecast and ocean simulation datasets.",
    "url": "https://arxiv.org/abs/2510.01190",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on visualizing uncertainty in the local divergence of 2D vector fields, which is crucial for analyzing fluid flows. The study presents a closed-form approach for efficient and accurate uncertainty visualization, based on independently Gaussian-distributed vector uncertainties. The results show a significant improvement in efficiency and accuracy compared to traditional Monte Carlo methods, with speed-ups of up to 1946X for serial algorithms and 19698X for parallel algorithms, demonstrating the potential for more reliable analysis of vector field data in various applications."
  },
  {
    "title": "Social Photo-Elicitation: The Use of Communal Production of Meaning to Hear a Vulnerable Population",
    "abstract": "We report on an initial ethnographic exploration of the situation of sex-trafficking survivors in Nepal. In the course of studying trafficking survivors in a protected-living situation created by a non-governmental organization in Nepal, we adapted photo-elicitation to hear the voices of the survivors by making the technique more communal. Bringing sociality to the forefront of the method reduced the pressure on survivors to assert voices as individuals, allowing them to speak. We make three contributions to research. First, we propose a communal form of photo-elicitation as a method to elicit values in sensitive settings. Second, we present the complex circumstances of the survivors as they undergo rehabilitation and move towards life with a ``new normal''. Third, our work adds to HCI and CSCW literature on understanding specific concerns of trafficking survivors and aims to inform designs that can support reintegration of survivors in society. The values that the survivors hold and their notion of future opportunities suggest possession of limited but important social capital in some domains that could be leveraged to aid reintegration.",
    "url": "https://arxiv.org/abs/2510.00964",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of communal photo-elicitation to hear the voices of sex-trafficking survivors in Nepal. By adapting the technique to be more communal, the pressure on survivors to assert their voices as individuals was reduced, allowing them to speak more freely. The study highlights the importance of understanding the values and experiences of vulnerable populations like trafficking survivors in order to inform designs and support their reintegration into society."
  },
  {
    "title": "\"We are not Future-ready\": Understanding AI Privacy Risks and Existing Mitigation Strategies from the Perspective of AI Developers in Europe",
    "abstract": "The proliferation of AI has sparked privacy concerns related to training data, model interfaces, downstream applications, and more. We interviewed 25 AI developers based in Europe to understand which privacy threats they believe pose the greatest risk to users, developers, and businesses and what protective strategies, if any, would help to mitigate them. We find that there is little consensus among AI developers on the relative ranking of privacy risks. These differences stem from salient reasoning patterns that often relate to human rather than purely technical factors. Furthermore, while AI developers are aware of proposed mitigation strategies for addressing these risks, they reported minimal real-world adoption. Our findings highlight both gaps and opportunities for empowering AI developers to better address privacy risks in AI.",
    "url": "https://arxiv.org/abs/2510.00909",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the privacy risks associated with AI development and the lack of consensus among European AI developers on the ranking of these risks. Despite being aware of mitigation strategies, developers reported minimal real-world adoption. The findings emphasize the need to empower AI developers to address privacy risks effectively."
  },
  {
    "title": "Virtual Reality Alters Perceived Functional Body Size",
    "abstract": "Virtual reality (VR) introduces sensory perturbations that may impact perception and action. The current study was designed to investigate how immersive VR presented through a head-mounted display (HMD) affects perceived functional body size using a passable aperture paradigm. Participants (n=60) performed an action task (sidle through apertures) and a perception task (adjust aperture width until passable without contact) in both physical, unmediated reality (UR) and VR. Results revealed significantly higher action and perceptual thresholds in VR compared to UR. Affordance ratios (perceptual threshold over action threshold) were also higher in VR, indicating that the increase in perceptual thresholds in VR was driven partly by sensorimotor uncertainty, as reflected in the increase in the action thresholds, and partly by perceptual distortions imposed by VR. This perceptual overestimation in VR also persisted as an aftereffect in UR following VR exposure. Geometrical modelling attributed the disproportionate increase in the perceptual threshold in VR primarily to depth compression. This compression, stemming from the vergence-accommodation conflict (VAC), caused the virtual aperture to be perceived as narrower than depicted, thus requiring a wider adjusted aperture. Critically, after mathematically correcting for the VAC's impact on perceived aperture width, the affordance ratios in VR became equivalent to those in UR. These outcomes demonstrate a recovered invariant geometrical scaling, suggesting that perception remained functionally attuned to action capabilities once VAC-induced distortions were accounted for. These findings highlight that VR-induced depth compression systematically alters perceived body-environment relationships, leading to an altered sense of one's functional body size.",
    "url": "https://arxiv.org/abs/2510.00824",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study investigated how immersive virtual reality (VR) affects perceived functional body size using a passable aperture paradigm. The results showed that participants had higher action and perceptual thresholds in VR compared to physical reality, with the increase in perceptual thresholds driven by sensorimotor uncertainty and perceptual distortions imposed by VR. The study also found that the increase in perceptual threshold in VR was primarily due to depth compression caused by the vergence-accommodation conflict, but after correcting for this, perception in VR became equivalent to physical reality, highlighting the impact of VR on perceived body-environment relationships."
  },
  {
    "title": "Datasets for Valence and Arousal Inference: A Survey",
    "abstract": "Understanding human affect can be used in robotics, marketing, education, human-computer interaction, healthcare, entertainment, autonomous driving, and psychology to enhance decision-making, personalize experiences, and improve emotional well-being. This work presents a comprehensive overview of affect inference datasets that utilize continuous valence and arousal labels. We reviewed 25 datasets published between 2008 and 2024, examining key factors such as dataset size, subject distribution, sensor configurations, annotation scales, and data formats for valence and arousal values. While camera-based datasets dominate the field, we also identified several widely used multimodal combinations. Additionally, we explored the most common approaches to affect detection applied to these datasets, providing insights into the prevailing methodologies in the field. Our overview of sensor fusion approaches shows promising advancements in model improvement for valence and arousal inference.",
    "url": "https://arxiv.org/abs/2510.00738",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research provides a survey of affect inference datasets focusing on valence and arousal labels, which are crucial for various applications such as robotics, marketing, and healthcare. The study reviewed 25 datasets from 2008 to 2024, highlighting key factors like dataset size, sensor configurations, and annotation scales. The findings suggest that camera-based datasets are prevalent, but there are also promising advancements in sensor fusion approaches for improving models in valence and arousal inference."
  },
  {
    "title": "Designing Wine Tasting Experiences for All: The role of Human Diversity and Personal food memory",
    "abstract": "This study investigates the design of inclusive wine-tasting experiences by examining the roles of human diversity and personal food memory. Through field studies conducted in various wine regions, we explored how Chinese visitors engage with wine-tasting activities during winery tours, highlighting the cross-cultural challenges they face. Our findings underscore the importance of experiencers' abilities, necessities, and aspirations (ANAs), the authenticity of wine tasting within the context of winery tours, and the use of personal food memories as a wine-tasting tool accessible to all. These insights lay the groundwork for developing more inclusive and engaging wine-tasting services, offering new perspectives for cultural exchange and sustainable wine business practices in China.",
    "url": "https://arxiv.org/abs/2510.00607",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study examines the design of inclusive wine-tasting experiences, focusing on the roles of human diversity and personal food memory. The research highlights the challenges faced by Chinese visitors during winery tours and emphasizes the importance of experiencers' abilities, necessities, and aspirations (ANAs) in creating authentic and engaging wine-tasting activities. The findings suggest that incorporating personal food memories as a wine-tasting tool can make the experience accessible to all, providing insights for developing more inclusive and culturally enriching wine-tasting services in China."
  },
  {
    "title": "Rethinking Wine Tasting for Chinese Consumers: A Service Design Approach Enhanced by Multimodal Personalization",
    "abstract": "Wine tasting is a multimodal and culturally embedded activity that presents unique challenges when adapted to non-Western contexts. This paper proposes a service design approach rooted in contextual co-creation to reimagine wine tasting experiences for Chinese consumers. Drawing on 26 in-situ interviews and follow-up validation sessions, we identify three distinct user archetypes: Curious Tasters, Experience Seekers, and Knowledge Builders, each exhibiting different needs in vocabulary, interaction, and emotional pacing. Our findings reveal that traditional wine descriptors lack cultural resonance and that cross-modal metaphors grounded in local gastronomy (e.g., green mango for acidity) significantly improve cognitive and emotional engagement. These insights informed a partially implemented prototype, featuring AI-driven metaphor-to-flavour mappings and real-time affective feedback visualisation. A small-scale usability evaluation confirmed improvements in engagement and comprehension. Our comparative analysis shows alignment with and differentiation from prior multimodal and affect-aware tasting systems. This research contributes to CBMI by demonstrating how culturally adaptive interaction systems can enhance embodied consumption experiences in physical tourism and beyond.",
    "url": "https://arxiv.org/abs/2510.00583",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the challenges of adapting wine tasting experiences for Chinese consumers and proposes a service design approach rooted in contextual co-creation. Through interviews and validation sessions, the study identifies three user archetypes with different needs and preferences. The findings suggest that using local gastronomy metaphors improves engagement and comprehension, leading to the development of a prototype with AI-driven features that enhance the tasting experience. This research contributes to the field by demonstrating how culturally adaptive interaction systems can enhance consumption experiences in physical tourism and beyond."
  },
  {
    "title": "PromptPilot: Improving Human-AI Collaboration Through LLM-Enhanced Prompt Engineering",
    "abstract": "Effective prompt engineering is critical to realizing the promised productivity gains of large language models (LLMs) in knowledge-intensive tasks. Yet, many users struggle to craft prompts that yield high-quality outputs, limiting the practical benefits of LLMs. Existing approaches, such as prompt handbooks or automated optimization pipelines, either require substantial effort, expert knowledge, or lack interactive guidance. To address this gap, we design and evaluate PromptPilot, an interactive prompting assistant grounded in four empirically derived design objectives for LLM-enhanced prompt engineering. We conducted a randomized controlled experiment with 80 participants completing three realistic, work-related writing tasks. Participants supported by PromptPilot achieved significantly higher performance (median: 78.3 vs. 61.7; p = .045, d = 0.56), and reported enhanced efficiency, ease-of-use, and autonomy during interaction. These findings empirically validate the effectiveness of our proposed design objectives, establishing LLM-enhanced prompt engineering as a viable technique for improving human-AI collaboration.",
    "url": "https://arxiv.org/abs/2510.00555",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces PromptPilot, an interactive prompting assistant aimed at improving human-AI collaboration by enhancing prompt engineering for large language models (LLMs). The study found that participants supported by PromptPilot achieved significantly higher performance in work-related writing tasks compared to those without assistance. This suggests that LLM-enhanced prompt engineering can enhance efficiency, ease-of-use, and autonomy during interaction, validating the effectiveness of the proposed design objectives."
  },
  {
    "title": "Face2Feel: Emotion-Aware Adaptive User Interface",
    "abstract": "This paper presents Face2Feel, a novel user interface (UI) model that dynamically adapts to user emotions and preferences captured through computer vision. This adaptive UI framework addresses the limitations of traditional static interfaces by integrating digital image processing, face recognition, and emotion detection techniques. Face2Feel analyzes user expressions utilizing a webcam or pre-installed camera as the primary data source to personalize the UI in real-time. Although dynamically changing user interfaces based on emotional states are not yet widely implemented, their advantages and the demand for such systems are evident. This research contributes to the development of emotion-aware applications, particularly in recommendation systems and feedback mechanisms. A case study, \"Shresta: Emotion-Based Book Recommendation System,\" demonstrates the practical implementation of this framework, the technologies employed, and the system's usefulness. Furthermore, a user survey conducted after presenting the working model reveals a strong demand for such adaptive interfaces, emphasizing the importance of user satisfaction and comfort in human-computer interaction. The results showed that nearly 85.7\\% of the users found these systems to be very engaging and user-friendly. This study underscores the potential for emotion-driven UI adaptation to improve user experiences across various applications.",
    "url": "https://arxiv.org/abs/2510.00489",
    "journal": "arXiv cs.HC",
    "ai_summary": "The Face2Feel user interface model dynamically adapts to user emotions and preferences through computer vision, addressing the limitations of static interfaces. By analyzing user expressions in real-time, the system personalizes the UI to enhance user experience, as demonstrated in the \"Shresta\" book recommendation system case study. The research highlights the demand for emotion-aware applications and the potential of emotion-driven UI adaptation to improve user satisfaction in human-computer interaction."
  },
  {
    "title": "RELATE-Sim: Leveraging Turning Point Theory and LLM Agents to Predict and Understand Long-Term Relationship Dynamics through Interactive Narrative Simulations",
    "abstract": "Most dating technologies optimize for getting together, not staying together. We present RELATE-Sim, a theory-grounded simulator that models how couples behave at consequential turning points-exclusivity talks, conflict-and-repair episodes, relocations-rather than static traits. Two persona-aligned LLM agents (one per partner) interact under a centralized Scene Master that frames each turning point as a compact set of realistic options, advances the narrative, and infers interpretable state changes and an auditable commitment estimate after each scene. On a longitudinal dataset of 71 couples with two-year follow-ups, simulation-aware predictions outperform a personas-only baseline while surfacing actionable markers (e.g., repair attempts acknowledged, clarity shifts) that explain why trajectories diverge. RELATE-Sim pushes the relationship research's focus from matchmaking to maintenance, providing a transparent, extensible platform for understanding and forecasting long-term relationship dynamics.",
    "url": "https://arxiv.org/abs/2510.00414",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces RELATE-Sim, a simulator that models how couples behave at important relationship turning points rather than static traits. By using LLM agents and a centralized Scene Master, the simulator can predict and understand long-term relationship dynamics more accurately than traditional methods. This shift from matchmaking to maintenance in relationship research provides actionable insights for improving relationship outcomes."
  }
]