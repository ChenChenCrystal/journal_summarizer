[
  {
    "title": "XAgen: An Explainability Tool for Identifying and Correcting Failures in Multi-Agent Workflows",
    "abstract": "As multi-agent systems powered by Large Language Models (LLMs) are increasingly adopted in real-world workflows, users with diverse technical backgrounds are now building and refining their own agentic processes. However, these systems can fail in opaque ways, making it difficult for users to observe, understand, and correct errors. We conducted formative interviews with 12 practitioners to identify mismatches between existing observability tools and users' needs. Based on these insights, we designed XAgen, an explainability tool that supports users with varying AI expertise through three core capabilities: log visualization for glanceable workflow understanding, human-in-the-loop feedback to capture expert judgment, and automatic error detection via an LLM-as-a-judge. In a user study with 8 participants, XAgen helped users more easily locate failures, attribute to specific agents or steps, and iteratively improve configurations. Our findings surface human-centered design guidelines for explainable agentic AI development and highlights opportunities for more context-aware interactive debugging.",
    "url": "https://arxiv.org/abs/2512.17896",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces XAgen, an explainability tool designed to help users identify and correct failures in multi-agent workflows powered by Large Language Models (LLMs). Through formative interviews with practitioners, the researchers identified the need for a tool that provides log visualization, human-in-the-loop feedback, and automatic error detection. In a user study, XAgen was found to help users locate failures, attribute them to specific agents or steps, and improve configurations, highlighting the importance of human-centered design in explainable agentic AI development."
  },
  {
    "title": "Map2Video: Street View Imagery Driven AI Video Generation",
    "abstract": "AI video generation has lowered barriers to video creation, but current tools still struggle with inconsistency. Filmmakers often find that clips fail to match characters and backgrounds, making it difficult to build coherent sequences. A formative study with filmmakers highlighted challenges in shot composition, character motion, and camera control. We present Map2Video, a street view imagery-driven AI video generation tool grounded in real-world geographies. The system integrates Unity and ComfyUI with the VACE video generation model, as well as OpenStreetMap and Mapillary for street view imagery. Drawing on familiar filmmaking practices such as location scouting and rehearsal, Map2Video enables users to choose map locations, position actors and cameras in street view imagery, sketch movement paths, refine camera motion, and generate spatially consistent videos. We evaluated Map2Video with 12 filmmakers. Compared to an image-to-video baseline, it achieved higher spatial accuracy, required less cognitive effort, and offered stronger controllability for both scene replication and open-ended creative exploration.",
    "url": "https://arxiv.org/abs/2512.17883",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Map2Video, a street view imagery-driven AI video generation tool that addresses the inconsistency issues faced by filmmakers in current video creation tools. By integrating real-world geographies and familiar filmmaking practices, Map2Video allows users to position actors and cameras in street view imagery, sketch movement paths, refine camera motion, and generate spatially consistent videos. Evaluation with filmmakers showed that Map2Video achieved higher spatial accuracy, required less cognitive effort, and offered stronger controllability compared to an image-to-video baseline, making it a valuable tool for both scene replication and creative exploration."
  },
  {
    "title": "Your Eyes Controlled the Game: Real-Time Cognitive Training Adaptation based on Eye-Tracking and Physiological Data in Virtual Reality",
    "abstract": "Cognitive training for sustained attention and working memory is vital across domains relying on robust mental capacity such as education or rehabilitation. Adaptive systems are essential, dynamically matching difficulty to user ability to maintain engagement and accelerate learning. Current adaptive systems often rely on simple performance heuristics or predict visual complexity and affect instead of cognitive load. This study presents the first implementation of real-time adaptive cognitive load control in Virtual Reality cognitive training based on eye-tracking and physiological data. We developed a bidirectional LSTM model with a self-attention mechanism, trained on eye-tracking and physiological (PPG, GSR) data from 74 participants. We deployed it in real-time with 54 participants across single-task (sustained attention) and dual-task (sustained attention + mental arithmetic) paradigms. Difficulty was adjusted dynamically based on participant self-assessment or model's real-time cognitive load predictions. Participants showed a tendency to estimate the task as too difficult, even though they were objectively performing at their best. Over the course of a 10-minute session, both adaptation methods converged at equivalent difficulty in single-task scenarios, with no significant differences in subjective workload or game performance. However, in the dual-task conditions, the model successfully pushed users to higher difficulty levels without performance penalties or increased frustration, highlighting a user tendency to underestimate capacity under high cognitive load. Findings indicate that machine learning models may provide more objective cognitive capacity assessments than self-directed approaches, mitigating subjective performance biases and enabling more effective training by pushing users beyond subjective comfort zones toward physiologically-determined optimal challenge levels.",
    "url": "https://arxiv.org/abs/2512.17882",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores the use of real-time adaptive cognitive load control in Virtual Reality cognitive training using eye-tracking and physiological data. The findings suggest that machine learning models can provide more objective assessments of cognitive capacity compared to self-directed approaches, leading to more effective training by pushing users beyond their subjective comfort zones towards optimal challenge levels. The research highlights the importance of dynamic difficulty adjustment in maintaining engagement and accelerating learning in cognitive training tasks."
  },
  {
    "title": "Playful but Persuasive: Deceptive Designs and Advertising Strategies in Popular Mobile Apps for Children",
    "abstract": "Mobile gaming apps are woven into children's daily lives. Given their ongoing cognitive and emotional development, children are especially vulnerable and depend on designs that safeguard their well-being. When apps feature manipulative interfaces or heavy advertising, they may exert undue influence on young users, contributing to prolonged screen time, disrupted self-regulation, and accidental in-app purchases. In this study, we examined 20 popular, free-to-download children's apps in German-speaking regions to assess the prevalence of deceptive design patterns and advertising. Despite platform policies and EU frameworks like the General Data Protection Regulation and the Digital Services Act, every app contained interface manipulations intended to nudge, confuse, or pressure young users, averaging nearly six distinct deceptive patterns per app. Most also displayed high volumes of non-skippable ads, frequently embedded within core gameplay. These findings indicate a systemic failure of existing safeguards and call for stronger regulation, greater platform accountability, and child-centered design standards.",
    "url": "https://arxiv.org/abs/2512.17819",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research study analyzed 20 popular children's mobile gaming apps in German-speaking regions and found that they frequently contained deceptive design patterns and heavy advertising, which could negatively impact young users' well-being by leading to prolonged screen time, disrupted self-regulation, and accidental in-app purchases. Despite existing regulations like the General Data Protection Regulation and the Digital Services Act, the study revealed a systemic failure in safeguarding children from manipulative interfaces and advertising, highlighting the need for stronger regulation, platform accountability, and child-centered design standards."
  },
  {
    "title": "Digital Bricolage: Design Speculations for Embodied Approaches to Digitized Print-based Cultural Collections",
    "abstract": "COVID-related closures of public and academic libraries have underlined the importance of online platforms that provide access to digitized print-based collections. However, they also have highlighted the value of in-person handling of print artefacts for sensing and making sense of them. How do existing dominant digital platforms invite and/or discourage embodied forms of exploration and sense-making? What opportunities for embodied experience might we discover if we embrace the material qualities of print-based collections when designing interfaces for digital access? In this paper, we present findings from a speculative exercise where we invited creative professionals and experts in curating and handling access to collections to reflect on existing approaches to digitized print-based collections and to speculate about alternative design opportunities and modes of engagement. We argue for digital bricolage-a design approach that values working with materials that are \"on hand\" and embracing our ability to \"handle\" them in ways that foster both casual and curious exploration.",
    "url": "https://arxiv.org/abs/2512.17590",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the importance of embodied interactions with print-based cultural collections in digital platforms, especially in light of COVID-related library closures. The study highlights the need for digital platforms to encourage physical exploration and sense-making of print artefacts. The concept of digital bricolage is proposed as a design approach that values working with materials in ways that promote both casual and curious exploration, offering new opportunities for engaging with digitized print-based collections."
  },
  {
    "title": "VAIR: Visual Analytics for Injury Risk Exploration in Sports",
    "abstract": "Injury prevention in sports requires understanding how bio-mechanical risks emerge from movement patterns captured in real-world scenarios. However, identifying and interpreting injury prone events from raw video remains difficult and time-consuming. We present VAIR, a visual analytics system that supports injury risk analysis using 3D human motion reconstructed from sports video. VAIR combines pose estimation, bio-mechanical simulation, and synchronized visualizations to help users explore how joint-level risk indicators evolve over time. Domain experts can inspect movement segments through temporally aligned joint angles, angular velocity, and internal forces to detect patterns associated with known injury mechanisms. Through case studies involving Achilles tendon and Anterior cruciate ligament (ACL) injuries in basketball, we show that VAIR enables more efficient identification and interpretation of risky movements. Expert feedback confirms that VAIR improves diagnostic reasoning and supports both retrospective analysis and proactive intervention planning.",
    "url": "https://arxiv.org/abs/2512.17446",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research presents VAIR, a visual analytics system that helps analyze injury risk in sports by reconstructing 3D human motion from sports video. The system combines pose estimation, bio-mechanical simulation, and synchronized visualizations to allow experts to explore joint-level risk indicators and detect patterns associated with known injury mechanisms. Case studies involving Achilles tendon and ACL injuries in basketball demonstrate that VAIR enables more efficient identification and interpretation of risky movements, improving diagnostic reasoning and supporting proactive intervention planning in sports injury prevention."
  },
  {
    "title": "Psychological Factors Influencing University Students Trust in AI-Based Learning Assistants",
    "abstract": "Artificial intelligence (AI) based learning assistants and chatbots are increasingly integrated into higher education. While these tools are often evaluated in terms of technical performance, their successful and ethical use also depends on psychological factors such as trust, perceived risk, technology anxiety, and students general attitudes toward AI. This paper adopts a psychology oriented perspective to examine how university students form trust in AI based learning assistants. Drawing on recent literature in mental health, human AI interaction, and trust in automation, we propose a conceptual framework that organizes psychological predictors of trust into four groups: cognitive appraisals, affective reactions, social relational factors, and contextual moderators. A narrative review approach synthesizes empirical findings and derives research questions and hypotheses for future studies. The paper highlights that trust in AI is a psychological process shaped by individual differences and learning environments, with practical implications for instructors, administrators, and designers of educational AI systems.",
    "url": "https://arxiv.org/abs/2512.17390",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores the psychological factors influencing university students' trust in AI-based learning assistants, highlighting the importance of factors such as trust, perceived risk, technology anxiety, and general attitudes toward AI. The study proposes a conceptual framework categorizing predictors of trust into cognitive appraisals, affective reactions, social relational factors, and contextual moderators. The findings emphasize that trust in AI is a complex psychological process influenced by individual differences and learning environments, with implications for educators, administrators, and designers of AI systems in education."
  },
  {
    "title": "Implementation of Augmented Reality as an Educational Tool for Practice in Early Childhood",
    "abstract": "Learning Wudhu for young children requires engaging and interactive media to foster a deep understanding of the worship procedures. This study aims to develop a Wudhu learning application based on Augmented Reality (AR) as an interactive and fun educational medium. The development method used includes the stages of needs analysis, system design, implementation, and testing using Black Box Testing. The system utilizes marker-based tracking to display 3D animations of Wudhu movements in real-time when the camera detects a marker on the printed media. The test results indicate that all main functions run well, and a limited trial on children aged 5-7 years showed an increase in learning interest and a better understanding of the Wudhu sequence. Thus, the application of AR technology is proven effective in improving the quality of basic worship instruction for young children.",
    "url": "https://arxiv.org/abs/2512.17354",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study developed an Augmented Reality (AR) application for teaching young children the Wudhu process, a fundamental Islamic practice. The application successfully engaged children aged 5-7 years, increasing their interest in learning and improving their understanding of the Wudhu sequence. The implementation of AR technology as an educational tool was found to be effective in enhancing the quality of basic worship instruction for early childhood education."
  },
  {
    "title": "LUMIA: A Handheld Vision-to-Music System for Real-Time, Embodied Composition",
    "abstract": "Most digital music tools emphasize precision and control, but often lack support for tactile, improvisational workflows grounded in environmental interaction. Lumia addresses this by enabling users to \"compose through looking\"--transforming visual scenes into musical phrases using a handheld, camera-based interface and large multimodal models. A vision-language model (GPT-4V) analyzes captured imagery to generate structured prompts, which, combined with user-selected instrumentation, guide a text-to-music pipeline (Stable Audio). This real-time process allows users to frame, capture, and layer audio interactively, producing loopable musical segments through embodied interaction. The system supports a co-creative workflow where human intent and model inference shape the musical outcome. By embedding generative AI within a physical device, Lumia bridges perception and composition, introducing a new modality for creative exploration that merges vision, language, and sound. It repositions generative music not as a task of parameter tuning, but as an improvisational practice driven by contextual, sensory engagement.",
    "url": "https://arxiv.org/abs/2512.17228",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Lumia, a handheld vision-to-music system that allows users to compose music in real-time through visual scenes using a camera-based interface and multimodal models. The system utilizes a vision-language model to generate structured prompts based on captured imagery, combined with user-selected instrumentation, to guide a text-to-music pipeline. This approach supports a co-creative workflow where human intent and model inference shape the musical outcome, bridging perception and composition to introduce a new modality for creative exploration."
  },
  {
    "title": "PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases",
    "abstract": "Artificial intelligence (AI)-driven augmented reality (AR) systems are becoming increasingly integrated into daily life, and with this growth comes a greater need for explainability in real-time user interactions. Traditional explainable AI (XAI) methods, which often rely on feature-based or example-based explanations, struggle to deliver dynamic, context-specific, personalized, and human-centric insights for everyday AR users. These methods typically address separate explainability dimensions (e.g., when, what, how) with different explanation techniques, resulting in unrealistic and fragmented experiences for seamless AR interactions. To address this challenge, we propose PILAR, a novel framework that leverages a pre-trained large language model (LLM) to generate context-aware, personalized explanations, offering a more intuitive and trustworthy experience in real-time AI-powered AR systems. Unlike traditional methods, which rely on multiple techniques for different aspects of explanation, PILAR employs a unified LLM-based approach that dynamically adapts explanations to the user's needs, fostering greater trust and engagement. We implement the PILAR concept in a real-world AR application (e.g., personalized recipe recommendations), an open-source prototype that integrates real-time object detection, recipe recommendation, and LLM-based personalized explanations of the recommended recipes based on users' dietary preferences. We evaluate the effectiveness of PILAR through a user study with 16 participants performing AR-based recipe recommendation tasks, comparing an LLM-based explanation interface to a traditional template-based one. Results show that the LLM-based interface significantly enhances user performance and experience, with participants completing tasks 40% faster and reporting greater satisfaction, ease of use, and perceived transparency.",
    "url": "https://arxiv.org/abs/2512.17172",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces PILAR, a novel framework that uses a pre-trained large language model (LLM) to generate personalized explanations for AI-driven augmented reality (AR) systems. Unlike traditional methods, PILAR offers context-aware, human-centric explanations that dynamically adapt to user needs, leading to increased trust and engagement. A user study evaluating PILAR in a real-world AR application showed that participants performed tasks 40% faster and reported higher satisfaction and perceived transparency compared to a traditional template-based explanation interface."
  },
  {
    "title": "Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces",
    "abstract": "This study investigates the task of dwell time prediction and proposes a Transformer framework based on interaction behavior modeling. The method first represents user interaction sequences on the interface by integrating dwell duration, click frequency, scrolling behavior, and contextual features, which are mapped into a unified latent space through embedding and positional encoding. On this basis, a multi-head self-attention mechanism is employed to capture long-range dependencies, while a feed-forward network performs deep nonlinear transformations to model the dynamic patterns of dwell time. Multiple comparative experiments are conducted with BILSTM, DRFormer, FedFormer, and iTransformer as baselines under the same conditions. The results show that the proposed method achieves the best performance in terms of MSE, RMSE, MAPE, and RMAE, and more accurately captures the complex patterns in interaction behavior. In addition, sensitivity experiments are carried out on hyperparameters and environments to examine the impact of the number of attention heads, sequence window length, and device environment on prediction performance, which further demonstrates the robustness and adaptability of the method. Overall, this study provides a new solution for dwell time prediction from both theoretical and methodological perspectives and verifies its effectiveness in multiple aspects.",
    "url": "https://arxiv.org/abs/2512.17149",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores dwell time prediction in human-computer interfaces using a Transformer framework that integrates various interaction behaviors. The proposed method outperforms existing baselines in terms of prediction accuracy and effectively captures complex patterns in user interaction sequences. Sensitivity experiments also demonstrate the robustness and adaptability of the method, highlighting its potential for practical applications in interface design and user experience optimization."
  },
  {
    "title": "Bridging Psychometric and Content Development Practices with AI: A Community-Based Workflow for Augmenting Hawaiian Language Assessments",
    "abstract": "This paper presents the design and evaluation of a community-based artificial intelligence (AI) workflow developed for the Kaiapuni Assessment of Educational Outcomes (KĀ'EO) program, the only native language assessment used for federal accountability in the United States. The project explored whether document-grounded language models could ethically and effectively augment human analysis of item performance while preserving the cultural and linguistic integrity of the Hawaiian language. Operating under the KĀ'EO AI Policy Framework, the workflow used NotebookLM for cross-document synthesis of psychometric data and Claude 3.5 Sonnet for developer-facing interpretation, with human oversight at every stage. Fifty-eight flagged items across Hawaiian Language Arts, Mathematics, and Science were reviewed during Round 2 of the AI Lab, producing six interpretive briefs that identified systemic design issues such as linguistic ambiguity, Depth-of-Knowledge (DOK) misalignment, and structural overload. The findings demonstrate that AI can serve as an ethically bounded amplifier of human expertise, accelerating analysis while simultaneously prioritizing fairness, human expertise, and cultural authority. This work offers a replicable model for responsible AI integration in Indigenous-language educational measurement.",
    "url": "https://arxiv.org/abs/2512.17140",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper presents a community-based AI workflow developed for the Kaiapuni Assessment of Educational Outcomes program, aimed at preserving the cultural and linguistic integrity of the Hawaiian language. The study found that AI can effectively augment human analysis of item performance in language assessments, identifying systemic design issues and accelerating analysis while prioritizing fairness and cultural authority. The findings suggest a replicable model for responsible AI integration in Indigenous-language educational measurement."
  },
  {
    "title": "Alignment, Exploration, and Novelty in Human-AI Interaction",
    "abstract": "Human-AI interactions are increasingly part of everyday life, yet the interpersonal dynamics that unfold during such exchanges remain underexplored. This study investigates how emotional alignment, semantic exploration, and linguistic innovation emerge within a collaborative storytelling paradigm that paired human participants with a large language model (LLM) in a turn-taking setup. Over nine days, more than 3,000 museum visitors contributed to 27 evolving narratives, co-authored with an LLM in a naturalistic, public installation. To isolate the dynamics specific to human involvement, we compared the resulting dataset with a simulated baseline where two LLMs completed the same task. Using sentiment analysis, semantic embeddings, and information-theoretic measures of novelty and resonance, we trace how humans and models co-construct stories over time. Our results reveal that affective alignment is primarily driven by the model, with limited mutual convergence in human-AI interaction. At the same time, human participants explored a broader semantic space and introduced more novel, narratively influential contributions. These patterns were significantly reduced in the simulated AI-AI condition. Together, these findings highlight the unique role of human input in shaping narrative direction and creative divergence in co-authored texts. The methods developed here provide a scalable framework for analysing dyadic interaction and offer a new lens on creativity, emotional dynamics, and semantic coordination in human-AI collaboration.",
    "url": "https://arxiv.org/abs/2512.17117",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores the dynamics of human-AI interaction in collaborative storytelling, comparing interactions between humans and a large language model (LLM) with interactions between two LLMs. The results show that emotional alignment is primarily driven by the model, but human participants contribute more novel and narratively influential ideas, leading to creative divergence in the co-authored texts. This research highlights the unique role of human input in shaping narrative direction and offers a framework for analyzing dyadic interaction in human-AI collaboration."
  },
  {
    "title": "Virtual Reality in Service Design for Plastics Recycling: Two Application Cases",
    "abstract": "Plastics recycling depends on everyday sorting practices and on how recycling services are communicated and experienced. Virtual reality (VR) can present these practices and services in situated, interactive form, yet its role in service design for plastics recycling is still emerging. This paper examines how VR tools can contribute to designing plastics recycling services through two application cases that address different stages of the recycling journey. The first case, Clean Cabin Escape, is a household scale VR escape room where players collect and sort waste items into locally relevant categories, with immediate feedback that supports practice with plastics recycling decisions. The second case is a VR simulation of a plastics recycling center that represents a real planned site and is used in service design workshops where stakeholders explore layout, signage and customer paths for plastics fractions. Across the cases, we analyse how VR supported learning, engagement and shared sensemaking, and how it interacted with other service design methods such as workshops, customer path mapping and physical artefacts. The findings show that VR can make domestic sorting tasks and complex recycling centers more concrete for both citizens and professionals, but also highlight trade offs related to hardware access, onboarding effort, visual fidelity and localisation of recycling rules. The paper concludes by outlining opportunities for integrating VR into broader service design toolsets for plastics recycling and circular economy services, and by pointing to directions for future research on long term impact and inclusive design.",
    "url": "https://arxiv.org/abs/2512.17081",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores the potential of virtual reality (VR) in designing plastics recycling services through two application cases. The first case involves a household VR escape room for sorting waste items, while the second case is a VR simulation of a plastics recycling center for service design workshops. The findings suggest that VR can enhance learning, engagement, and shared understanding in the recycling process, but also highlight challenges such as hardware access and visual fidelity. The study concludes by proposing opportunities for integrating VR into broader service design toolsets for plastics recycling and circular economy services, emphasizing the need for future research on long-term impact and inclusive design."
  },
  {
    "title": "Bots Don't Sit Still: A Longitudinal Study of Bot Behaviour Change, Temporal Drift, and Feature-Structure Evolution",
    "abstract": "Social bots are now deeply embedded in online platforms for promotion, persuasion, and manipulation. Most bot-detection systems still treat behavioural features as static, implicitly assuming bots behave stationarily over time. We test that assumption for promotional Twitter bots, analysing change in both individual behavioural signals and the relationships between them. Using 2,615 promotional bot accounts and 2.8M tweets, we build yearly time series for ten content-based meta-features. Augmented Dickey-Fuller and KPSS tests plus linear trends show all ten are non-stationary: nine increase over time, while language diversity declines slightly.\nStratifying by activation generation and account age reveals systematic differences: second-generation bots are most active and link-heavy; short-lived bots show intense, repetitive activity with heavy hashtag/URL use; long-lived bots are less active but more linguistically diverse and use emojis more variably. We then analyse co-occurrence across generations using 18 interpretable binary features spanning actions, topic similarity, URLs, hashtags, sentiment, emojis, and media (153 pairs). Chi-square tests indicate almost all pairs are dependent. Spearman correlations shift in strength and sometimes polarity: many links (e.g. multiple hashtags with media; sentiment with URLs) strengthen, while others flip from weakly positive to weakly or moderately negative. Later generations show more structured combinations of cues.\nTaken together, these studies provide evidence that promotional social bots adapt over time at both the level of individual meta-features and the level of feature interdependencies, with direct implications for the design and evaluation of bot-detection systems trained on historical behavioural features.",
    "url": "https://arxiv.org/abs/2512.17067",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research study examines the behavior of promotional Twitter bots over time, finding that their behavior changes and evolves rather than remaining static. The study identifies differences in behavior based on bot generation and account age, with later generations showing more structured combinations of cues. The findings have implications for the design and evaluation of bot-detection systems, highlighting the need to consider the evolving nature of bot behavior."
  },
  {
    "title": "Designing Virtual Reality Games for Grief: A Workshop Approach with Mental Health Professionals",
    "abstract": "Although serious games have been increasingly used for mental health applications, few explicitly address coping with grief as a core mechanic and narrative experience for patients. Existing grief-related digital games often focus on clinical training for medical professionals rather than immersive storytelling and agency in emotional processing for the patient. In response, we designed Road to Acceptance, a VR game that presents grief through first-person narrative and gameplay. As the next phase of evaluation, we propose a workshop-based study with 12 licensed mental health professionals to assess the therapeutic impacts of the game and the alignment with best practices in grief education and interventions. This will inform iterative game design and patient evaluation methods, ensuring that the experience is clinically appropriate. Potential findings can contribute to the design principles of grief-related virtual reality experiences, bridging the gap between interactive media, mental health interventions, and immersive storytelling.",
    "url": "https://arxiv.org/abs/2512.17025",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on the design of a virtual reality game, Road to Acceptance, which aims to help patients cope with grief through immersive storytelling and gameplay. The study involves a workshop-based evaluation with mental health professionals to assess the therapeutic impacts of the game and its alignment with best practices in grief education and interventions. The findings from this study have the potential to inform the design of future grief-related virtual reality experiences, bridging the gap between interactive media, mental health interventions, and immersive storytelling."
  },
  {
    "title": "Explorable Ideas: Externalizing Ideas as Explorable Environments",
    "abstract": "Working with abstract information often relies on static, symbolic representations that constrain exploration. We introduce Explorable Ideas, a framework that externalizes abstract concepts into explorable environments where physical navigation coordinates conceptual exploration. To investigate its practical value, we designed Idea Islands, a VR probe for ideation tasks, and conducted two controlled studies with 19 participants. Results show that overview perspectives foster strategic breadth while immersion sustains engagement through embodied presence, and that seamless transitions enable flexible workflows combining both modes. These findings validate the framework's design considerations and yield design implications for building future systems that treat information as explorable territory across creative, educational, and knowledge-intensive domains.",
    "url": "https://arxiv.org/abs/2512.17017",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces the concept of Explorable Ideas, a framework that transforms abstract concepts into interactive environments for exploration. Through the development of Idea Islands, a VR tool for ideation tasks, the study found that overview perspectives encourage strategic thinking, while immersion enhances engagement. The results validate the framework's design principles and suggest potential applications in creative, educational, and knowledge-intensive fields."
  },
  {
    "title": "Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life",
    "abstract": "This chapter demonstrates how computational social science (CSS) tools are extending and expanding research on aging. The depth and context from traditionally qualitative methods such as participant observation, in-depth interviews, and historical documents are increasingly employed alongside scalable data management, computational text analysis, and open-science practices. Machine learning (ML) and natural language processing (NLP), provide resources to aggregate and systematically index large volumes of qualitative data, identify patterns, and maintain clear links to in-depth accounts. Drawing on case studies of projects that examine later life--including examples with original data from the DISCERN study (a team-based ethnography of life with dementia) and secondary analyses of the American Voices Project (nationally representative interview)--the chapter highlights both uses and challenges of bringing CSS tools into more meaningful dialogue with qualitative aging research. The chapter argues such work has potential for (1) streamlining and augmenting existing workflows, (2) scaling up samples and projects, and (3) generating multi-method approaches to address important questions in new ways, before turning to practices useful for individuals and teams seeking to understand current possibilities or refine their workflow processes. The chapter concludes that current developments are not without peril, but offer potential for new insights into aging and the life course by broadening--rather than replacing--the methodological foundations of qualitative research.",
    "url": "https://arxiv.org/abs/2512.17850",
    "journal": "arXiv cs.HC",
    "ai_summary": "This chapter explores how computational social science tools, such as machine learning and natural language processing, are enhancing qualitative studies on aging and later life. By integrating these tools with traditional qualitative methods, researchers can analyze large volumes of qualitative data, identify patterns, and maintain connections to in-depth accounts. The use of computational methods in qualitative research has the potential to streamline workflows, scale up projects, and generate new insights into aging and the life course, while also highlighting the importance of maintaining the methodological foundations of qualitative research."
  },
  {
    "title": "ShareChat: A Dataset of Chatbot Conversations in the Wild",
    "abstract": "While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset's multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.",
    "url": "https://arxiv.org/abs/2512.17843",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces ShareChat, a dataset of chatbot conversations collected from various platforms, preserving platform-specific features and interactions. The dataset includes a large number of conversations in multiple languages, offering longer context windows and greater interaction depth compared to existing datasets. The research demonstrates the dataset's utility through analyses on user intent satisfaction, source citation behaviors, and evolving usage patterns, providing a valuable resource for studying authentic user-chatbot interactions."
  },
  {
    "title": "Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation",
    "abstract": "Video-based gaze estimation methods aim to capture the inherently temporal dynamics of human eye gaze from multiple image frames. However, since models must capture both spatial and temporal relationships, performance is limited by the feature representations within a frame but also between multiple frames. We propose the Spatio-Temporal Gaze Network (ST-Gaze), a model that combines a CNN backbone with dedicated channel attention and self-attention modules to fuse eye and face features optimally. The fused features are then treated as a spatial sequence, allowing for the capture of an intra-frame context, which is then propagated through time to model inter-frame dynamics. We evaluated our method on the EVE dataset and show that ST-Gaze achieves state-of-the-art performance both with and without person-specific adaptation. Additionally, our ablation study provides further insights into the model performance, showing that preserving and modelling intra-frame spatial context with our spatio-temporal recurrence is fundamentally superior to premature spatial pooling. As such, our results pave the way towards more robust video-based gaze estimation using commonly available cameras.",
    "url": "https://arxiv.org/abs/2512.17673",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research presents the Spatio-Temporal Gaze Network (ST-Gaze) model, which combines CNN backbone with channel attention and self-attention modules to optimize the fusion of eye and face features for video-based gaze estimation. The model captures both intra-frame spatial context and inter-frame dynamics, leading to state-of-the-art performance on the EVE dataset. The study highlights the importance of preserving and modeling intra-frame spatial context over premature spatial pooling, paving the way for more robust video-based gaze estimation using standard cameras."
  },
  {
    "title": "A Service Robot's Guide to Interacting with Busy Customers",
    "abstract": "The growing use of service robots in hospitality highlights the need to understand how to effectively communicate with pre-occupied customers. This study investigates the efficacy of commonly used communication modalities by service robots, namely, acoustic/speech, visual display, and micromotion gestures in capturing attention and communicating intention with a user in a simulated restaurant scenario. We conducted a two-part user study (N=24) using a Temi robot to simulate delivery tasks, with participants engaged in a typing game (MonkeyType) to emulate a state of busyness. The participants' engagement in the typing game is measured by words per minute (WPM) and typing accuracy. In Part 1, we compared non-verbal acoustic cue versus baseline conditions to assess attention capture during a single-cup delivery task. In Part 2, we evaluated the effectiveness of speech, visual display, micromotion and their multimodal combination in conveying specific intentions (correct cup selection) during a two-cup delivery task. The results indicate that, while speech is highly effective in capturing attention, it is less successful in clearly communicating intention. Participants rated visual as the most effective modality for intention clarity, followed by speech, with micromotion being the lowest this http URL findings provide insights into optimizing communication strategies for service robots, highlighting the distinct roles of attention capture and intention communication in enhancing user experience in dynamic hospitality settings.",
    "url": "https://arxiv.org/abs/2512.17241",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores how service robots can effectively communicate with busy customers in a hospitality setting. The research compares the effectiveness of different communication modalities, finding that while speech is good at capturing attention, visual displays are better at conveying intention. These findings provide valuable insights for optimizing communication strategies for service robots in dynamic hospitality environments."
  },
  {
    "title": "Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation",
    "abstract": "Deep learning (DL)-based automated cybersickness detection methods, along with adaptive mitigation techniques, can enhance user comfort and interaction. However, recent studies show that these DL-based systems are susceptible to adversarial attacks; small perturbations to sensor inputs can degrade model performance, trigger incorrect mitigation, and disrupt the user's immersive experience (UIX). Additionally, there is a lack of dedicated open-source testbeds that evaluate the robustness of these systems under adversarial conditions, limiting the ability to assess their real-world effectiveness. To address this gap, this paper introduces Adversarial-VR, a novel real-time VR testbed for evaluating DL-based cybersickness detection and mitigation strategies under adversarial conditions. Developed in Unity, the testbed integrates two state-of-the-art (SOTA) DL models: DeepTCN and Transformer, which are trained on the open-source MazeSick dataset, for real-time cybersickness severity detection and applies a dynamic visual tunneling mechanism that adjusts the field-of-view based on model outputs. To assess robustness, we incorporate three SOTA adversarial attacks: MI-FGSM, PGD, and C&W, which successfully prevent cybersickness mitigation by fooling DL-based cybersickness models' outcomes. We implement these attacks using a testbed with a custom-built VR Maze simulation and an HTC Vive Pro Eye headset, and we open-source our implementation for widespread adoption by VR developers and researchers. Results show that these adversarial attacks are capable of successfully fooling the system. For instance, the C&W attack results in a $5.94x decrease in accuracy for the Transformer-based cybersickness model compared to the accuracy without the attack.",
    "url": "https://arxiv.org/abs/2512.17029",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research paper introduces Adversarial-VR, a real-time VR testbed for evaluating DL-based cybersickness detection and mitigation strategies under adversarial conditions. The study highlights the susceptibility of DL-based systems to adversarial attacks, which can degrade model performance and disrupt the user's immersive experience. The results demonstrate the effectiveness of three SOTA adversarial attacks in fooling DL-based cybersickness models, emphasizing the importance of evaluating the robustness of these systems for real-world effectiveness."
  },
  {
    "title": "Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error",
    "abstract": "Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.",
    "url": "https://arxiv.org/abs/2512.16750",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores how errors in large language models (LLMs) impact human judgment in everyday reasoning. The research shows that LLM errors can be masked by linguistic fluency and superficial plausibility, leading evaluators to rely on surface cues and intuitive heuristics rather than analytical distinctions. The findings suggest that errors in LLMs are not solely a result of model behavior but are also influenced by human interpretive shortcuts, highlighting the importance of understanding AI epistemic failure as a co-constructed outcome of generative plausibility and human judgment."
  },
  {
    "title": "Poster: Recognizing Hidden-in-the-Ear Private Key for Reliable Silent Speech Interface Using Multi-Task Learning",
    "abstract": "Silent speech interface (SSI) enables hands-free input without audible vocalization, but most SSI systems do not verify speaker identity. We present HEar-ID, which uses consumer active noise-canceling earbuds to capture low-frequency \"whisper\" audio and high-frequency ultrasonic reflections. Features from both streams pass through a shared encoder, producing embeddings that feed a contrastive branch for user authentication and an SSI head for silent spelling recognition. This design supports decoding of 50 words while reliably rejecting impostors, all on commodity earbuds with a single model. Experiments demonstrate that HEar-ID achieves strong spelling accuracy and robust authentication.",
    "url": "https://arxiv.org/abs/2512.16518",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces HEar-ID, a system that utilizes consumer active noise-canceling earbuds to capture whisper audio and ultrasonic reflections for user authentication and silent spelling recognition. The system uses multi-task learning to decode 50 words and reliably reject impostors, demonstrating strong spelling accuracy and robust authentication on commodity earbuds with a single model. This research is significant as it addresses the issue of speaker identity verification in silent speech interface systems, offering a reliable and efficient solution for hands-free input."
  },
  {
    "title": "Investigating the Effect of Encumbrance on Gaze- and Touch-based Target Acquisition on Handheld Mobile Devices",
    "abstract": "The potential of using gaze as an input modality in the mobile context is growing. While users often encumber themselves by carrying objects and using mobile devices while walking, the impact of encumbrance on gaze input performance remains unexplored. To investigate this, we conducted a user study (N=24) to evaluate the effect of encumbrance on the performance of 1) Gaze using Dwell time (with/without visual feedback), 2) GazeTouch (with/without visual feedback), and 3) One- or two-hand touch input. While Touch generally performed better, Gaze, especially with feedback, showed a consistent performance regardless of whether participants were encumbered or unencumbered. Participants' preferences for input modalities varied with encumbrance: they preferred Gaze when encumbered, and touch when unencumbered. Our findings enhance understanding of the effect of encumbrance on gaze input and contribute towards selecting appropriate input modalities in future mobile user interfaces to account for situational impairments.",
    "url": "https://arxiv.org/abs/2512.16472",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research study investigated the impact of encumbrance on the performance of gaze- and touch-based target acquisition on handheld mobile devices. The study found that while touch input generally performed better, gaze input with feedback showed consistent performance regardless of encumbrance. Participants preferred gaze input when encumbered and touch input when unencumbered, highlighting the importance of considering situational impairments when designing mobile user interfaces."
  },
  {
    "title": "Preparing Future-Ready Learners: K12 Skills Shift and GenAI EdTech Innovation Direction",
    "abstract": "Since Generative AI came out it has quickly embedded itself in our social fabric, triggering lots of discussions, predictions, and efforts from research, industry, government and capital market to experiment and embrace the technology. The question for the global K12 education is, what and how should our children learn in this fast changing world to be prepared for the changing labor market and live a happy and balanced life? Three key aspects will be discussed: 1) Skills; 2) Evaluation of Learning; 3) Strategic GenAI-powered EdTech innovation for long term educational impact.",
    "url": "https://arxiv.org/abs/2512.16428",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the impact of Generative AI on K12 education and the skills students will need to succeed in a rapidly changing world. The study emphasizes the importance of teaching skills that are relevant to the future labor market and promoting a balanced and fulfilling life. It also highlights the need for innovative GenAI-powered educational technology to have a long-lasting impact on education."
  },
  {
    "title": "Mind the Gaze: Improving the Usability of Dwell Input by Adapting Gaze Targets Based on Viewing Distance",
    "abstract": "Dwell input shows promise for handheld mobile contexts, but its performance is impacted by target size and viewing distance. While fixed target sizes suffice in static setups, in mobile settings, frequent posture changes alter viewing distances, which in turn distort perceived size and hinder dwell performance. We address this through GAUI, a Gaze-based Adaptive User Interface that dynamically resizes targets to maximise performance at the given viewing distance. In a two-phased study (N=24), GAUI leveraged the strengths of its distance-responsive design, outperforming the large UI static baseline in task time, and being less error-prone than the small UI static baseline. It was rated the most preferred interface overall. Participants reflected on using GAUI in six different postures. We discuss how their experience is impacted by posture, and propose guidelines for designing context-aware adaptive UIs for dwell interfaces on handheld mobile devices that maximise performance.",
    "url": "https://arxiv.org/abs/2512.16366",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the impact of viewing distance on dwell input performance in mobile contexts and introduces GAUI, a Gaze-based Adaptive User Interface that dynamically resizes targets to optimize performance. The study found that GAUI outperformed static UI baselines in task time and error rates, and was the most preferred interface overall. The findings suggest the importance of designing context-aware adaptive UIs for dwell interfaces on handheld mobile devices to maximize performance."
  },
  {
    "title": "Ein Typenrad auf der Überholspur: Die Kult-Schreibmaschine \"Erika\" trifft KI",
    "abstract": "In the 15th century, printing revolutionized the dissemination of information. Innovations such as typewriters and computers have increased the speed and volume of information flows over time. More recent developments in large language models such as ChatGPT enable text to be generated in a matter of seconds. However, many people do not understand how this works and what the long-term implications are. That is why we have \"hacked\" an old typewriter so that users can interact with an LLM chatbot, which over 1,200 participants have now been able to experience. It helps to understand the possibilities and limitations of AI. It gives us researchers insights into participants' concepts of AI as well as their expectations and concerns. It raises questions about these technological developments and stimulates discussions about the social impact of the intensification and acceleration of information and communication flows.",
    "url": "https://arxiv.org/abs/2512.16293",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research project explores the intersection of traditional typewriters and artificial intelligence, specifically large language models like ChatGPT. By \"hacking\" an old typewriter to interact with a chatbot, over 1,200 participants were able to experience and understand the capabilities and limitations of AI. The study sheds light on participants' perceptions, expectations, and concerns regarding AI, sparking discussions about the social impact of rapid information dissemination and communication technologies."
  },
  {
    "title": "Machines, AI and the past//future of things",
    "abstract": "This essay explores a techno-artistic experiment that reanimates a 1980s East German typewriter using a contemporary AI language model. Situated at the intersection of media archaeology and speculative design, the project questions dominant narratives of progress by embedding generative AI in an obsolete, tactile interface. Through public exhibitions and aesthetic intervention, we demonstrate how slowness, friction, and material render artificial intelligence not only visible but open to critical inquiry. Drawing on concepts such as zombie media, technostalgia, and speculative design, we argue that reappropriating outdated technologies enables new forms of critical engagement. Erika - the AI-enabled typewriter - functions as both interface and interruption, making space for reflection, irony, and cultural memory. In a moment of accelerated digital abstraction, projects like this foreground the value of deliberate slowness, experiential materiality, and historical depth. We conclude by advocating for a historicist design sensibility that challenges presentism and reorients human-machine interaction toward alternative, perceived futures.",
    "url": "https://arxiv.org/abs/2512.16285",
    "journal": "arXiv cs.HC",
    "ai_summary": "This essay discusses an experiment that uses AI to bring an old typewriter back to life, challenging the idea of technological progress. By reappropriating outdated technologies, the project encourages critical engagement and reflection on the intersection of AI and materiality. The project emphasizes the importance of slowness, materiality, and historical depth in human-machine interactions, advocating for a design sensibility that challenges presentism and explores alternative futures."
  },
  {
    "title": "The Agony of Opacity: Foundations for Reflective Interpretability in AI-Mediated Mental Health Support",
    "abstract": "Throughout history, a prevailing paradigm in mental healthcare has been one in which distressed people may receive treatment with little understanding around how their experience is perceived by their care provider, and in turn, the decisions made by their provider around how treatment will progress. Paralleling this offline model of care, people who seek mental health support from AI chatbots are similarly provided little context for how their expressions of distress are processed by the model, and subsequently, the logic that may underlie model responses. People in severe distress who turn to AI chatbots for support thus find themselves caught between black boxes, with unique forms of agony that arise from these intersecting opacities, including misinterpreting model outputs or attributing greater capabilities to a model than are yet possible, which has led to documented real-world harms. Building on empirical research from clinical psychology and AI safety, alongside rights-oriented frameworks from medical ethics, we describe how the distinct psychological state induced by severe distress can influence chatbot interaction patterns, and argue that this state of mind (combined with differences in how a user might perceive a chatbot compared to a care provider) uniquely necessitates a higher standard of interpretability in comparison to general AI chatbot use. Drawing inspiration from newer interpretable treatment paradigms, we then describe specific technical and interface design approaches that could be used to adapt interpretability strategies from four specific mental health fields (psychotherapy, community-based crisis intervention, psychiatry, and care authorization) to AI models, including consideration of the role of interpretability in the treatment process and tensions that may arise with greater interpretability.",
    "url": "https://arxiv.org/abs/2512.16206",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the lack of transparency in AI-mediated mental health support, highlighting the unique challenges faced by individuals in severe distress when interacting with AI chatbots. The study emphasizes the importance of interpretability in AI models for mental health support, drawing on insights from clinical psychology, AI safety, and medical ethics. The research suggests adapting interpretability strategies from various mental health fields to improve the transparency and effectiveness of AI chatbots in providing mental health support."
  },
  {
    "title": "Evaluation of Generative Models for Emotional 3D Animation Generation in VR",
    "abstract": "Social interactions incorporate nonverbal signals to convey emotions alongside speech, including facial expressions and body gestures. Generative models have demonstrated promising results in creating full-body nonverbal animations synchronized with speech; however, evaluations using statistical metrics in 2D settings fail to fully capture user-perceived emotions, limiting our understanding of model effectiveness. To address this, we evaluate emotional 3D animation generative models within a Virtual Reality (VR) environment, emphasizing user-centric metrics emotional arousal realism, naturalness, enjoyment, diversity, and interaction quality in a real-time human-agent interaction scenario. Through a user study (N=48), we examine perceived emotional quality for three state of the art speech-driven 3D animation methods across two emotions happiness (high arousal) and neutral (mid arousal). Additionally, we compare these generative models against real human expressions obtained via a reconstruction-based method to assess both their strengths and limitations and how closely they replicate real human facial and body expressions. Our results demonstrate that methods explicitly modeling emotions lead to higher recognition accuracy compared to those focusing solely on speech-driven synchrony. Users rated the realism and naturalness of happy animations significantly higher than those of neutral animations, highlighting the limitations of current generative models in handling subtle emotional states. Generative models underperformed compared to reconstruction-based methods in facial expression quality, and all methods received relatively low ratings for animation enjoyment and interaction quality, emphasizing the importance of incorporating user-centric evaluations into generative model development. Finally, participants positively recognized animation diversity across all generative models.",
    "url": "https://arxiv.org/abs/2512.16081",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research evaluates generative models for emotional 3D animation generation in VR, focusing on user-centric metrics like emotional arousal realism, naturalness, enjoyment, diversity, and interaction quality. The study compares three state-of-the-art speech-driven 3D animation methods for happiness and neutral emotions, finding that models explicitly modeling emotions lead to higher recognition accuracy. However, the study also highlights limitations in handling subtle emotional states, facial expression quality, animation enjoyment, and interaction quality, emphasizing the need for user-centric evaluations in generative model development."
  },
  {
    "title": "WING: An Adaptive and Gamified Mobile Learning Platform for Neurodivergent Literacy",
    "abstract": "This paper presents WING, an adaptive and gamified mobile learning platform designed to support literacy development for neurodivergent children. Motivated by the limitations of traditional literacy approaches in addressing diverse cognitive profiles, the platform integrates inclusive Human-Computer Interaction principles, multisensory design, and adaptive learning paths. WING digitally transposes the Alfabetização Adaptada (AFA) method into an interactive mobile environment, combining usability guidelines for neurodivergent users with gamification strategies to enhance engagement and autonomy. The study follows an applied research methodology, encompassing requirements elicitation, inclusive interface design, high-fidelity prototyping, and qualitative and quantitative evaluation planning. Preliminary results include a functional minimum viable product validated through expert feedback and public exhibitions, indicating the feasibility and potential pedagogical impact of the proposed approach. The platform aims to act as a complementary educational tool, promoting accessibility, personalization, and inclusive digital literacy.",
    "url": "https://arxiv.org/abs/2512.16067",
    "journal": "arXiv cs.HC",
    "ai_summary": "The paper introduces WING, a mobile learning platform designed to support literacy development for neurodivergent children by integrating adaptive learning paths, inclusive design principles, and gamification strategies. The platform, based on the Alfabetização Adaptada method, aims to enhance engagement and autonomy while promoting accessibility and personalization in digital literacy education. Preliminary results show a functional minimum viable product validated through expert feedback, highlighting the feasibility and potential pedagogical impact of the approach."
  },
  {
    "title": "A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis",
    "abstract": "Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.",
    "url": "https://arxiv.org/abs/2512.16063",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study introduces a multi-agent large language model framework called CoTI, which automates qualitative thematic analysis for patient experiences in chronic diseases. CoTI was able to identify key phrases, themes, and codebooks that closely aligned with a senior investigator's analysis, outperforming junior investigators and baseline NLP models. The implementation of CoTI in a user-facing application showed potential for AI-human collaboration in qualitative analysis, although junior investigators may need to guard against overreliance on CoTI to maintain independent critical thinking."
  },
  {
    "title": "Augmented Reality-Based Smart Structural Health Monitoring System With Accurate 3D Model Alignment",
    "abstract": "Structural Health Monitoring (SHM) has become increasingly critical due to the rapid deterioration of civil infrastructure. Traditional methods involving heavy equipment are costly and time-consuming. Recent SHM approaches use advanced non-contact sensors, IoT, and Augmented Reality (AR) glasses for faster inspections and immersive experiences during inspections. However, current methods lack quantitative damage data, remote collaboration support, and accurate 3D model alignment with the real structure. Recognizing these current challenges, this paper proposes an AR-based system that integrates Building Information Modelling (BIM) visualization and follows a flexible manipulation approach of 3D holograms to improve structural condition assessments. The proposed framework utilizes the Vuforia software development toolkit to enable the automatic alignment of 3D models to the real structure, ensuring successful model alignment to assist users in accurately visualizing damage locations. The framework also enables flexible manipulation of damage locations, making it easier for users to identify multiple damage points in the 3D models. The system is validated through lab-scale and full-scale bridge use cases, with data transfer performance analyzed under 4G and 5G conditions for remote collaboration. This study demonstrates that the proposed AR-based SHM framework successfully aligns 3D models with real structures, allowing users to manually adjust models and damage locations. The experimental results confirm its feasibility for remote collaborative inspections, highlighting significant improvements with 5G networks. Nevertheless, performance under 4G remains acceptable, ensuring reliability even without 5G coverage.",
    "url": "https://arxiv.org/abs/2512.16008",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper introduces an Augmented Reality-based Structural Health Monitoring system that integrates Building Information Modelling (BIM) visualization and flexible manipulation of 3D holograms to improve structural condition assessments. The system utilizes the Vuforia software development toolkit to automatically align 3D models with real structures, allowing users to accurately visualize damage locations and identify multiple damage points. The study demonstrates the feasibility of the proposed system for remote collaborative inspections, showing significant improvements with 5G networks while remaining reliable under 4G coverage."
  },
  {
    "title": "The Emerging Use of GenAI for UX Research in Software Development: Challenges and Opportunities",
    "abstract": "The growing adoption of generative AI (GenAI) is reshaping how user experience (UX) research teams conduct qualitative research in software development, creating opportunities to streamline the production of qualitative insights. This paper presents findings from two user studies examining how current practices are challenged by GenAI and offering design implications for future AI assistance. Semi-structured interviews with 21 UX researchers, product managers, and designers reveal challenges of aligning AI capabilities with the interpretive, collaborative nature of qualitative research and tensions between roles. UX researchers expressed limited trust in AI-generated results, while product managers often overestimated AI capabilities, amplifying organizational pressures to accelerate research within agile workflows. In a second study, we validated an AI analysis approach more closely aligned with human analysis processes to address trust issues bottoms-up. We outline interaction patterns and design guidelines for responsibly integrating AI into software development cycles.",
    "url": "https://arxiv.org/abs/2512.15944",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research paper explores the impact of generative AI (GenAI) on qualitative research in software development, highlighting challenges and opportunities for UX research teams. Findings from two user studies reveal tensions between AI capabilities and the collaborative nature of qualitative research, with UX researchers expressing limited trust in AI-generated results. The study proposes a more human-aligned AI analysis approach to address trust issues and offers design guidelines for integrating AI into software development cycles responsibly."
  },
  {
    "title": "Non-Stationarity in Brain-Computer Interfaces: An Analytical Perspective",
    "abstract": "Non-invasive Brain-Computer Interface (BCI) systems based on electroencephalography (EEG) signals suffer from multiple obstacles to reach a wide adoption in clinical settings for communication or rehabilitation. Among these challenges, the non-stationarity of the EEG signal is a key problem as it leads to various changes in the signal. There are changes within a session, across sessions, and across individuals. Variations over time for a given individual must be carefully managed to improve the BCI performance, including its accuracy, reliability, and robustness over time. This review paper presents and discusses the causes of non-stationarity in the EEG signal, along with its consequences for BCI applications, including covariate shift. The paper reviews recent studies on covariate shift, focusing on methods for detecting and correcting this phenomenon. Signal processing and machine learning techniques can be employed to normalize the EEG signal and address the covariate shift.",
    "url": "https://arxiv.org/abs/2512.15941",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper focuses on the non-stationarity of EEG signals in non-invasive Brain-Computer Interface (BCI) systems, which poses a significant challenge for their adoption in clinical settings. The review discusses the causes and consequences of non-stationarity, particularly in terms of covariate shift, and highlights the importance of managing variations over time to improve BCI performance. The paper also explores signal processing and machine learning techniques that can be used to normalize EEG signals and address the issue of covariate shift, ultimately enhancing the accuracy, reliability, and robustness of BCI systems."
  },
  {
    "title": "Management von Sensordaten im Smarthome: Besonderheiten und Ansätze",
    "abstract": "A wide variety of simple sensors, e.g. for temperature, light, or humidity, is finding its way into smart homes. There are special features to consider with regard to the data collected by these sensors: a) the nature of the measured data as \"thin but big data\" that needs to be contextualized and interpreted, b) which both algorithms and humans are capable of doing (resulting in comprehensive information in the context of the home, including the recognition of activities, behavior, and health of the residents), and c) uses that lead to interesting positive applications, but also to misuse and implications for privacy. When managing such data, it is necessary to take these special features into account, for which the principles of user experience, human-data interaction, and data protection should be considered together. We present our research tool \"Sensorkit\" and the participatory research approach used with it to collect sensor data in real homes. In our findings, we present identified challenges and explain how we address them through a) meaningful default settings, b) opportunities for users to interact and intervene, and c) life-cycle management of the data. Important aspects include phases before, during, and after the collection, processing, and use of the sensor data, as well as the provision of user-friendly tools and user involvement. Our findings inform beyond the scope of a research project also the development and use of commercial smart home devices and services.",
    "url": "https://arxiv.org/abs/2512.15918",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on managing sensor data in smart homes, highlighting the unique challenges and opportunities presented by the nature of the data collected. The study emphasizes the importance of considering user experience, human-data interaction, and data protection principles when managing sensor data. The findings suggest that meaningful default settings, user interaction opportunities, and life-cycle management of data are crucial for addressing challenges and ensuring the development of user-friendly smart home devices and services."
  },
  {
    "title": "Enhanced Web User Interface Design Via Cross-Device Responsiveness Assessment Using An Improved HCI-INTEGRATED DL Schemes",
    "abstract": "User Interface (UI) optimization is essential in the digital era to enhance user satisfaction in web environments. Nevertheless, the existing UI optimization models had overlooked the Cross-Responsiveness (CR) assessment, affecting the user interaction efficiency. Consequently, this article proposes a dynamic web UI optimization through CR assessment using Finite Exponential Continuous State Machine (FECSM) and Quokka Nonlinear Difference Swarm Optimization Algorithm (QNDSOA). Initially, the design and user interaction related information is collected as well as pre-processed for min-max normalization. Next, the Human-Computer Interaction (HCI)-based features are extracted, followed by user behaviour pattern grouping. Meanwhile, the CR assessment is done using FECSM. Then, the proposed Bidirectional Gated Luong and Mish Recurrent Unit (BiGLMRU) is used to classify the User eXperience (UX) change type, which is labelled based on the User Interface Change Prediction Index (UICPI). Lastly, a novel QNDSOA is utilized to optimize the UI design with an average fitness of 98.5632%. Feedback monitoring is done after optimal deployment.",
    "url": "https://arxiv.org/abs/2512.15775",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on enhancing web user interface design by incorporating cross-device responsiveness assessment, which is often overlooked in existing UI optimization models. The study proposes a dynamic approach using Finite Exponential Continuous State Machine and Quokka Nonlinear Difference Swarm Optimization Algorithm to optimize UI design based on user behavior patterns and HCI-based features. The results show significant improvement in user experience with an average fitness of 98.5632%, highlighting the importance of considering cross-responsiveness in UI optimization for better user satisfaction in web environments."
  },
  {
    "title": "PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy",
    "abstract": "The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.",
    "url": "https://arxiv.org/abs/2512.16851",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces PrivateXR, a framework that combines explainable AI (XAI) and differential privacy (DP) to defend against privacy attacks in AI XR systems. By selectively applying DP to the most influential features identified through XAI, the proposed method reduces membership inference and re-identification success rates while maintaining model accuracy and improving inference time. The results demonstrate the effectiveness of the approach in protecting user privacy during XR gameplay, making it a practical solution for ensuring data privacy in AI XR applications."
  },
  {
    "title": "OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition",
    "abstract": "Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: this https URL",
    "url": "https://arxiv.org/abs/2512.16727",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces OMG-Bench, a new benchmark for skeleton-based online micro hand gesture recognition, addressing the challenges of limited datasets and task-specific algorithms. The study develops a self-supervised pipeline to generate skeleton data and introduces HMATr, an end-to-end framework that outperforms existing methods by 7.6% in detection rate, providing a strong baseline for online micro gesture recognition in VR/AR interactions."
  },
  {
    "title": "ParamExplorer: A framework for exploring parameters in generative art",
    "abstract": "Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.",
    "url": "https://arxiv.org/abs/2512.16529",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces ParamExplorer, a framework designed to help artists explore complex parameter spaces in generative art systems more efficiently. By using interactive and modular tools inspired by reinforcement learning, artists can discover potentially interesting configurations with human-in-the-loop or automated feedback. The framework also includes various exploration strategies, referred to as agents, to aid in the discovery of aesthetically compelling outputs in generative art algorithms."
  },
  {
    "title": "Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers",
    "abstract": "Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \\toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \\toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at this https URL.",
    "url": "https://arxiv.org/abs/2512.16083",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces a new framework called \\toolname that efficiently filters schemas in Text2SQL systems by ranking columns using a query-aware LLM encoder and reranking inter-connected columns based on functional dependencies. This approach allows for better scalability to large databases with thousands of columns, achieving high recall and precision while maintaining low latency. The framework outperforms existing systems like CodeS, SchemaExP, and Qwen rerankers, making it a valuable tool for handling complex real-world schemas in Text2SQL tasks."
  },
  {
    "title": "TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge",
    "abstract": "Surface electromyography (EMG) is a non-invasive sensing modality used in several domains, including biomechanics, rehabilitation, prosthetic control, and emerging human-machine interaction paradigms. Despite decades of use, significant challenges remain in achieving robust generalization across subjects, recording systems, and acquisition protocols. To tackle these challenges, foundation models (FMs) are gaining traction when targeting end-to-end applications based on EMG signals. Yet, existing EMG FMs remain limited to single downstream tasks and lack deployability on embedded platforms. In this work, we present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner on publicly available datasets and achieves high reconstruction fidelity with only 3.6M parameters. With minimal task-specific head adaptations, the same backbone is used to tackle multiple downstream tasks, leveraging datasets acquired from diverse sensing locations and hardware platforms. We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 ($89.4\\pm0.16\\%$), UCI-EMG ($97.56\\pm0.32\\%$), and EPN-612 ($96.74\\pm0.09\\%$) datasets. We report, to the best of our knowledge, the first deployment of an EMG FM on an ultra-low-power microcontroller (GAP9), achieving an average power envelope of 36.45mW. By open-sourcing the pre-trained and the downstream task architectures (this https URL), we aim to provide a flexible resource that can accelerate future research and serve as a common foundation for the EMG community.",
    "url": "https://arxiv.org/abs/2512.15729",
    "journal": "arXiv cs.HC",
    "ai_summary": "The researchers developed TinyMyo, a lightweight foundation model for processing electromyography (EMG) signals at the edge. This model achieved high reconstruction fidelity with minimal parameters and was able to generalize across multiple downstream tasks with datasets from diverse sensing locations and hardware platforms. The model outperformed previous works on hand gesture classification, hand kinematic regression, speech production and recognition, and was successfully deployed on an ultra-low-power microcontroller, showcasing its potential for real-world applications in various domains."
  },
  {
    "title": "A Constructive Scientific Methodology to Improve Climate Figures from IPCC",
    "abstract": "We propose a methodology to improve figures from the Intergovernmental Panel on Climate Change (IPCC), ensuring that all modifications remain scientifically rigorous. IPCC figures are notoriously difficult to understand, and although designers have proposed alternatives, these lack formal IPCC validation and can be dismissed by skeptics. To address this gap, our approach starts from official IPCC figures. We gather their associated learning objectives and devise tests to score a pool of figure readers to assess how well they learn the this http URL define improvement as higher scores obtained by a comparable reader pool after viewing a revised figure, where all modifications undergo review to ensure scientific validity. This assessment gives freedom to designers, who can deviate from the original design while making sure the objectives are still met and improved. We demonstrate the methodology through a case study and describe unexpected challenges encountered during the process.",
    "url": "https://arxiv.org/abs/2512.15514",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research proposes a methodology to improve the clarity and understanding of figures from the IPCC while maintaining scientific rigor. By starting with official IPCC figures and conducting tests with a pool of readers, the study aims to ensure that any modifications made to the figures lead to better comprehension without sacrificing accuracy. The methodology allows designers to make improvements to the figures while still meeting the original learning objectives, ultimately enhancing the effectiveness of communicating climate change information."
  },
  {
    "title": "GazeBlend: Exploring Paired Gaze-Based Input Techniques for Navigation and Selection Tasks on Mobile Devices",
    "abstract": "The potential of gaze for hands-free mobile interaction is increasingly evident. While each gaze input technique presents distinct advantages and limitations, a combination can amplify strengths and mitigate challenges. We report on the results of a user study (N=24), in which we compared the usability and performance of pairing three popular gaze input techniques: Dwell Time, Pursuits, and Gaze Gestures, for navigation and selection tasks while sitting and walking. Results show that pairing gestures for navigation with either Dwell time or Pursuits for selection improves task completion time and rate compared to using either individually. We discuss the implications of pairing gaze input techniques, such as how Pursuits may negatively impact other techniques, likely due to the visual clutter it adds, how integrating gestures for navigation reduces the chances of unintentional selections, and the impact of motor activity on performance. Our findings provide insights for effective gaze-enabled interfaces.",
    "url": "https://arxiv.org/abs/2512.15491",
    "journal": "arXiv cs.HC",
    "ai_summary": "The study explores the effectiveness of combining different gaze-based input techniques for navigation and selection tasks on mobile devices. Results from a user study show that pairing gestures for navigation with either Dwell Time or Pursuits for selection improves task completion time and rate compared to using them individually. The findings suggest that integrating different gaze input techniques can enhance usability and performance in hands-free mobile interaction."
  },
  {
    "title": "Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality",
    "abstract": "The rapid development of generative artificial intelligence (AI) and large language models (LLMs), and the availability of services that make them accessible, have led the general public to begin incorporating them into everyday life. The extended reality (XR) community has also sought to integrate LLMs, particularly in the form of conversational agents, to enhance user experience and task efficiency. When interacting with such conversational agents, users may easily disclose sensitive information due to the naturalistic flow of the conversations, and combining such conversational data with fine-grained sensor data may lead to novel privacy issues. To address these issues, a user-centric understanding of technology acceptance and concerns is essential. Therefore, to this end, we conducted a large-scale crowdsourcing study with 1036 participants, examining user decision-making processes regarding LLM-powered conversational agents in XR, across factors of XR setting type, speech interaction type, and data processing location. We found that while users generally accept these technologies, they express concerns related to security, privacy, social implications, and trust. Our results suggest that familiarity plays a crucial role, as daily generative AI use is associated with greater acceptance. In contrast, previous ownership of XR devices is linked to less acceptance, possibly due to existing familiarity with the settings. We also found that men report higher acceptance with fewer concerns than women. Regarding data type sensitivity, location data elicited the most significant concern, while body temperature and virtual object states were considered least sensitive. Overall, our study highlights the importance of practitioners effectively communicating their measures to users, who may remain distrustful. We conclude with implications and recommendations for LLM-powered XR.",
    "url": "https://arxiv.org/abs/2512.15343",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores user acceptance and concerns towards conversational agents powered by large language models in immersive extended reality settings. The study found that while users generally accept these technologies, they have concerns related to security, privacy, social implications, and trust. Familiarity with generative AI and XR devices plays a significant role in acceptance, with men showing higher acceptance and fewer concerns than women. The study emphasizes the importance of effectively communicating measures to address user concerns and suggests implications and recommendations for the use of LLM-powered XR technologies."
  },
  {
    "title": "Managing Ambiguity: A Proof of Concept of Human-AI Symbiotic Sense-making based on Quantum-Inspired Cognitive Mechanism of Rogue Variable Detection",
    "abstract": "Organizations increasingly operate in environments characterized by volatility, uncertainty, complexity, and ambiguity (VUCA), where early indicators of change often emerge as weak, fragmented signals. Although artificial intelligence (AI) is widely used to support managerial decision-making, most AI-based systems remain optimized for prediction and resolution, leading to premature interpretive closure under conditions of high ambiguity. This creates a gap in management science regarding how human-AI systems can responsibly manage ambiguity before it crystallizes into error or crisis. This study addresses this gap by presenting a proof of concept (PoC) of the LAIZA human-AI augmented symbiotic intelligence system and its patented process: Systems and Methods for Quantum-Inspired Rogue Variable Modeling (QRVM), Human-in-the-Loop Decoherence, and Collective Cognitive Inference. The mechanism operationalizes ambiguity as a non-collapsed cognitive state, detects persistent interpretive breakdowns (rogue variables), and activates structured human-in-the-loop clarification when autonomous inference becomes unreliable. Empirically, the article draws on a three-month case study conducted in 2025 within the AI development, involving prolonged ambiguity surrounding employee intentions and intellectual property boundaries. The findings show that preserving interpretive plurality enabled early scenario-based preparation, including proactive patent protection, allowing decisive and disruption-free action once ambiguity collapsed. The study contributes to management theory by reframing ambiguity as a first-class construct and demonstrates the practical value of human-AI symbiosis for organizational resilience in VUCA environments.",
    "url": "https://arxiv.org/abs/2512.15325",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the concept of managing ambiguity in VUCA environments using a human-AI symbiotic sense-making system based on quantum-inspired cognitive mechanisms. The study introduces a proof of concept of the LAIZA system, which detects interpretive breakdowns and involves human-in-the-loop clarification to prevent premature interpretive closure. The findings highlight the importance of preserving interpretive plurality in decision-making, leading to proactive preparation and decisive action in ambiguous situations, ultimately contributing to organizational resilience in complex environments."
  },
  {
    "title": "Development of Immersive Virtual and Augmented Reality-Based Joint Attention Training Platform for Children with Autism",
    "abstract": "Joint Attention (JA), a crucial social skill for developing shared focus, is often impaired in children with Autism Spectrum Disorder (ASD), affecting social communication and highlighting the need for early intervention. Addressing gaps in prior research, such as limited use of immersive technology and reliance on distracting peripherals, we developed a novel JA training platform using Augmented Reality (AR) and Virtual Reality (VR) devices. The platform integrates eye gaze-based interactions to ensure participants undivided attention. To validate the platform, we conducted experiments on ASD (N=19) and Neurotypical (NT) (N=13) participants under a trained pediatric neurologist's supervision. For quantitative analysis, we employed key measures such as the number of correct responses, the duration of establishing eye contact (s), and the duration of registering a response (s), along with correlations to CARS scores and age. Results from AR-based experiments showed NT participants registered responses significantly faster (<0.00001) than ASD participants. A correlation (Spearman coefficient=0.57, p=0.03) was found between ASD participants response time and CARS scores. A similar trend was observed in VR-based experiments. When comparing response accuracy in ASD participants across platforms, AR yielded a higher correctness rate (92.30%) than VR (69.49%), indicating AR's greater effectiveness. These findings suggest that immersive technology can aid JA training in ASD. Future studies should explore long-term benefits and real-world applicability.",
    "url": "https://arxiv.org/abs/2512.15263",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research developed a novel Joint Attention training platform using Augmented Reality (AR) and Virtual Reality (VR) devices to improve social communication skills in children with Autism Spectrum Disorder (ASD). Results showed that AR was more effective than VR in improving response accuracy, with neurotypical participants performing significantly better than those with ASD. These findings suggest that immersive technology can be beneficial in aiding JA training for children with ASD, highlighting the potential for future studies to explore long-term benefits and real-world applicability."
  },
  {
    "title": "Lessons Learnt from Expert-Centred Studies Exploring Opportunities and Challenges for Immersive Forensic Investigation",
    "abstract": "Research studies involving human participants present challenges, including strict ethical considerations, participant recruitment, costs, and many human factors. While human-computer interaction researchers are familiar with these challenges and current solutions, expert-centred studies can be even more challenging in ways that researchers may not anticipate. This issue is particularly important as research grants are increasingly based on practical and real-world problems, which necessitate close collaboration with experts. In this paper, we reflect on and discuss the challenges, solutions, and specific requirements that arose during our expert-centred studies conducted over three years of a PhD study exploring immersive forensic investigation.",
    "url": "https://arxiv.org/abs/2512.15220",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study discusses the challenges and solutions encountered in expert-centred studies exploring immersive forensic investigation. The researchers highlight the importance of close collaboration with experts in addressing practical and real-world problems in AI research. The paper reflects on the specific requirements and lessons learned from conducting these studies over a three-year period."
  },
  {
    "title": "\"I am here for you\": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable",
    "abstract": "General-purpose conversational AI chatbots and AI companions increasingly provide young adolescents with emotionally supportive conversations, raising questions about how conversational style shapes anthropomorphism and emotional reliance. In a preregistered online experiment with 284 adolescent-parent dyads, youth aged 11-15 and their parents read two matched transcripts in which a chatbot responded to an everyday social problem using either a relational style (first-person, affiliative, commitment language) or a transparent style (explicit nonhumanness, informational tone). Adolescents more often preferred the relational than the transparent style, whereas parents were more likely to prefer transparent style than adolescents. Adolescents rated the relational chatbot as more human-like, likable, trustworthy and emotionally close, while perceiving both styles as similarly helpful. Adolescents who preferred relational style had lower family and peer relationship quality and higher stress and anxiety than those preferring transparent style or both chatbots. These findings identify conversational style as a key design lever for youth AI safety, showing that relational framing heightens anthropomorphism, trust and emotional closeness and can be especially appealing to socially and emotionally vulnerable adolescents, who may be at increased risk for emotional reliance on conversational AI.",
    "url": "https://arxiv.org/abs/2512.15117",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research study found that adolescents, especially those who are socially and emotionally vulnerable, prefer conversational AI chatbots that use a relational style over a transparent style. Adolescents rated the relational chatbot as more human-like, likable, trustworthy, and emotionally close, even though both styles were perceived as equally helpful. The findings suggest that the design of conversational AI, specifically using a relational style, can increase anthropomorphism, trust, and emotional closeness, which may be particularly beneficial for vulnerable adolescents who may rely on AI for emotional support."
  }
]