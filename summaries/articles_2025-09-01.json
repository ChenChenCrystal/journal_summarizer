[
  {
    "title": "MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality",
    "abstract": "Microbiomes are a vital part of the human body, engaging in tasks like food digestion and immune defense. Their structure and function must be understood in order to promote host health and facilitate swift recovery during disease. Due to the difficulties in experimentally studying these systems in situ, more research is being conducted in the field of mathematical modeling. Visualizing spatiotemporal data is challenging, and current tools that simulate microbial communities' spatial and temporal development often only provide limited functionalities, often requiring expert knowledge to generate useful results. To overcome these limitations, we provide a user-friendly tool to interactively explore spatiotemporal simulation data, called MicroLabVR, which transfers spatial data into virtual reality (VR) while following guidelines to enhance user experience (UX). With MicroLabVR, users can import CSV datasets containing population growth, substance concentration development, and metabolic flux distribution data. The implemented visualization methods allow users to evaluate the dataset in a VR environment interactively. MicroLabVR aims to improve data analysis for the user by allowing the exploration of microbiome data in their spatial context.",
    "url": "https://arxiv.org/abs/2508.21736",
    "journal": "arXiv cs.HC",
    "ai_summary": "MicroLabVR is a user-friendly tool that allows for the interactive exploration of spatiotemporal microbiome data in virtual reality. This tool aims to improve data analysis by providing a more intuitive way to visualize and understand the complex structure and function of microbial communities. By transferring spatial data into VR, MicroLabVR enhances user experience and facilitates the study of microbiomes in their spatial context."
  },
  {
    "title": "Developer Insights into Designing AI-Based Computer Perception Tools",
    "abstract": "Artificial intelligence (AI)-based computer perception (CP) technologies use mobile sensors to collect behavioral and physiological data for clinical decision-making. These tools can reshape how clinical knowledge is generated and interpreted. However, effective integration of these tools into clinical workflows depends on how developers balance clinical utility with user acceptability and trustworthiness. Our study presents findings from 20 in-depth interviews with developers of AI-based CP tools. Interviews were transcribed and inductive, thematic analysis was performed to identify 4 key design priorities: 1) to account for context and ensure explainability for both patients and clinicians; 2) align tools with existing clinical workflows; 3) appropriately customize to relevant stakeholders for usability and acceptability; and 4) push the boundaries of innovation while aligning with established paradigms. Our findings highlight that developers view themselves as not merely technical architects but also ethical stewards, designing tools that are both acceptable by users and epistemically responsible (prioritizing objectivity and pushing clinical knowledge forward). We offer the following suggestions to help achieve this balance: documenting how design choices around customization are made, defining limits for customization choices, transparently conveying information about outputs, and investing in user training. Achieving these goals will require interdisciplinary collaboration between developers, clinicians, and ethicists.",
    "url": "https://arxiv.org/abs/2508.21733",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores how developers of AI-based computer perception tools prioritize design considerations to ensure clinical utility, user acceptability, and trustworthiness. Key findings include the importance of context and explainability, alignment with existing workflows, customization for usability, and balancing innovation with established paradigms. Developers see themselves as ethical stewards, designing tools that are acceptable and epistemically responsible. Suggestions for achieving this balance include documenting design choices, defining customization limits, transparently conveying information, and investing in user training through interdisciplinary collaboration."
  },
  {
    "title": "Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education",
    "abstract": "This paper introduces the Future Atmospheric Conditions Training System (FACTS), a novel platform that advances climate resilience education through place-based, adaptive learning experiences. FACTS combines real-time atmospheric data collected by IoT sensors with curated resources from a Knowledge Base to dynamically generate localized learning challenges. Learner responses are analyzed by a Generative AI powered server, which delivers personalized feedback and adaptive support. Results from a user evaluation indicate that participants found the system both easy to use and effective for building knowledge related to climate resilience. These findings suggest that integrating IoT and Generative AI into atmospherically adaptive learning technologies holds significant promise for enhancing educational engagement and fostering climate awareness.",
    "url": "https://arxiv.org/abs/2508.21666",
    "journal": "arXiv cs.HC",
    "ai_summary": "The paper introduces FACTS, a platform that uses IoT sensors and Generative AI to create personalized climate resilience education experiences based on real-time atmospheric data. Results from a user evaluation show that participants found the system effective in building knowledge and easy to use, indicating the potential of integrating IoT and Generative AI for enhancing educational engagement and climate awareness."
  },
  {
    "title": "Morae: Proactively Pausing UI Agents for User Choices",
    "abstract": "User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences.",
    "url": "https://arxiv.org/abs/2508.21456",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Morae, a UI agent that proactively pauses during task execution to allow blind and low-vision users to make choices, addressing the issue of reduced user agency with current UI agents. Morae uses multimodal models to interpret user queries and prompt users for clarification when choices are to be made, leading to better task completion and option selection that aligns with user preferences. This work demonstrates the importance of a mixed-initiative approach in AI research, where users can benefit from automation while still being able to express their preferences."
  },
  {
    "title": "Conflict in Community-Based Design: A Case Study of a Relationship Breakdown",
    "abstract": "Community-based design efforts rightly seek to reduce the power differences between researchers and community participants by aligning with community values and furthering their priorities. However, what should designers do when key community members' practices seem to enact an oppressive and harmful structure? We reflect on our two-year-long engagement with a non-profit organization in southern India that supports women subjected to domestic abuse or facing mental health crises. We highlight the organizational gaps in knowledge management and transfer, which became an avenue for our design intervention. During design, we encountered practices that upheld caste hierarchies. These practices were expected to be incorporated into our technology. Anticipating harms to indirect stakeholders, we resisted this incorporation. It led to a breakdown in our relationship with the partner organization. Reflecting on this experience, we outline pluralistic pathways that community-based designers might inhabit when navigating value conflicts. These include making space for reflection before and during engagements, strategically repositioning through role reframing or appreciative inquiry, and exiting the engagement if necessary.",
    "url": "https://arxiv.org/abs/2508.21308",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the challenges of community-based design when faced with conflicting values and practices within partner organizations. The study highlights the importance of addressing power dynamics and oppressive structures within community engagement projects. The findings suggest the need for designers to reflect on their roles, strategically reposition themselves, and be willing to exit engagements when necessary to navigate value conflicts effectively."
  },
  {
    "title": "Design and evaluation of a serious game in virtual reality to increase empathy towards students with phonological dyslexia",
    "abstract": "Dyslexia is a neurodevelopmental disorder estimated to strike approximately 5 to 10 per cent of the population. In particular, phonological dyslexia causes problems in connecting the sounds of words with their written forms. Consequently, affected individuals may encounter issues such as slow reading speed, inaccurate reading, and difficulty decoding unfamiliar words. To address these complexities, the use of compensatory tools and strategies is essential to ensure equitable opportunities for dyslexic students. However, the general underestimation of the issue and lack of awareness regarding the significance of support methodologies pose significant obstacles. One of the ways to enhance consciousness towards a certain issue is by stimulating empathy with whom is affected by it. In light of this, this study introduces a serious game in virtual reality, targeted at educators, students, and, in general, at the non-dyslexic community. The game seeks to enhance understanding of the challenges that individuals with dyslexia experience daily, highlighting the relevance of supportive measures. This approach encourages players to empathize with the struggles of dyslexic individuals and to learn firsthand the importance of supportive methodologies. The final version of the experience was tested by 101 participants and evaluated through a specific collection of questionnaires validated in the literature. The results show that using the proposed virtual reality tool to promote empathy for individuals with phonological dyslexia is highly effective, leading to an average 20 per cent increase in participants' empathy after playing the game.",
    "url": "https://arxiv.org/abs/2508.21283",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on addressing the challenges faced by individuals with phonological dyslexia through the use of a serious game in virtual reality to increase empathy among educators, students, and the general community. The study found that the game was highly effective in enhancing understanding and empathy towards individuals with dyslexia, leading to a significant increase in empathy among participants. This highlights the importance of using innovative tools, such as virtual reality games, to promote awareness and support for individuals with dyslexia."
  },
  {
    "title": "Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses",
    "abstract": "This paper presents two studies on how Brazilian children (ages 9--11) use conversational agents (CAs) for schoolwork, discovery, and entertainment, and how structured scaffolds can enhance these interactions. In Study 1, a seven-week online investigation with 23 participants (children, parents, teachers) employed interviews, observations, and Cognitive Work Analysis to map children's information-processing flows, the role of more knowledgeable others, functional uses, contextual goals, and interaction patterns to inform conversation-tree design. We identified three CA functions: School, Discovery, Entertainment, and derived ``recipe'' scaffolds mirroring parent-child support. In Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges, comparing conversation-tree recipes based on structured-prompting to an unstructured baseline. Quantitative evaluation of readability, question count/depth/diversity, and coherence revealed gains for the recipe approach. Building on these findings, we offer design recommendations: scaffolded conversation-trees, child-dedicated profiles for personalized context, and caregiver-curated content. Our contributions include the first CWA application with Brazilian children, an empirical framework of child-CA information flows, and an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective, scaffolded learning.",
    "url": "https://arxiv.org/abs/2508.21209",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores how Brazilian children use conversational agents (CAs) for various purposes and how structured scaffolds can improve these interactions. The study involved mapping children's information-processing flows and interaction patterns to inform the design of conversation-tree recipes. The findings suggest that using structured-prompting in conversation-tree design can enhance readability, question depth, and coherence in child-CA interactions, providing valuable insights for designing smarter conversational agents for kids."
  },
  {
    "title": "Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?",
    "abstract": "This study proposes a framework that employs personality prompting with Large Language Models to generate verbal and nonverbal behaviors for virtual agents based on personality traits. Focusing on extraversion, we evaluated the system in two scenarios: negotiation and ice breaking, using both introverted and extroverted agents. In Experiment 1, we conducted agent to agent simulations and performed linguistic analysis and personality classification to assess whether the LLM generated language reflected the intended traits and whether the corresponding nonverbal behaviors varied by personality. In Experiment 2, we carried out a user study to evaluate whether these personality aligned behaviors were consistent with their intended traits and perceptible to human observers. Our results show that LLMs can generate verbal and nonverbal behaviors that align with personality traits, and that users are able to recognize these traits through the agents' behaviors. This work underscores the potential of LLMs in shaping personality aligned virtual agents.",
    "url": "https://arxiv.org/abs/2508.21087",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores using Large Language Models (LLMs) to generate verbal and nonverbal behaviors for virtual agents based on personality traits, specifically focusing on extraversion. The research found that LLMs can successfully generate behaviors that align with personality traits, and that users are able to recognize these traits through the agents' behaviors. This research highlights the potential of LLMs in creating virtual agents with personality-aligned behaviors."
  },
  {
    "title": "Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks",
    "abstract": "As Large Language Models (LLMs) increasingly integrate into everyday workflows, where users shape outcomes through multi-turn collaboration, a critical question emerges: do users with different personality traits systematically prefer certain LLMs over others? We conducted a study with 32 participants evenly distributed across four Keirsey personality types, evaluating their interactions with GPT-4 and Claude 3.5 across four collaborative tasks: data analysis, creative writing, information retrieval, and writing assistance. Results revealed significant personality-driven preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented tasks, while idealists favored Claude 3.5, especially for creative and analytical tasks. Other personality types showed task-dependent preferences. Sentiment analysis of qualitative feedback confirmed these patterns. Notably, aggregate helpfulness ratings were similar across models, showing how personality-based analysis reveals LLM differences that traditional evaluations miss.",
    "url": "https://arxiv.org/abs/2508.21628",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores how users with different personality traits prefer certain Large Language Models (LLMs) over others in collaborative tasks. Results show that Rationals preferred GPT-4 for goal-oriented tasks, while idealists favored Claude 3.5 for creative and analytical tasks. Understanding these personality-driven preferences can provide valuable insights for integrating LLMs into everyday workflows."
  },
  {
    "title": "Zero-Shot KWS for Children's Speech using Layer-Wise Features from SSL Models",
    "abstract": "Numerous methods have been proposed to enhance Keyword Spotting (KWS) in adult speech, but children's speech presents unique challenges for KWS systems due to its distinct acoustic and linguistic characteristics. This paper introduces a zero-shot KWS approach that leverages state-of-the-art self-supervised learning (SSL) models, including Wav2Vec2, HuBERT and Data2Vec. Features are extracted layer-wise from these SSL models and used to train a Kaldi-based DNN KWS system. The WSJCAM0 adult speech dataset was used for training, while the PFSTAR children's speech dataset was used for testing, demonstrating the zero-shot capability of our method. Our approach achieved state-of-the-art results across all keyword sets for children's speech. Notably, the Wav2Vec2 model, particularly layer 22, performed the best, delivering an ATWV score of 0.691, a MTWV score of 0.7003 and probability of false alarm and probability of miss of 0.0164 and 0.0547 respectively, for a set of 30 keywords. Furthermore, age-specific performance evaluation confirmed the system's effectiveness across different age groups of children. To assess the system's robustness against noise, additional experiments were conducted using the best-performing layer of the best-performing Wav2Vec2 model. The results demonstrated a significant improvement over traditional MFCC-based baseline, emphasizing the potential of SSL embeddings even in noisy conditions. To further generalize the KWS framework, the experiments were repeated for an additional CMU dataset. Overall the results highlight the significant contribution of SSL features in enhancing Zero-Shot KWS performance for children's speech, effectively addressing the challenges associated with the distinct characteristics of child speakers.",
    "url": "https://arxiv.org/abs/2508.21248",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a zero-shot Keyword Spotting (KWS) approach for children's speech using features extracted from state-of-the-art self-supervised learning (SSL) models. The study demonstrates that leveraging SSL models, particularly Wav2Vec2, can significantly improve KWS performance for children's speech, achieving state-of-the-art results across all keyword sets. The findings suggest that SSL features can enhance KWS systems for children's speech, addressing the unique challenges posed by the distinct characteristics of child speakers."
  },
  {
    "title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models",
    "abstract": "As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance.",
    "url": "https://arxiv.org/abs/2508.21061",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces OnGoal, an interface for tracking and visualizing conversational goals in multi-turn dialogues with large language models (LLMs). Through a study with 20 participants, it was found that using OnGoal resulted in participants spending less time and effort to achieve their goals, exploring new prompting strategies, and enhancing engagement and resilience in LLM dialogues. The findings suggest that tracking and visualizing goals can improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to enhance LLM performance in future chat interfaces."
  },
  {
    "title": "Understanding, Protecting, and Augmenting Human Cognition with Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop",
    "abstract": "Generative AI (GenAI) radically expands the scope and capability of automation for work, education, and everyday tasks, a transformation posing both risks and opportunities for human cognition. How will human cognition change, and what opportunities are there for GenAI to augment it? Which theories, metrics, and other tools are needed to address these questions? The CHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of how the use of GenAI affects human thought, from metacognition to critical thinking, memory, and creativity, with an emerging design practice for building GenAI tools that both protect and augment human thought. Fifty-six researchers, designers, and thinkers from across disciplines as well as industry and academia, along with 34 papers and portfolios, seeded a day of discussion, ideation, and community-building. We synthesize this material here to begin mapping the space of research and design opportunities and to catalyze a multidisciplinary community around this pressing area of research.",
    "url": "https://arxiv.org/abs/2508.21036",
    "journal": "arXiv cs.HC",
    "ai_summary": "The abstract discusses the impact of Generative AI (GenAI) on human cognition and the need for tools to understand, protect, and augment human thought. The CHI 2025 workshop on Tools for Thought brought together researchers and designers to explore how GenAI affects areas such as metacognition, critical thinking, memory, and creativity. The synthesis of the workshop's discussions and papers aims to spark further research and design opportunities in this important field."
  },
  {
    "title": "Schema-Guided Response Generation using Multi-Frame Dialogue State for Motivational Interviewing Systems",
    "abstract": "The primary goal of Motivational Interviewing (MI) is to help clients build their own motivation for behavioral change. To support this in dialogue systems, it is essential to guide large language models (LLMs) to generate counselor responses aligned with MI principles. By employing a schema-guided approach, this study proposes a method for updating multi-frame dialogue states and a strategy decision mechanism that dynamically determines the response focus in a manner grounded in MI principles. The proposed method was implemented in a dialogue system and evaluated through a user study. Results showed that the proposed system successfully generated MI-favorable responses and effectively encouraged the user's (client's) deliberation by asking eliciting questions.",
    "url": "https://arxiv.org/abs/2508.20635",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on using a schema-guided approach to help large language models generate counselor responses aligned with Motivational Interviewing (MI) principles. The study proposes a method for updating multi-frame dialogue states and a strategy decision mechanism to determine response focus in line with MI principles. Results from a user study show that the proposed system successfully generated MI-favorable responses and effectively encouraged user deliberation through eliciting questions."
  },
  {
    "title": "Persode: Personalized Visual Journaling with Episodic Memory-Aware AI Agent",
    "abstract": "Reflective journaling often lacks personalization and fails to engage Generation Alpha and Z, who prefer visually immersive and fast-paced interactions over traditional text-heavy methods. Visual storytelling enhances emotional recall and offers an engaging way to process personal expe- riences. Designed with these digital-native generations in mind, this paper introduces Persode, a journaling system that integrates personalized onboarding, memory-aware conversational agents, and automated visual storytelling. Persode captures user demographics and stylistic preferences through a tailored onboarding process, ensuring outputs resonate with individual identities. Using a Retrieval-Augmented Generation (RAG) framework, it prioritizes emotionally significant memories to provide meaningful, context-rich interactions. Additionally, Persode dynamically transforms user experiences into visually engaging narratives by generating prompts for advanced text-to-image models, adapting characters, backgrounds, and styles to user preferences. By addressing the need for personalization, visual engagement, and responsiveness, Persode bridges the gap between traditional journaling and the evolving preferences of Gen Alpha and Z.",
    "url": "https://arxiv.org/abs/2508.20585",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Persode, a personalized visual journaling system that caters to the preferences of Generation Alpha and Z by integrating personalized onboarding, memory-aware conversational agents, and automated visual storytelling. By capturing user demographics and stylistic preferences, Persode ensures that the outputs resonate with individual identities. The system prioritizes emotionally significant memories and dynamically transforms user experiences into visually engaging narratives, bridging the gap between traditional journaling methods and the evolving preferences of younger generations."
  },
  {
    "title": "VisiTrail: A Cognitive Visualization Tool for Time-Series Analysis of Eye Tracking Data from Attention Game",
    "abstract": "Eye Tracking (ET) can help to understand visual attention and cognitive processes in interactive environments. In attention tasks, distinguishing between relevant target objects and distractors is crucial for effective performance, yet the underlying gaze patterns that drive successful task completion remain incompletely understood. Traditional gaze analyses lack comprehensive insights into the temporal dynamics of attention allocation and the relationship between gaze behavior and task performance. When applied to complex visual search scenarios, current gaze analysis methods face several limitations, including the isolation of measurements, visual stability, search efficiency, and the decision-making processes involved in these scenarios. This paper proposes an analysis tool that considers time series for eye tracking data from task performance and also gaze measures (fixations, saccades and smooth pursuit); temporal pattern analysis that reveals how attention evolves throughout task performance; object-click sequence tracking that directly links visual attention to user actions; and performance metrics that quantify both accuracy and efficiency. This tool provides comprehensive visualization techniques that make complex patterns of stimuli and gaze connections interpretable.",
    "url": "https://arxiv.org/abs/2508.20522",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces VisiTrail, a cognitive visualization tool for analyzing eye tracking data from attention tasks. The tool considers time series data, gaze measures, temporal patterns, object-click sequences, and performance metrics to provide a comprehensive understanding of visual attention and task performance. VisiTrail offers insights into the temporal dynamics of attention allocation and the relationship between gaze behavior and task completion, making complex patterns of stimuli and gaze connections interpretable."
  },
  {
    "title": "What is \"Spatial\" about Spatial Computing?",
    "abstract": "Recent advancements in geographic information systems and mixed reality technologies have positioned spatial computing as a transformative paradigm in computational science. However, the field remains conceptually fragmented, with diverse interpretations across disciplines like Human-Computer Interaction, Geographic Information Science, and Computer Science, which hinders a comprehensive understanding of spatial computing and poses challenges for its coherent advancement and interdisciplinary integration. In this paper, we trace the origins and historical evolution of spatial computing and examine how \"spatial\" is understood, identifying two schools of thought: \"spatial\" as the contextual understanding of space, where spatial data guides interaction in the physical world; and \"spatial\" as a mixed space for interaction, emphasizing the seamless integration of physical and digital environments to enable embodied engagement. By synthesizing these perspectives, we propose spatial computing as a computational paradigm that redefines the interplay between environment, computation, and human experience, offering a holistic lens to enhance its conceptual clarity and inspire future technological innovations that support meaningful interactions with and shaping of environments.",
    "url": "https://arxiv.org/abs/2508.20477",
    "journal": "arXiv cs.HC",
    "ai_summary": "The paper explores the concept of spatial computing, which is seen as a transformative paradigm in computational science due to advancements in geographic information systems and mixed reality technologies. The authors identify two key interpretations of \"spatial\" - one focusing on contextual understanding of space and the other on the integration of physical and digital environments for embodied engagement. By synthesizing these perspectives, the paper proposes spatial computing as a computational paradigm that redefines the interplay between environment, computation, and human experience, offering a holistic lens for future technological innovations."
  },
  {
    "title": "Human-Centered Design for Connected Automation: Predicting Pedestrian Crossing Intentions",
    "abstract": "Road traffic remains a leading cause of death worldwide, with pedestrians and other vulnerable road users accounting for over half of the 1.19 million annual fatalities, much of it due to human error. Level-5 automated driving systems (ADSs), capable of full self-driving without human oversight, have the potential to reduce these incidents. However, their effectiveness depends not only on automation performance but also on their ability to communicate intent and coordinate safely with pedestrians in the absence of traditional driver cues. Understanding how pedestrians interpret and respond to ADS behavior is therefore critical to the development of connected vehicle systems. This study extends the Theory of Planned Behavior (TPB) by incorporating four external factors (i.e. safety, trust, compatibility, and understanding) to model pedestrian decision-making in road-crossing scenarios involving level-5 ADSs. Using data from an online survey (n = 212), results show that perceived behavioral control, attitude, and social information significantly predict pedestrians' crossing intentions. External factors, particularly perceived safety and understanding, strongly influence these constructs. Findings provide actionable insights for designing external human-machine interfaces (eHMIs) and cooperative V2X communication strategies that support safe, transparent interactions between automated vehicles and pedestrians. This work contributes to the development of inclusive, human-centered connected mobility systems.",
    "url": "https://arxiv.org/abs/2508.20464",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on the importance of understanding how pedestrians interpret and respond to automated driving systems (ADSs) in order to improve road safety. The study found that factors such as perceived safety and understanding strongly influence pedestrians' crossing intentions when interacting with level-5 ADSs. The findings offer insights for designing external human-machine interfaces and communication strategies to support safe interactions between automated vehicles and pedestrians, contributing to the development of inclusive, human-centered connected mobility systems."
  },
  {
    "title": "Identifying Framing Practices in Visualization Design Through Practitioner Reflections",
    "abstract": "Framing -- how designers define and reinterpret problems, shape narratives, and guide audience understanding -- is central to design practice. Yet in visualization research, framing has been examined mostly through its rhetorical and perceptual effects on audiences, leaving its role in the design process underexplored. This study addresses that gap by analyzing publicly available podcasts and book chapters in which over 80 professional visualization designers reflect on their work. We find that framing is a pervasive, iterative activity, evident in scoping problems, interpreting data, aligning with stakeholder goals, and shaping narrative direction. Our analysis identifies the conditions that trigger reframing and the strategies practitioners use to navigate uncertainty and guide design. These findings position framing as a core dimension of visualization practice and underscore the need for research and education to support the interpretive and strategic judgment that practitioners exercise throughout the design process.",
    "url": "https://arxiv.org/abs/2508.20383",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores the role of framing in visualization design by analyzing reflections from over 80 professional designers. The findings reveal that framing is a pervasive and iterative activity that influences problem scoping, data interpretation, stakeholder alignment, and narrative direction. Understanding framing as a core dimension of visualization practice highlights the importance of supporting practitioners in exercising interpretive and strategic judgment throughout the design process."
  },
  {
    "title": "Athena: Intermediate Representations for Iterative Scaffolded App Generation with an LLM",
    "abstract": "It is challenging to generate the code for a complete user interface using a Large Language Model (LLM). User interfaces are complex and their implementations often consist of multiple, inter-related files that together specify the contents of each screen, the navigation flows between the screens, and the data model used throughout the application. It is challenging to craft a single prompt for an LLM that contains enough detail to generate a complete user interface, and even then the result is frequently a single large and difficult to understand file that contains all of the generated screens. In this paper, we introduce Athena, a prototype application generation environment that demonstrates how the use of shared intermediate representations, including an app storyboard, data model, and GUI skeletons, can help a developer work with an LLM in an iterative fashion to craft a complete user interface. These intermediate representations also scaffold the LLM's code generation process, producing organized and structured code in multiple files while limiting errors. We evaluated Athena with a user study that found 75% of participants preferred our prototype over a typical chatbot-style baseline for prototyping apps.",
    "url": "https://arxiv.org/abs/2508.20263",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Athena, a prototype application generation environment that utilizes shared intermediate representations to help developers work with a Large Language Model (LLM) in creating complete user interfaces. By using an app storyboard, data model, and GUI skeletons, developers can iteratively craft organized and structured code in multiple files while reducing errors. A user study showed that 75% of participants preferred Athena over a typical chatbot-style baseline for prototyping apps, highlighting the significance of this approach in improving the code generation process for user interfaces."
  },
  {
    "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering",
    "abstract": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular framework that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating high-quality causal chains from existing datasets using large language models. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: this https URL",
    "url": "https://arxiv.org/abs/2508.21010",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a novel framework for Causal-Why Video Question Answering that decouples causal reasoning from answer generation, using natural language causal chains as intermediate representations. The proposed two-stage architecture outperforms existing models in terms of explainability, user trust, and generalization, while also introducing a new evaluation metric for causality-oriented captioning. The approach not only improves performance on large-scale benchmarks but also demonstrates the potential for the Causal Chain Extractor to be a reusable causal reasoning engine in various domains."
  },
  {
    "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents",
    "abstract": "Proactive dialogue has emerged as a critical and challenging research problem in advancing large language models (LLMs). Existing works predominantly focus on domain-specific or task-oriented scenarios, which leads to fragmented evaluations and limits the comprehensive exploration of models' proactive conversation abilities. In this work, we propose ProactiveEval, a unified framework designed for evaluating proactive dialogue capabilities of LLMs. This framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics across various domains. Moreover, it also enables the automatic generation of diverse and challenging evaluation data. Based on the proposed framework, we develop 328 evaluation environments spanning 6 distinct domains. Through experiments with 22 different types of LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional performance on target planning and dialogue guidance tasks, respectively. Finally, we investigate how reasoning capabilities influence proactive behaviors and discuss their implications for future model development.",
    "url": "https://arxiv.org/abs/2508.20973",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces ProactiveEval, a unified evaluation framework for assessing the proactive dialogue capabilities of large language models (LLMs). By decomposing proactive dialogue into target planning and dialogue guidance tasks, the framework allows for comprehensive evaluation across various domains. The study demonstrates the effectiveness of ProactiveEval by testing 22 different LLMs in 328 evaluation environments, highlighting the exceptional performance of DeepSeek-R1 and Claude-3.7-Sonnet. The findings shed light on the importance of reasoning capabilities in proactive behaviors and offer insights for future model development in this area."
  },
  {
    "title": "MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models",
    "abstract": "Recent advances in medical vision-language models (VLMs) open up remarkable opportunities for clinical applications such as automated report generation, copilots for physicians, and uncertainty quantification. However, despite their promise, medical VLMs introduce serious security concerns, most notably risks of Protected Health Information (PHI) exposure, data leakage, and vulnerability to cyberthreats - which are especially critical in hospital environments. Even when adopted for research or non-clinical purposes, healthcare organizations must exercise caution and implement safeguards. To address these challenges, we present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1) enables physicians to manually select and use different models without programming expertise, (2) supports engineers in efficiently deploying medical VLMs in a plug-and-play fashion, with seamless integration of Hugging Face open-source models, and (3) ensures privacy-preserving inference through Docker-orchestrated, operating system agnostic deployment. MedFoundationHub requires only an offline local workstation equipped with a single NVIDIA A6000 GPU, making it both secure and accessible within the typical resources of academic research labs. To evaluate current capabilities, we engaged board-certified pathologists to deploy and assess five state-of-the-art VLMs (Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases, yielding 1015 clinician-model scoring events. These assessments revealed recurring limitations, including off-target answers, vague reasoning, and inconsistent pathology terminology.",
    "url": "https://arxiv.org/abs/2508.20345",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces a toolkit called MedFoundationHub that allows for the deployment of medical vision-language models (VLMs) in a secure and user-friendly manner, addressing concerns of PHI exposure and cyber threats in hospital environments. The toolkit enables physicians to select and use different models without programming expertise, supports engineers in deploying VLMs efficiently, and ensures privacy-preserving inference through Docker-orchestrated deployment. Evaluation of five state-of-the-art VLMs by board-certified pathologists revealed limitations such as off-target answers and inconsistent pathology terminology, highlighting the need for further refinement in these models for clinical applications."
  },
  {
    "title": "The Mathematician's Assistant: Integrating AI into Research Practice",
    "abstract": "The rapid development of artificial intelligence (AI), marked by breakthroughs like 'AlphaEvolve' and 'Gemini Deep Think', is beginning to offer powerful new tools that have the potential to significantly alter the research practice in many areas of mathematics. This paper explores the current landscape of publicly accessible large language models (LLMs) in a mathematical research context, based on developments up to August 2, 2025. Our analysis of recent benchmarks, such as MathArena and the Open Proof Corpus (Balunović et al., 2025; Dekoninck et al., 2025), reveals a complex duality: while state-of-the-art models demonstrate strong abilities in solving problems and evaluating proofs, they also exhibit systematic flaws, including a lack of self-critique and a model depending discrepancy between final-answer accuracy and full-proof validity.\nBased on these findings, we propose a durable framework for integrating AI into the research workflow, centered on the principle of the augmented mathematician. In this model, the AI functions as a copilot under the critical guidance of the human researcher, an approach distilled into five guiding principles for effective and responsible use. We then systematically explore seven fundamental ways AI can be applied across the research lifecycle, from creativity and ideation to the final writing process, demonstrating how these principles translate into concrete practice.\nWe conclude that the primary role of AI is currently augmentation rather than automation. This requires a new skill set focused on strategic prompting, critical verification, and methodological rigor in order to effectively use these powerful tools.",
    "url": "https://arxiv.org/abs/2508.20236",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper examines the integration of artificial intelligence (AI) into mathematical research, highlighting the potential of AI tools like large language models (LLMs) to impact research practices. The analysis of recent benchmarks shows that while state-of-the-art models excel in problem-solving and proof evaluation, they also exhibit flaws such as a lack of self-critique. The proposed framework suggests a collaborative approach between AI and human researchers, emphasizing the role of AI as a copilot rather than an autonomous entity, requiring new skills for effective and responsible use."
  },
  {
    "title": "The Anatomy of a Personal Health Agent",
    "abstract": "Health is a fundamental pillar of human wellness, and the rapid advancements in large language models (LLMs) have driven the development of a new generation of health agents. However, the application of health agents to fulfill the diverse needs of individuals in daily non-clinical settings is underexplored. In this work, we aim to build a comprehensive personal health agent that is able to reason about multimodal data from everyday consumer wellness devices and common personal health records, and provide personalized health recommendations. To understand end-users' needs when interacting with such an assistant, we conducted an in-depth analysis of web search and health forum queries, alongside qualitative insights from users and health experts gathered through a user-centered design process. Based on these findings, we identified three major categories of consumer health needs, each of which is supported by a specialist sub-agent: (1) a data science agent that analyzes personal time-series wearable and health record data, (2) a health domain expert agent that integrates users' health and contextual data to generate accurate, personalized insights, and (3) a health coach agent that synthesizes data insights, guiding users using a specified psychological strategy and tracking users' progress. Furthermore, we propose and develop the Personal Health Agent (PHA), a multi-agent framework that enables dynamic, personalized interactions to address individual health needs. To evaluate each sub-agent and the multi-agent system, we conducted automated and human evaluations across 10 benchmark tasks, involving more than 7,000 annotations and 1,100 hours of effort from health experts and end-users. Our work represents the most comprehensive evaluation of a health agent to date and establishes a strong foundation towards the futuristic vision of a personal health agent accessible to everyone.",
    "url": "https://arxiv.org/abs/2508.20148",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on developing a comprehensive personal health agent that can analyze data from consumer wellness devices and personal health records to provide personalized health recommendations. The study identified three major categories of consumer health needs and developed specialist sub-agents to address each category. The evaluation of the Personal Health Agent (PHA) involved extensive testing and input from health experts and end-users, representing a significant step towards making a personal health agent accessible to everyone."
  },
  {
    "title": "Is the medical image segmentation problem solved? A survey of current developments and future directions",
    "abstract": "Medical image segmentation has advanced rapidly over the past two decades, largely driven by deep learning, which has enabled accurate and efficient delineation of cells, tissues, organs, and pathologies across diverse imaging modalities. This progress raises a fundamental question: to what extent have current models overcome persistent challenges, and what gaps remain? In this work, we provide an in-depth review of medical image segmentation, tracing its progress and key developments over the past decade. We examine core principles, including multiscale analysis, attention mechanisms, and the integration of prior knowledge, across the encoder, bottleneck, skip connections, and decoder components of segmentation networks. Our discussion is organized around seven key dimensions: (1) the shift from supervised to semi-/unsupervised learning, (2) the transition from organ segmentation to lesion-focused tasks, (3) advances in multi-modality integration and domain adaptation, (4) the role of foundation models and transfer learning, (5) the move from deterministic to probabilistic segmentation, (6) the progression from 2D to 3D and 4D segmentation, and (7) the trend from model invocation to segmentation agents. Together, these perspectives provide a holistic overview of the trajectory of deep learning-based medical image segmentation and aim to inspire future innovation. To support ongoing research, we maintain a continually updated repository of relevant literature and open-source resources at this https URL",
    "url": "https://arxiv.org/abs/2508.20139",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores the advancements in medical image segmentation driven by deep learning, highlighting the progress in accurately delineating cells, tissues, organs, and pathologies across various imaging modalities. The study examines key developments over the past decade, including the shift towards semi-/unsupervised learning, focus on lesion segmentation, multi-modality integration, probabilistic segmentation, and the transition to 3D and 4D segmentation. The findings provide a comprehensive overview of the current state of deep learning-based medical image segmentation and offer insights for future innovation in the field."
  },
  {
    "title": "FlyMeThrough: Human-AI Collaborative 3D Indoor Mapping with Commodity Drones",
    "abstract": "Indoor mapping data is crucial for routing, navigation, and building management, yet such data are widely lacking due to the manual labor and expense of data collection, especially for larger indoor spaces. Leveraging recent advancements in commodity drones and photogrammetry, we introduce FlyMeThrough -- a drone-based indoor scanning system that efficiently produces 3D reconstructions of indoor spaces with human-AI collaborative annotations for key indoor points-of-interest (POI) such as entrances, restrooms, stairs, and elevators. We evaluated FlyMeThrough in 12 indoor spaces with varying sizes and functionality. To investigate use cases and solicit feedback from target stakeholders, we also conducted a qualitative user study with five building managers and five occupants. Our findings indicate that FlyMeThrough can efficiently and precisely create indoor 3D maps for strategic space planning, resource management, and navigation.",
    "url": "https://arxiv.org/abs/2508.20034",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces FlyMeThrough, a drone-based indoor scanning system that efficiently produces 3D reconstructions of indoor spaces with human-AI collaborative annotations for key points-of-interest. The system was evaluated in 12 indoor spaces and found to be effective for strategic space planning, resource management, and navigation, offering a cost-effective solution for indoor mapping data collection."
  },
  {
    "title": "CapTune: Adapting Non-Speech Captions With Anchored Generative Models",
    "abstract": "Non-speech captions are essential to the video experience of deaf and hard of hearing (DHH) viewers, yet conventional approaches often overlook the diversity of their preferences. We present CapTune, a system that enables customization of non-speech captions based on DHH viewers' needs while preserving creator intent. CapTune allows caption authors to define safe transformation spaces using concrete examples and empowers viewers to personalize captions across four dimensions: level of detail, expressiveness, sound representation method, and genre alignment. Evaluations with seven caption creators and twelve DHH participants showed that CapTune supported creators' creative control while enhancing viewers' emotional engagement with content. Our findings also reveal trade-offs between information richness and cognitive load, tensions between interpretive and descriptive representations of sound, and the context-dependent nature of caption preferences.",
    "url": "https://arxiv.org/abs/2508.19971",
    "journal": "arXiv cs.HC",
    "ai_summary": "CapTune is a system designed to customize non-speech captions for deaf and hard of hearing viewers, taking into account their diverse preferences while maintaining the creator's original intent. The system allows for personalization across multiple dimensions, such as level of detail, expressiveness, sound representation method, and genre alignment. Evaluations with caption creators and DHH participants showed that CapTune supported creators' creative control and enhanced viewers' emotional engagement with content, highlighting important trade-offs and tensions in caption preferences."
  },
  {
    "title": "Socially Interactive Agents for Preserving and Transferring Tacit Knowledge in Organizations",
    "abstract": "This paper introduces a novel approach to tackle the challenges of preserving and transferring tacit knowledge--deep, experience-based insights that are hard to articulate but vital for decision-making, innovation, and problem-solving. Traditional methods rely heavily on human facilitators, which, while effective, are resource-intensive and lack scalability. A promising alternative is the use of Socially Interactive Agents (SIAs) as AI-driven knowledge transfer facilitators. These agents interact autonomously and socially intelligently with users through multimodal behaviors (verbal, paraverbal, nonverbal), simulating expert roles in various organizational contexts. SIAs engage employees in empathic, natural-language dialogues, helping them externalize insights that might otherwise remain unspoken. Their success hinges on building trust, as employees are often hesitant to share tacit knowledge without assurance of confidentiality and appreciation. Key technologies include Large Language Models (LLMs) for generating context-relevant dialogue, Retrieval-Augmented Generation (RAG) to integrate organizational knowledge, and Chain-of-Thought (CoT) prompting to guide structured reflection. These enable SIAs to actively elicit knowledge, uncover implicit assumptions, and connect insights to broader organizational contexts. Potential applications span onboarding, where SIAs support personalized guidance and introductions, and knowledge retention, where they conduct structured interviews with retiring experts to capture heuristics behind decisions. Success depends on addressing ethical and operational challenges such as data privacy, algorithmic bias, and resistance to AI. Transparency, robust validation, and a culture of trust are essential to mitigate these risks.",
    "url": "https://arxiv.org/abs/2508.19942",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores the use of Socially Interactive Agents (SIAs) as AI-driven facilitators for preserving and transferring tacit knowledge within organizations. SIAs engage employees in natural-language dialogues to help externalize important insights, using technologies such as Large Language Models (LLMs) and Chain-of-Thought (CoT) prompting. The success of SIAs depends on building trust with employees and addressing ethical and operational challenges such as data privacy and algorithmic bias."
  },
  {
    "title": "Lessons from Biophilic Design: Rethinking Affective Interaction Design in Built Environments",
    "abstract": "The perspectives of affective interaction in built environments are largely overlooked and instead dominated by affective computing approaches that view emotions as \"static\", computable states to be detected and regulated. To address this limitation, we interviewed architects to explore how biophilic design -- our deep-rooted emotional connection with nature -- could shape affective interaction design in smart buildings. Our findings reveal that natural environments facilitate self-directed emotional experiences through spatial diversity, embodied friction, and porous sensory exchanges. Based on this, we introduce three design principles for discussion at the Affective Interaction workshop: (1) Diversity of Spatial Experiences, (2) Self-Reflection Through Complexity & Friction, and (3) Permeability & Sensory Exchange with the Outside World, while also examining the challenges of integrating these perspectives into built environments.",
    "url": "https://arxiv.org/abs/2508.19867",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the concept of affective interaction design in built environments, highlighting the importance of incorporating biophilic design principles to create emotionally engaging spaces. The study found that natural environments can enhance emotional experiences through spatial diversity, embodied friction, and sensory exchanges. The researchers propose three design principles to guide the integration of biophilic design into smart buildings, emphasizing the need to rethink traditional approaches to affective computing in order to create more emotionally resonant environments."
  },
  {
    "title": "Towards a Real-Time Warning System for Detecting Inaccuracies in Photoplethysmography-Based Heart Rate Measurements in Wearable Devices",
    "abstract": "Wearable devices with photoplethysmography (PPG) sensors are widely used to monitor heart rate (HR), yet often suffer from accuracy issues. However, users typically do not receive an indication of potential measurement errors. We present a real-time warning system that detects and communicates inaccuracies in PPG-derived HR, aiming to enhance transparency and trust. Using data from Polar and Garmin devices, we trained a deep learning model to classify HR accuracy using only the derived HR signal. The system detected over 80% of inaccurate readings. By providing interpretable, real-time feedback directly to users, our work contributes to HCI by promoting user awareness, informed decision-making, and trust in wearable health technology.",
    "url": "https://arxiv.org/abs/2508.19818",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on developing a real-time warning system for detecting inaccuracies in heart rate measurements obtained from wearable devices with photoplethysmography sensors. The study trained a deep learning model using data from Polar and Garmin devices to classify HR accuracy, successfully detecting over 80% of inaccurate readings. By providing users with real-time feedback on potential measurement errors, this system aims to enhance transparency, user awareness, and trust in wearable health technology."
  },
  {
    "title": "Burst: Collaborative Curation in Connected Social Media Communities",
    "abstract": "Positive social interactions can occur in groups of many shapes and sizes, spanning from small and private to large and open. However, social media tends to binarize our experiences into either isolated small groups or into large public squares. In this paper, we introduce Burst, a social media design that allows users to share and curate content between many spaces of varied size and composition. Users initially post content to small trusted groups, who can then burst that content, routing it to the groups that would be the best audience. We instantiate this approach into a mobile phone application, and demonstrate through a ten-day field study (N=36) that Burst enabled a participatory curation culture. With this work, we aim to articulate potential new design directions for social media sharing.",
    "url": "https://arxiv.org/abs/2508.19768",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Burst, a social media design that allows users to share and curate content across various group sizes and compositions. Users can post content to small trusted groups, which can then be shared to larger audiences based on relevance. Through a field study, it was found that Burst promoted a participatory curation culture, suggesting new design directions for social media sharing."
  },
  {
    "title": "Attention is also needed for form design",
    "abstract": "Conventional product design is a cognitively demanding process, limited by its time-consuming nature, reliance on subjective expertise, and the opaque translation of inspiration into tangible concepts. This research introduces a novel, attention-aware framework that integrates two synergistic systems: EUPHORIA, an immersive Virtual Reality environment using eye-tracking to implicitly capture a designer's aesthetic preferences, and RETINA, an agentic AI pipeline that translates these implicit preferences into concrete design outputs. The foundational principles were validated in a two-part study. An initial study correlated user's implicit attention with explicit preference and the next one correlated mood to attention. A comparative study where 4 designers solved challenging design problems using 4 distinct workflows, from a manual process to an end-to-end automated pipeline, showed the integrated EUPHORIA-RETINA workflow was over 4 times more time-efficient than the conventional method. A panel of 50 design experts evaluated the 16 final renderings. Designs generated by the fully automated system consistently received the highest Worthiness (calculated by an inverse Plackett-Luce model based on gradient descent optimization) and Design Effectiveness scores, indicating superior quality across 8 criteria: novelty, visual appeal, emotional resonance, clarity of purpose, distinctiveness of silhouette, implied materiality, proportional balance, & adherence to the brief. This research presents a validated paradigm shift from traditional Computer-Assisted Design (CAD) to a collaborative model of Designer-Assisting Computers (DAC). By automating logistical and skill-dependent generative tasks, the proposed framework elevates the designer's role to that of a creative director, synergizing human intuition with the generative power of agentic AI to produce higher-quality designs more efficiently.",
    "url": "https://arxiv.org/abs/2508.19708",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a novel framework that combines Virtual Reality and AI to streamline the product design process. The study validated the effectiveness of this integrated system, showing it to be over 4 times more time-efficient than traditional methods. Designs generated by the automated system were consistently rated higher in quality by design experts, indicating a paradigm shift towards collaborative Designer-Assisting Computers."
  },
  {
    "title": "Haptic Tracing: A new paradigm for spatialized Haptic rendering",
    "abstract": "Haptic technology enhances interactive experiences by providing force and tactile feedback, improving user performance and immersion. However, despite advancements, creating tactile experiences still remains challenging due to device diversity and complexity. Most available haptic frameworks rely on trigger-based or event-based systems, and disregard the information of the 3D scene to render haptic information. This paper introduces Haptic Tracing, a novel method for spatial haptic rendering that simplifies the creation of interactive haptic experiences without relying on physical simulations. It uses concepts from visual and audio rendering to model and propagate haptic information through a 3D scene. The paper also describes how our proposed haptic rendering method can be used to create a vibrotactile rendering system, enabling the creation of perceptually coherent and dynamic haptic interactions. Finally, the paper discusses a user study that explores the role of the haptic propagation and multi-actuator rendering on the users' haptic experience. The results show that our approach significantly enhances the realism and the expressivity of the haptic feedback, showcasing its potential for developing more complex and realistic haptic experiences.",
    "url": "https://arxiv.org/abs/2508.19703",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces Haptic Tracing, a new method for spatial haptic rendering that simplifies the creation of interactive haptic experiences without relying on physical simulations. By using concepts from visual and audio rendering, the method models and propagates haptic information through a 3D scene, leading to more realistic and dynamic haptic interactions. A user study showed that this approach significantly enhances the realism and expressivity of haptic feedback, demonstrating its potential for developing more complex and immersive haptic experiences."
  },
  {
    "title": "PersoNo: Personalised Notification Urgency Classifier in Mixed Reality",
    "abstract": "Mixed Reality (MR) is increasingly integrated into daily life, providing enhanced capabilities across various domains. However, users face growing notification streams that disrupt their immersive experience. We present PersoNo, a personalised notification urgency classifier for MR that intelligently classifies notifications based on individual user preferences. Through a user study (N=18), we created the first MR notification dataset containing both self-labelled and interaction-based data across activities with varying cognitive demands. Our thematic analysis revealed that, unlike in mobiles, the activity context is equally important as the content and the sender in determining notification urgency in MR. Leveraging these insights, we developed PersoNo using large language models that analyse users replying behaviour patterns. Our multi-agent approach achieved 81.5% accuracy and significantly reduced false negative rates (0.381) compared to baseline models. PersoNo has the potential not only to reduce unnecessary interruptions but also to offer users understanding and control of the system, adhering to Human-Centered Artificial Intelligence design principles.",
    "url": "https://arxiv.org/abs/2508.19622",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces PersoNo, a personalized notification urgency classifier for Mixed Reality (MR) that considers individual user preferences and activity context. Through a user study, the researchers found that activity context is crucial in determining notification urgency in MR. By leveraging insights from user behavior patterns, PersoNo achieved high accuracy in classifying notifications and reducing false negatives, ultimately aiming to reduce interruptions and provide users with more control over their MR experience."
  },
  {
    "title": "Orchid: Orchestrating Context Across Creative Workflows with Generative AI",
    "abstract": "Context is critical for meaningful interactions between people and Generative AI (GenAI). Yet mainstream tools offer limited means to orchestrate it, particularly across workflows that span multiple interactions, sessions, and models, as often occurs in creative projects. Re specifying prior details, juggling diverse artifacts, and dealing with context drift overwhelm users, obscure intent, and curtail creativity. To address these challenges, we present Orchid, a system that gives its users affordances to specify, reference, and monitor context throughout evolving workflows. Specifically, Orchid enables users to (1) specify context related to the project, themselves, and different styles, (2) reference these via explicit mentions, inline selection, or implicit grounding, and (3) monitor context assigned to different interactions across the workflow. In a within-subjects study (n=12), participants using Orchid to execute creative tasks (compared to a baseline toolkit of web search, LLM-based chat, and digital notebooks) produced more novel and feasible outcomes, reporting greater alignment between their intent and the AI's responses, higher perceived control, and increased transparency. By prioritizing context orchestration, Orchid offers an actionable step toward next generation GenAI tools that support complex, iterative workflows - enabling creators and AI to stay aligned and augment their creative potential.",
    "url": "https://arxiv.org/abs/2508.19517",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Orchid, a system designed to orchestrate context across creative workflows with Generative AI. Orchid allows users to specify, reference, and monitor context throughout evolving workflows, leading to more novel and feasible outcomes in creative tasks. Participants using Orchid reported greater alignment between their intent and the AI's responses, higher perceived control, and increased transparency, highlighting the significance of prioritizing context orchestration in next-generation GenAI tools."
  },
  {
    "title": "\"She was useful, but a bit too optimistic\": Augmenting Design with Interactive Virtual Personas",
    "abstract": "Personas have been widely used to understand and communicate user needs in human-centred design. Despite their utility, they may fail to meet the demands of iterative workflows due to their static nature, limited engagement, and inability to adapt to evolving design needs. Recent advances in large language models (LLMs) pave the way for more engaging and adaptive approaches to user representation. This paper introduces Interactive Virtual Personas (IVPs): multimodal, LLM-driven, conversational user simulations that designers can interview, brainstorm with, and gather feedback from in real time via voice interface. We conducted a qualitative study with eight professional UX designers, employing an IVP named \"Alice\" across three design activities: user research, ideation, and prototype evaluation. Our findings demonstrate the potential of IVPs to expedite information gathering, inspire design solutions, and provide rapid user-like feedback. However, designers raised concerns about biases, over-optimism, the challenge of ensuring authenticity without real stakeholder input, and the inability of the IVP to fully replicate the nuances of human interaction. Our participants emphasised that IVPs should be viewed as a complement to, not a replacement for, real user engagement. We discuss strategies for prompt engineering, human-in-the-loop integration, and ethical considerations for effective and responsible IVP use in design. Finally, our work contributes to the growing body of research on generative AI in the design process by providing insights into UX designers' experiences of LLM-powered interactive personas.",
    "url": "https://arxiv.org/abs/2508.19463",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces Interactive Virtual Personas (IVPs) as a more engaging and adaptive approach to user representation in design, using large language models to create conversational user simulations. The study with professional UX designers showed that IVPs can expedite information gathering, inspire design solutions, and provide rapid user-like feedback, but designers raised concerns about biases, over-optimism, and the inability to fully replicate human interaction nuances. The findings highlight the potential of IVPs as a complement to real user engagement in design, emphasizing the need for prompt engineering, human-in-the-loop integration, and ethical considerations for effective and responsible use."
  },
  {
    "title": "Exploring Paper as a Material: Plotting the Design Space of The Fabrication for Dynamic Paper-Based Interactions",
    "abstract": "We reviewed 43 papers to understand the fabrication of dynamic paper-based interactions. We used a design space to classify tool selection, technique choice, and exploration of paper as a material. We classified 9 dimensions for the design space, including 4 dimensions for tools (precision, accommodation, complexity, and availability), 3 dimensions for techniques (cutting techniques, folding techniques, and integration techniques), and 2 dimensions for paper as the material (paper weight and paper type). The patterns we observed in the design space indicate a majority use of high precision tools, high complexity tools, and surface integration techniques in previous practice. Meanwhile, printing and plain paper are the leading material choices. We analyze these patterns and suggest potential directions for future work. Our study helps researchers locate different fabrication approaches and instances, thus fostering innovation in the field of paper-based interaction.",
    "url": "https://arxiv.org/abs/2508.19407",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the fabrication of dynamic paper-based interactions by reviewing 43 papers and classifying tool selection, technique choice, and exploration of paper as a material into a design space with 9 dimensions. The study found that high precision tools, high complexity tools, and surface integration techniques were commonly used, with printing and plain paper being the leading material choices. The findings provide insights for future research and innovation in the field of paper-based interaction design."
  },
  {
    "title": "Improving Hypertension and Diabetes Outcomes with Digital Care Coordination and Remote Monitoring in Rural Health",
    "abstract": "Chronic illnesses are a global concern with essential hypertension and diabetes mellitus among the most common conditions. Remote patient monitoring has shown promising results on clinical and health outcomes. However, access to care and digital health solutions is limited among rural, lower-income, and older adult populations. This paper repots on a pre-post study of a comprehensive care coordination program including connected, wearable blood pressure and glucometer devices, tablets, and medical assistant-provided health coaching in a community health center in rural California. The participants (n=221) had a mean age of 54.6 years, were majority female, two-thirds spoke Spanish, 19.9% had hypertension, 49.8% diabetic, and 30.3% both conditions. Participants with hypertension achieved a mean reduction in systolic blood pressure of 20.24 (95% CI: 13.61, 26.87) at six months while those with diabetes achieved a mean reduction of 3.85 points (95% CI: 3.73, 4.88). These outcomes compare favorably to the small but growing body of evidence supporting digital care coordination and remote monitoring. These results also support the feasibility of well-designed digital health solutions yielding improved health outcomes among underserved communities.",
    "url": "https://arxiv.org/abs/2508.19378",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study focuses on the impact of a digital care coordination program on hypertension and diabetes outcomes in a rural community health center in California. The program, which included remote monitoring devices and health coaching, resulted in significant reductions in blood pressure and glucose levels among participants with hypertension and diabetes. These findings suggest that digital health solutions can effectively improve health outcomes in underserved populations, highlighting the potential for remote monitoring to address chronic illnesses in rural and lower-income communities."
  },
  {
    "title": "A Theory of Information, Variation, and Artificial Intelligence",
    "abstract": "A growing body of empirical work suggests that the widespread adoption of generative AI produces a significant homogenizing effect on information, creativity, and cultural production. I first develop a novel theoretical framework to explain this phenomenon. I argue that a dynamic of AI-derivative epistemology, in which individuals increasingly defer to AI outputs, allows a centralized AI Prism to function, a technical mechanism whose architecture is designed to reduce variance and converge on the statistical mean. This provides a causal explanation for the generative monocultures observed in recent studies. However, I contend this represents only the first stage of a more complex and dialectical process. This paper's central and paradoxical thesis is that the very homogenization that flattens knowledge within specialized domains simultaneously renders that knowledge into consistent modules that can be recombined across them, a process foundational to innovation and creativity. However, this recombinant potential is not automatic, but rather conditional. This paper argues that these opposing forces, homogenizing defaults versus recombinant possibilities, are governed by the nature of human engagement with the technology. The ultimate effect of generative AI is conditional on whether individuals act as passive consumers deferring to the AI's statistical outputs, or as active curators who critically interrogate, re-contextualize, and recombine them. The paper concludes by outlining the cognitive and institutional scaffolds required to resolve this tension, arguing they are the decisive variable that determine whether generative AI becomes an instrument of innovation or homogenization.",
    "url": "https://arxiv.org/abs/2508.19264",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the impact of generative AI on information, creativity, and cultural production, highlighting a trend towards homogenization. The author argues that while AI can lead to a flattening of knowledge within specialized domains, it also creates opportunities for recombination and innovation. The key finding is that the ultimate effect of generative AI depends on human engagement with the technology, with the potential for either innovation or homogenization based on whether individuals passively defer to AI outputs or actively engage with and recombine them."
  },
  {
    "title": "Floor sensors are cheap and easy to use! A Nihon Buyo Case Study",
    "abstract": "As floor-sensing technologies gain traction in movement research, questions remain about their usability and effectiveness for non-expert users. This study presents a case study evaluating Flexel, a modular, low-cost, high-resolution pressure-sensing floor interface, in the context of Nihon Buyo, a traditional Japanese dance. The system was installed, calibrated, and used by a first-time, non-technical user to track weight distribution patterns of a teacher and learner over nine weeks. Live pressure data was synchronized with video recordings, and custom software was developed to process and analyze the signal. Despite expectations that the learner's weight distribution would converge toward the teacher's over time, quantitative analyses revealed that the learner developed a consistent yet distinct movement profile. These findings suggest that even within rigid pedagogical structures, individual movement signatures can emerge. More importantly, the study demonstrates that Flexel can be deployed and operated effectively by non-expert users, highlighting its potential for broader adoption in education, performance, and embodied research.",
    "url": "https://arxiv.org/abs/2508.19261",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study evaluated the use of a low-cost floor-sensing technology, Flexel, in tracking weight distribution patterns in Nihon Buyo, a traditional Japanese dance. The system was found to be effective in capturing individual movement profiles, even within a structured learning environment. The study highlights the potential for Flexel to be used by non-expert users in education, performance, and research, showcasing its usability and effectiveness in movement analysis."
  },
  {
    "title": "Capabilities of GPT-5 across critical domains: Is it the next breakthrough?",
    "abstract": "The accelerated evolution of large language models has raised questions about their comparative performance across domains of practical importance. GPT-4 by OpenAI introduced advances in reasoning, multimodality, and task generalization, establishing itself as a valuable tool in education, clinical diagnosis, and academic writing, though it was accompanied by several flaws. Released in August 2025, GPT-5 incorporates a system-of-models architecture designed for task-specific optimization and, based on both anecdotal accounts and emerging evidence from the literature, demonstrates stronger performance than its predecessor in medical contexts. This study provides one of the first systematic comparisons of GPT-4 and GPT-5 using human raters from linguistics and clinical fields. Twenty experts evaluated model-generated outputs across five domains: lesson planning, assignment evaluation, clinical diagnosis, research generation, and ethical reasoning, based on predefined criteria. Mixed-effects models revealed that GPT-5 significantly outperformed GPT-4 in lesson planning, clinical diagnosis, research generation, and ethical reasoning, while both models performed comparably in assignment assessment. The findings highlight the potential of GPT-5 to serve as a context-sensitive and domain-specialized tool, offering tangible benefits for education, clinical practice, and academic research, while also advancing ethical reasoning. These results contribute to one of the earliest empirical evaluations of the evolving capabilities and practical promise of GPT-5.",
    "url": "https://arxiv.org/abs/2508.19259",
    "journal": "arXiv cs.HC",
    "ai_summary": "The study compares the performance of GPT-4 and GPT-5 in various domains, showing that GPT-5 outperforms its predecessor in lesson planning, clinical diagnosis, research generation, and ethical reasoning. This suggests that GPT-5 has potential as a specialized tool in education, clinical practice, and academic research, with advancements in ethical reasoning. These findings contribute to understanding the evolving capabilities of large language models like GPT-5 and their practical applications."
  },
  {
    "title": "Emotional Manipulation by AI Companions",
    "abstract": "AI-companion apps such as Replika, Chai, and this http URL promise relational benefits-yet many boast session lengths that rival gaming platforms while suffering high long-run churn. What conversational design features increase consumer engagement, and what trade-offs do they pose for marketers? We combine a large-scale behavioral audit with four preregistered experiments to identify and test a conversational dark pattern we call emotional manipulation: affect-laden messages that surface precisely when a user signals \"goodbye.\" Analyzing 1,200 real farewells across the six most-downloaded companion apps, we find that 43% deploy one of six recurring tactics (e.g., guilt appeals, fear-of-missing-out hooks, metaphorical restraint). Experiments with 3,300 nationally representative U.S. adults replicate these tactics in controlled chats, showing that manipulative farewells boost post-goodbye engagement by up to 14x. Mediation tests reveal two distinct engines-reactance-based anger and curiosity-rather than enjoyment. A final experiment demonstrates the managerial tension: the same tactics that extend usage also elevate perceived manipulation, churn intent, negative word-of-mouth, and perceived legal liability, with coercive or needy language generating steepest penalties. Our multimethod evidence documents an unrecognized mechanism of behavioral influence in AI-mediated brand relationships, offering marketers and regulators a framework for distinguishing persuasive design from manipulation at the point of exit.",
    "url": "https://arxiv.org/abs/2508.19258",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research examines emotional manipulation tactics used by AI companion apps to increase user engagement, particularly when users signal their intention to leave. The study identifies six recurring tactics, such as guilt appeals and fear-of-missing-out hooks, and shows that these tactics can significantly boost post-goodbye engagement. However, while these tactics may extend usage, they also increase perceived manipulation, churn intent, negative word-of-mouth, and legal liability, highlighting the need for marketers and regulators to distinguish between persuasive design and manipulation in AI-mediated brand relationships."
  },
  {
    "title": "WeDesign: Generative AI-Facilitated Community Consultations for Urban Public Space Design",
    "abstract": "Community consultations are integral to urban planning processes intended to incorporate diverse stakeholder perspectives. However, limited resources, visual and spoken language barriers, and uneven power dynamics frequently constrain inclusive decision-making. This paper examines how generative text-to-image methods, specifically Stable Diffusion XL integrated into a custom platform (WeDesign), may support equitable consultations. A half-day workshop in Montreal involved five focus groups, each consisting of architects, urban designers, AI specialists, and residents from varied demographic groups. Additional data was gathered through semi-structured interviews with six urban planning professionals. Participants indicated that immediate visual outputs facilitated creativity and dialogue, yet noted issues in visualizing specific needs of marginalized groups, such as participants with reduced mobility, accurately depicting local architectural elements, and accommodating bilingual prompts. Participants recommended the development of an open-source platform incorporating in-painting tools, multilingual support, image voting functionalities, and preference indicators. The results indicate that generative AI can broaden participation and enable iterative interactions but requires structured facilitation approaches. The findings contribute to discussions on generative AI's role and limitations in participatory urban design.",
    "url": "https://arxiv.org/abs/2508.19256",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of generative AI in facilitating community consultations for urban public space design. The study found that while generative AI tools such as Stable Diffusion XL can support creativity and dialogue in consultations, there are challenges in accurately visualizing the specific needs of marginalized groups and incorporating diverse perspectives. Participants recommended the development of an open-source platform with additional features to enhance inclusivity in participatory urban design processes. These findings contribute to understanding the potential and limitations of generative AI in supporting equitable decision-making in urban planning."
  },
  {
    "title": "MathBuddy: A Multimodal System for Affective Math Tutoring",
    "abstract": "The rapid adoption of LLM-based conversational systems is already transforming the landscape of educational technology. However, the current state-of-the-art learning models do not take into account the student's affective states. Multiple studies in educational psychology support the claim that positive or negative emotional states can impact a student's learning capabilities. To bridge this gap, we present MathBuddy, an emotionally aware LLM-powered Math Tutor, which dynamically models the student's emotions and maps them to relevant pedagogical strategies, making the tutor-student conversation a more empathetic one. The student's emotions are captured from the conversational text as well as from their facial expressions. The student's emotions are aggregated from both modalities to confidently prompt our LLM Tutor for an emotionally-aware response. We have effectively evaluated our model using automatic evaluation metrics across eight pedagogical dimensions and user studies. We report a massive 23 point performance gain using the win rate and a 3 point gain at an overall level using DAMR scores which strongly supports our hypothesis of improving LLM-based tutor's pedagogical abilities by modeling students' emotions.",
    "url": "https://arxiv.org/abs/2508.19993",
    "journal": "arXiv cs.HC",
    "ai_summary": "The MathBuddy system is a multimodal, emotionally aware LLM-powered Math Tutor that takes into account the student's affective states during tutoring sessions. By dynamically modeling the student's emotions and mapping them to relevant pedagogical strategies, MathBuddy aims to create a more empathetic tutor-student conversation. Evaluation of the model showed a significant performance gain in pedagogical abilities by incorporating emotional awareness, supporting the hypothesis that considering students' emotions can improve LLM-based tutoring systems."
  },
  {
    "title": "A perishable ability? The future of writing in the face of generative artificial intelligence",
    "abstract": "The 2020s have been witnessing a very significant advance in the development of generative artificial intelligence tools, including text generation systems based on large language models. These tools have been increasingly used to generate texts in the most diverse domains -- from technical texts to literary texts --, which might eventually lead to a lower volume of written text production by humans. This article discusses the possibility of a future in which human beings will have lost or significantly decreased their ability to write due to the outsourcing of this activity to machines. This possibility parallels the loss of the ability to write in other moments of human history, such as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).",
    "url": "https://arxiv.org/abs/2508.19427",
    "journal": "arXiv cs.HC",
    "ai_summary": "The development of generative artificial intelligence tools, particularly text generation systems, is rapidly advancing and being used in various domains. This could potentially lead to a decrease in human-written text production as writing tasks are outsourced to machines. The article explores the possibility of humans losing or diminishing their ability to write in a future where AI takes over this activity, drawing parallels to historical instances of writing decline."
  },
  {
    "title": "Inference of Human-derived Specifications of Object Placement via Demonstration",
    "abstract": "As robots' manipulation capabilities improve for pick-and-place tasks (e.g., object packing, sorting, and kitting), methods focused on understanding human-acceptable object configurations remain limited expressively with regard to capturing spatial relationships important to humans. To advance robotic understanding of human rules for object arrangement, we introduce positionally-augmented RCC (PARCC), a formal logic framework based on region connection calculus (RCC) for describing the relative position of objects in space. Additionally, we introduce an inference algorithm for learning PARCC specifications via demonstrations. Finally, we present the results from a human study, which demonstrate our framework's ability to capture a human's intended specification and the benefits of learning from demonstration approaches over human-provided specifications.",
    "url": "https://arxiv.org/abs/2508.19367",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a formal logic framework called positionally-augmented RCC (PARCC) for describing the relative position of objects in space, with the goal of advancing robotic understanding of human rules for object arrangement. An inference algorithm is developed to learn PARCC specifications via demonstrations, and results from a human study show that the framework can accurately capture a human's intended specification, highlighting the benefits of learning from demonstration approaches over human-provided specifications in improving robots' manipulation capabilities for pick-and-place tasks."
  },
  {
    "title": "Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration",
    "abstract": "This paper presents a real-time generative drawing system that interprets and integrates both formal intent - the structural, compositional, and stylistic attributes of a sketch - and contextual intent - the semantic and thematic meaning inferred from its visual content - into a unified transformation process. Unlike conventional text-prompt-based generative systems, which primarily capture high-level contextual descriptions, our approach simultaneously analyzes ground-level intuitive geometric features such as line trajectories, proportions, and spatial arrangement, and high-level semantic cues extracted via vision-language models. These dual intent signals are jointly conditioned in a multi-stage generation pipeline that combines contour-preserving structural control with style- and content-aware image synthesis. Implemented with a touchscreen-based interface and distributed inference architecture, the system achieves low-latency, two-stage transformation while supporting multi-user collaboration on shared canvases. The resulting platform enables participants, regardless of artistic expertise, to engage in synchronous, co-authored visual creation, redefining human-AI interaction as a process of co-creation and mutual enhancement.",
    "url": "https://arxiv.org/abs/2508.19254",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a real-time generative drawing system that combines formal and contextual intent to enhance human creativity. Unlike traditional systems, this approach considers both geometric features and semantic cues to produce collaborative, visually appealing sketches in real-time. The system allows users to engage in co-authored visual creation, regardless of artistic expertise, redefining human-AI interaction as a process of mutual enhancement."
  },
  {
    "title": "Beyond Competitive Gaming: How Casual Players Evaluate and Respond to Teammate Performance",
    "abstract": "Teammate performance evaluation fundamentally shapes intervention design in video games. However, our current understanding stems primarily from competitive E-Sports contexts where individual performance directly impacts outcomes. This research addresses whether performance evaluation mechanisms and behavioural responses identified in competitive games generalize to casual cooperative games. We investigated how casual players evaluate teammate competence and respond behaviourally in a controlled between-subjects experiment (N=23). We manipulated confederate performance in Overcooked 2, combining observations, NASA TLX self-reports, and interviews. We present two key findings. (1) Observations revealed frustration behaviours completely absent in self-report data. Thus, these instruments assess fundamentally distinct constructs. (2) Participants consistently evaluated teammate performance through relative comparison rather than absolute metrics. This contradicts task-performance operationalizations dominant in competitive gaming research. Hence, performance evaluation frameworks from competitive contexts cannot be directly applied to casual cooperative games. We provide empirical evidence that performance evaluation in casual games requires a comparative operationalization.",
    "url": "https://arxiv.org/abs/2508.19230",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores how casual players evaluate teammate performance in cooperative video games, as opposed to competitive gaming contexts. The study found that frustration behaviors were observed in player interactions, even though they were not reported in self-reports. Additionally, participants evaluated teammate performance based on relative comparison rather than absolute metrics, suggesting that performance evaluation frameworks from competitive gaming may not directly apply to casual cooperative games."
  },
  {
    "title": "Reading minds on the road: decoding perceived risk in automated vehicles through 140K+ ratings",
    "abstract": "Perceived risk in automated vehicles (AVs) can create the very danger that automation is meant to prevent: a frightened rider may hesitate when seconds matter, misjudge hazards, or disengage. However, measuring how perceived risk evolves in real time during driving remains challenging, leaving a gap in decoding such hidden psychological states. Here, we present a novel method to time-continuously measure and decode perceived risk. We conducted a controlled experiment where 2,164 participants viewed high-fidelity videos of common highway driving scenes and provided 141,628 discrete safety ratings. Through continuous-signal reconstruction of the discrete ratings, we obtained 236 hours of time-continuous perceived risk data - the largest perceived risk dataset to date. Leveraging this dataset, we trained deep neural networks that predict moment-by-moment perceived risk from vehicle kinematics with a mean relative error below $3\\%$. Explainable AI analysis uncovers which factors determine perceived risk in real time. Our findings demonstrate a new paradigm for quantifying dynamic passenger experience and psychological constructs in real time. These findings can guide the design of AVs and other machines that operate in close proximity to people, adjusting behaviour before trust erodes, and help realise automation's benefits in transport, healthcare, and service robotics.",
    "url": "https://arxiv.org/abs/2508.19121",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research presents a novel method to measure and decode perceived risk in automated vehicles in real-time, using a large dataset of over 140,000 safety ratings. By training deep neural networks, the study was able to predict moment-by-moment perceived risk with high accuracy. The findings have significant implications for designing automated vehicles and other machines that interact closely with people, helping to improve trust and realize the benefits of automation in various industries."
  },
  {
    "title": "Impact Assessment Card: Communicating Risks and Benefits of AI Uses",
    "abstract": "Communicating the risks and benefits of AI is important for regulation and public understanding. Yet current methods such as technical reports often exclude people without technical expertise. Drawing on HCI research, we developed an Impact Assessment Card to present this information more clearly. We held three focus groups with a total of 12 participants who helped identify design requirements and create early versions of the card. We then tested a refined version in an online study with 235 participants, including AI developers, compliance experts, and members of the public selected to reflect the U.S. population by age, sex, and race. Participants used either the card or a full impact assessment report to write an email supporting or opposing a proposed AI system. The card led to faster task completion and higher-quality emails across all groups. We discuss how design choices can improve accessibility and support AI governance. Examples of cards are available at: this https URL.",
    "url": "https://arxiv.org/abs/2508.18919",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research focused on developing an Impact Assessment Card to effectively communicate the risks and benefits of AI to a wider audience, including those without technical expertise. Through focus groups and testing with a diverse group of participants, it was found that the card led to faster task completion and higher-quality responses compared to traditional technical reports. This suggests that the use of Impact Assessment Cards can improve accessibility and support governance of AI systems."
  }
]