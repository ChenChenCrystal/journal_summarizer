[
  {
    "title": "SafeSpace: An Integrated Web Application for Digital Safety and Emotional Well-being",
    "abstract": "In the digital era, individuals are increasingly exposed to online harms such as toxicity, manipulation, and grooming, which often pose emotional and safety risks. Existing systems for detecting abusive content or issuing safety alerts operate in isolation and rarely combine digital safety with emotional well-being. In this paper, we present SafeSpace, a unified web application that integrates three modules: (1) toxicity detection in chats and screenshots using NLP models and Google's Perspective API, (2) a configurable safety ping system that issues emergency alerts with the user's live location (longitude and latitude) via SMTP-based emails when check-ins are missed or SOS alerts are manually triggered, and (3) a reflective questionnaire that evaluates relationship health and emotional resilience. The system employs Firebase for alert management and a modular architecture designed for usability, privacy, and scalability. The experimental evaluation shows 93% precision in toxicity detection, 100% reliability in safety alerts under emulator tests, and 92% alignment between automated and manual questionnaire scoring. SafeSpace, implemented as a web application, demonstrates the feasibility of integrating detection, protection, and reflection within a single platform, with future deployment envisioned as a mobile application for broader accessibility.",
    "url": "https://arxiv.org/abs/2508.16488",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces SafeSpace, a web application that combines toxicity detection in online interactions, a safety alert system with live location tracking, and a reflective questionnaire for emotional well-being assessment. The system shows high precision in toxicity detection, reliability in safety alerts, and alignment between automated and manual questionnaire scoring. SafeSpace demonstrates the potential of integrating digital safety and emotional well-being tools within a single platform, with plans for future deployment as a mobile application for wider use."
  },
  {
    "title": "Designing Doable and Locally-adapted Action Cards for an Interactive Tabletop Game To Support Bottom-Up Flood Resilience",
    "abstract": "Serious games can support communities in becoming more flood resilient. However, the process of identifying and integrating locally relevant and doable actions into gameplay is complex and underresearched. We approached the challenge by collaborating with a community-led education center and applying an iterative and participatory design process of identifying and defining actions that may increase local applicability and relevance. The process comprised a field observation, two expert focus groups (n=4), and an online survey (n=13). Our findings identified 27 actions related to increasing or maintaining individuals' and communities' flood resilience, which we turned into 20 playing cards. These action cards are a part of a larger interactive tabletop game, which we are currently developing. Our work discusses the potential of card games to educate non-experts to increase flood resilience, and contributes to our process of identifying local needs and conditions, and turning them into engaging game artifacts for bottom-up empowerment.",
    "url": "https://arxiv.org/abs/2508.16480",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of interactive tabletop games to support communities in becoming more flood resilient. Through a collaborative design process with a community-led education center, the researchers identified 27 locally relevant actions to increase flood resilience, which were turned into 20 playing cards for a larger game. The study highlights the potential of card games in educating non-experts on flood resilience and emphasizes the importance of identifying and integrating locally adapted actions into gameplay for bottom-up empowerment."
  },
  {
    "title": "Cooperative Design Optimization through Natural Language Interaction",
    "abstract": "Designing successful interactions requires identifying optimal design parameters. To do so, designers often conduct iterative user testing and exploratory trial-and-error. This involves balancing multiple objectives in a high-dimensional space, making the process time-consuming and cognitively demanding. System-led optimization methods, such as those based on Bayesian optimization, can determine for designers which parameters to test next. However, they offer limited opportunities for designers to intervene in the optimization process, negatively impacting the designer's experience. We propose a design optimization framework that enables natural language interactions between designers and the optimization system, facilitating cooperative design optimization. This is achieved by integrating system-led optimization methods with Large Language Models (LLMs), allowing designers to intervene in the optimization process and better understand the system's reasoning. Experimental results show that our method provides higher user agency than a system-led method and shows promising optimization performance compared to manual design. It also matches the performance of an existing cooperative method with lower cognitive load.",
    "url": "https://arxiv.org/abs/2508.16077",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores a design optimization framework that allows designers to interact with the optimization system through natural language, improving user agency and reducing cognitive load. By integrating Large Language Models with system-led optimization methods, designers can intervene in the optimization process and better understand the system's reasoning. Experimental results demonstrate that this approach offers higher user agency compared to a system-led method and performs similarly to existing cooperative methods with lower cognitive load."
  },
  {
    "title": "Prompting with Sign Parameters for Low-resource Sign Language Instruction Generation",
    "abstract": "Sign Language (SL) enables two-way communication for the deaf and hard-of-hearing community, yet many sign languages remain under-resourced in the AI space. Sign Language Instruction Generation (SLIG) produces step-by-step textual instructions that enable non-SL users to imitate and learn SL gestures, promoting two-way interaction. We introduce BdSLIG, the first Bengali SLIG dataset, used to evaluate Vision Language Models (VLMs) (i) on under-resourced SLIG tasks, and (ii) on long-tail visual concepts, as Bengali SL is unlikely to appear in the VLM pre-training data. To enhance zero-shot performance, we introduce Sign Parameter-Infused (SPI) prompting, which integrates standard SL parameters, like hand shape, motion, and orientation, directly into the textual prompts. Subsuming standard sign parameters into the prompt makes the instructions more structured and reproducible than free-form natural text from vanilla prompting. We envision that our work would promote inclusivity and advancement in SL learning systems for the under-resourced communities.",
    "url": "https://arxiv.org/abs/2508.16076",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a dataset for Bengali Sign Language Instruction Generation (BdSLIG) and evaluates Vision Language Models (VLMs) on under-resourced SLIG tasks. The study introduces Sign Parameter-Infused (SPI) prompting to improve zero-shot performance by integrating standard SL parameters into textual prompts, making instructions more structured and reproducible. This work aims to advance SL learning systems for under-resourced communities and promote inclusivity in AI research."
  },
  {
    "title": "Kokatsuji: A Visualization Approach for Typographic Forensics of Early Japanese Movable Type",
    "abstract": "We present a visualization system designed to support typographic forensics in the study of Kokatsuji, the short-lived tradition of Japanese movable wooden type printing. Building on recent advances in machine learning for block identification, our system provides expert users with an interactive tool for exploring, validating hypothesis, and integrating expert knowledge into model-generated results about the production process of early printed books. The system is structured around an ontology of four conceptual objects (spreads, segments, blocks, and characters) each corresponding to a dedicated view in the system. These coordinated views enable scholars to navigate between material evidence and computational abstractions, supporting close, near-by, and distant reading practices. Preliminary results from expert use of the system demonstrate its ability to reveal errors in segmentation, inconsistencies in clustering, and previously inaccessible patterns of block reuse.",
    "url": "https://arxiv.org/abs/2508.15995",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research presents a visualization system for studying Kokatsuji, a Japanese movable wooden type printing tradition. The system utilizes machine learning for block identification and allows expert users to explore and validate hypotheses about the production process of early printed books. The system's ontology of four conceptual objects enables scholars to navigate between material evidence and computational abstractions, revealing errors in segmentation, inconsistencies in clustering, and patterns of block reuse that were previously inaccessible."
  },
  {
    "title": "VR Fire safety training application",
    "abstract": "Fire emergencies can happen without warning and knowing how to respond quickly can save lives Unfortunately traditional fire drills can be disruptive costly and often fail to recreate the pressure of a real emergency This project introduces a Virtual Reality VR Fire Safety Training Application that gives people a safe yet realistic way to practice life saving skills Using a VR headset and motion controllers trainees step into a 3D world where fire hazards smoke and evacuation routes are brought to life They can learn how to use a fire extinguisher find safe exits and make decisions under pressure without any real danger The training adapts to the users skill level and tracks progress making it useful for beginners and experienced personnel alike By turning fire safety into an interactive experience this VR approach boosts confidence improves retention and makes learning both safer and more engaging",
    "url": "https://arxiv.org/abs/2508.15788",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a Virtual Reality Fire Safety Training Application that allows individuals to practice life-saving skills in a safe yet realistic environment. The application uses VR technology to simulate fire hazards, smoke, and evacuation routes, allowing users to learn how to respond to emergencies without any real danger. This interactive training approach improves confidence, retention, and engagement, making it beneficial for both beginners and experienced personnel in fire safety."
  },
  {
    "title": "Harmonious Color Pairings: Insights from Human Preference and Natural Hue Statistics",
    "abstract": "While color harmony has long been studied in art and design, a clear consensus remains elusive, as most models are grounded in qualitative insights or limited datasets. In this work, we present a quantitative, data-driven study of color pairing preferences using controlled hue-based palettes in the HSL color space. Participants evaluated combinations of thirteen distinct hues, enabling us to construct a preference matrix and define a combinability index for each color. Our results reveal that preferences are highly hue dependent, challenging the assumption of universal harmony rules proposed in the literature. Yet, when averaged over hues, statistically meaningful patterns of aesthetic preference emerge, with certain hue separations perceived as more harmonious. Strikingly, these patterns align with hue distributions found in natural landscapes, pointing to a statistical correspondence between human color preferences and the structure of color in nature. Together, these findings offer a quantitative framework for studying color harmony and its potential perceptual and ecological underpinnings.",
    "url": "https://arxiv.org/abs/2508.15777",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research uses a quantitative approach to study color pairing preferences, finding that preferences are highly dependent on hue. While there is no universal harmony rule, certain hue separations are perceived as more harmonious, aligning with natural hue distributions. These findings provide a new framework for understanding color harmony and its connection to human perception and the natural environment."
  },
  {
    "title": "Real-time 3D Light-field Viewing with Eye-tracking on Conventional Displays",
    "abstract": "Creating immersive 3D visual experiences typically requires expensive and specialized hardware such as VR headsets, autostereoscopic displays, or active shutter glasses. These constraints limit the accessibility and everyday use of 3D visualization technologies in resource-constrained settings. To address this, we propose a low-cost system that enables real-time 3D light-field viewing using only a standard 2D monitor, a conventional RGB webcam, and red-cyan anaglyph glasses. The system integrates real-time eye-tracking to dynamically adapt the displayed light-field image to the user's head position with a lightweight rendering pipeline that selects and composites stereoscopic views from pre-captured light-field data. The resulting anaglyph image is updated in real-time, creating a more immersive and responsive 3D experience. The system operates entirely on CPU and maintains a stable frame rate of 30 FPS, confirming its feasibility on typical consumer-grade hardware. All of these highlight the potential of our approach as an accessible platform for interactive 3D applications in education, digital media, and beyond.",
    "url": "https://arxiv.org/abs/2508.16535",
    "journal": "arXiv cs.HC",
    "ai_summary": "The researchers developed a low-cost system that enables real-time 3D light-field viewing on a standard 2D monitor using only a webcam and red-cyan anaglyph glasses. By integrating real-time eye-tracking, the system adapts the displayed image to the user's head position, creating a more immersive and responsive 3D experience. The system operates on CPU and maintains a stable frame rate of 30 FPS, making it a feasible and accessible platform for interactive 3D applications in various fields."
  },
  {
    "title": "HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images",
    "abstract": "Hand-object 3D reconstruction has become increasingly important for applications in human-robot interaction and immersive AR/VR experiences. A common approach for object-agnostic hand-object reconstruction from RGB sequences involves a two-stage pipeline: hand-object 3D tracking followed by multi-view 3D reconstruction. However, existing methods rely on keypoint detection techniques, such as Structure from Motion (SfM) and hand-keypoint optimization, which struggle with diverse object geometries, weak textures, and mutual hand-object occlusions, limiting scalability and generalization. As a key enabler to generic and seamless, non-intrusive applicability, we propose in this work a robust, keypoint detector-free approach to estimating hand-object 3D transformations from monocular motion video/images. We further integrate this with a multi-view reconstruction pipeline to accurately recover hand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely on pre-scanned object templates or camera intrinsics, and reaches state-of-the-art performance for the tasks of object-agnostic hand-object 3D transformation and shape estimation on the SHOWMe benchmark. We also experiment on sequences from the HO3D dataset, demonstrating generalization to unseen object categories.",
    "url": "https://arxiv.org/abs/2508.16465",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces a new method called HOSt3R for hand-object 3D reconstruction from RGB images without relying on keypoint detection techniques. This approach is shown to outperform existing methods in terms of scalability and generalization, achieving state-of-the-art performance on the SHOWMe benchmark and demonstrating generalization to unseen object categories. The significance of this research lies in its potential applications in human-robot interaction and immersive AR/VR experiences, offering a more robust and versatile solution for hand-object 3D reconstruction."
  },
  {
    "title": "Audio2Face-3D: Audio-driven Realistic Facial Animation For Digital Avatars",
    "abstract": "Audio-driven facial animation presents an effective solution for animating digital avatars. In this paper, we detail the technical aspects of NVIDIA Audio2Face-3D, including data acquisition, network architecture, retargeting methodology, evaluation metrics, and use cases. Audio2Face-3D system enables real-time interaction between human users and interactive avatars, facilitating facial animation authoring for game characters. To assist digital avatar creators and game developers in generating realistic facial animations, we have open-sourced Audio2Face-3D networks, SDK, training framework, and example dataset.",
    "url": "https://arxiv.org/abs/2508.16401",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research paper discusses the development of NVIDIA Audio2Face-3D, a system for audio-driven realistic facial animation for digital avatars. The system includes data acquisition, network architecture, retargeting methodology, evaluation metrics, and use cases, allowing for real-time interaction between human users and avatars. The open-sourcing of Audio2Face-3D networks, SDK, training framework, and example dataset aims to assist digital avatar creators and game developers in creating more realistic facial animations."
  },
  {
    "title": "The next question after Turing's question: Introducing the Grow-AI test",
    "abstract": "This study aims to extend the framework for assessing artificial intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom), designed to answer the question \"Can machines grow up?\" -- a natural successor to the Turing Test. The methodology applied is based on a system of six primary criteria (C1-C6), each assessed through a specific \"game\", divided into four arenas that explore both the human dimension and its transposition into AI. All decisions and actions of the entity are recorded in a standardized AI Journal, the primary source for calculating composite scores. The assessment uses the prior expert method to establish initial weights, and the global score -- Grow Up Index -- is calculated as the arithmetic mean of the six scores, with interpretation on maturity thresholds. The results show that the methodology allows for a coherent and comparable assessment of the level of \"growth\" of AI entities, regardless of their type (robots, software agents, LLMs). The multi-game structure highlights strengths and vulnerable areas, and the use of a unified journal guarantees traceability and replicability in the evaluation. The originality of the work lies in the conceptual transposition of the process of \"growing\" from the human world to that of artificial intelligence, in an integrated testing format that combines perspectives from psychology, robotics, computer science, and ethics. Through this approach, GROW-AI not only measures performance but also captures the evolutionary path of an AI entity towards maturity.",
    "url": "https://arxiv.org/abs/2508.16277",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study introduces the Grow-AI test as a framework for assessing artificial intelligence's ability to \"grow up\", building on the Turing Test. The methodology involves six criteria assessed through specific games in four arenas, with decisions and actions recorded in an AI Journal. The results show that this approach allows for a coherent assessment of AI entities' growth, highlighting strengths and areas for improvement, and providing a pathway towards maturity."
  },
  {
    "title": "EEG Study of the Influence of Imagined Temperature Sensations on Neuronal Activity in the Sensorimotor Cortex",
    "abstract": "Understanding the neural correlates of sensory imagery is crucial for advancing cognitive neuroscience and developing novel Brain-Computer Interface (BCI) paradigms. This study investigated the influence of imagined temperature sensations (ITS) on neural activity within the sensorimotor cortex. The experimental study involved the evaluation of neural activity using electroencephalography (EEG) during both real thermal stimulation (TS: 40°C Hot, 20°C Cold) applied to the participants' hand, and the mental temperature imagination (ITS) of the corresponding hot and cold sensations. The analysis focused on quantifying the event-related desynchronization (ERD) of the sensorimotor mu-rhythm (8-13 Hz). The experimental results revealed a characteristic mu-ERD localized over central scalp regions (e.g., C3) during both TS and ITS conditions. Although the magnitude of mu-ERD during ITS was slightly lower than during TS, this difference was not statistically significant (p>.05). However, ERD during both ITS and TS was statistically significantly different from the resting baseline (p<.001). These findings demonstrate that imagining temperature sensations engages sensorimotor cortical mechanisms in a manner comparable to actual thermal perception. This insight expands our understanding of the neurophysiological basis of sensory imagery and suggests the potential utility of ITS for non-motor BCI control and neurorehabilitation technologies.",
    "url": "https://arxiv.org/abs/2508.16274",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explored the impact of imagined temperature sensations on neural activity in the sensorimotor cortex using EEG. The results showed that imagining temperature sensations led to similar patterns of neural activation as actual thermal stimulation, indicating the potential for using imagined sensations in non-motor BCI control and neurorehabilitation technologies. This research contributes to our understanding of sensory imagery and its implications for cognitive neuroscience and BCI development."
  },
  {
    "title": "Towards Recommending Usability Improvements with Multimodal Large Language Models",
    "abstract": "Usability describes a set of essential quality attributes of user interfaces (UI) that influence human-computer interaction. Common evaluation methods, such as usability testing and inspection, are effective but resource-intensive and require expert involvement. This makes them less accessible for smaller organizations. Recent advances in multimodal LLMs offer promising opportunities to automate usability evaluation processes partly by analyzing textual, visual, and structural aspects of software interfaces. To investigate this possibility, we formulate usability evaluation as a recommendation task, where multimodal LLMs rank usability issues by severity. We conducted an initial proof-of-concept study to compare LLM-generated usability improvement recommendations with usability expert assessments. Our findings indicate the potential of LLMs to enable faster and more cost-effective usability evaluation, which makes it a practical alternative in contexts with limited expert resources.",
    "url": "https://arxiv.org/abs/2508.16165",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of multimodal Large Language Models (LLMs) to automate usability evaluation processes for user interfaces. By analyzing textual, visual, and structural aspects of software interfaces, LLMs can rank usability issues by severity, potentially enabling faster and more cost-effective evaluations compared to traditional methods. The study suggests that LLMs have the potential to provide practical recommendations for usability improvements, particularly in contexts where expert resources are limited."
  },
  {
    "title": "Embarrassed to observe: The effects of directive language in brand conversation",
    "abstract": "In social media, marketers attempt to influence consumers by using directive language, that is, expressions designed to get consumers to take action. While the literature has shown that directive messages in advertising have mixed results for recipients, we know little about the effects of directive brand language on consumers who see brands interacting with other consumers in social media conversations. On the basis of a field study and three online experiments, this study shows that directive language in brand conversation has a detrimental downstream effect on engagement of consumers who observe such exchanges. Specifically, in line with Goffman's facework theory, because a brand that encourages consumers to react could be perceived as face-threatening, consumers who see a brand interacting with others in a directive way may feel vicarious embarrassment and engage less (compared with a conversation without directive language). In addition, we find that when the conversation is nonproduct-centered (vs. product-centered), consumers expect more freedom, as in mundane conversations, even for others; therefore, directive language has a stronger negative effect. However, in this context, the strength of the brand relationship mitigates this effect. Thus, this study contributes to the literature on directive language and brand-consumer interactions by highlighting the importance of context in interactive communication, with direct relevance for social media and brand management.",
    "url": "https://arxiv.org/abs/2508.15826",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores the impact of directive language used by brands in social media conversations on consumer engagement. The research shows that consumers who observe brands using directive language may feel embarrassed and engage less with the brand. The study also suggests that the context of the conversation, such as whether it is product-centered or nonproduct-centered, and the strength of the brand relationship can influence the negative effects of directive language. This research contributes to the understanding of brand-consumer interactions in social media and emphasizes the importance of context in communication strategies."
  },
  {
    "title": "LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions",
    "abstract": "Phone call transcript labeling is prohibitively expensive (approximately 2 USD per minute) due to privacy regulations, consent requirements, and manual annotation costs requiring 3 hours of expert time per hour of audio. Existing extraction methods fail on conversational speech containing disfluencies, interruptions, and speaker overlap. We introduce LingVarBench, a synthetic data generation pipeline that addresses these constraints through automated validation. First, we prompt an LLM to generate realistic structured field values across multiple use cases. Second, we recursively prompt the model to transform these values into thousands of natural conversational utterances containing typical phone call characteristics. Third, we validate each synthetic utterance by testing whether a separate LLM-based extractor can recover the original structured information. We employ DSPy's SIMBA optimizer to automatically synthesize extraction prompts from validated synthetic transcripts, eliminating manual prompt engineering. Our optimized prompts achieve up to 95 percent accuracy for numeric fields (vs. 88-89 percent zero-shot), 90 percent for names (vs. 47-79 percent), and over 80 percent for dates (vs. 72-77 percent) on real customer transcripts, demonstrating substantial gains over zero-shot prompting. The synthetic-to-real transfer demonstrates that conversational patterns learned from generated data generalize effectively to authentic phone calls containing background noise and domain-specific terminology. LingVarBench provides the first systematic benchmark for structured extraction from synthetic conversational data, demonstrating that automated prompt optimization overcomes cost and privacy barriers preventing large-scale phone call analysis in commercial settings.",
    "url": "https://arxiv.org/abs/2508.15801",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces LingVarBench, a synthetic data generation pipeline that addresses the high cost and privacy constraints of labeling phone call transcripts for named entity recognition. By prompting a language model to generate realistic structured field values and transforming them into conversational utterances, the researchers were able to achieve high accuracy in extracting information from real customer transcripts. The study demonstrates that automated prompt optimization can overcome barriers to large-scale phone call analysis in commercial settings, providing a systematic benchmark for structured extraction from synthetic conversational data."
  },
  {
    "title": "Physically Plausible Data Augmentations for Wearable IMU-based Human Activity Recognition Using Physics Simulation",
    "abstract": "The scarcity of high-quality labeled data in sensor-based Human Activity Recognition (HAR) hinders model performance and limits generalization across real-world scenarios. Data augmentation is a key strategy to mitigate this issue by enhancing the diversity of training datasets. Signal Transformation-based Data Augmentation (STDA) techniques have been widely used in HAR. However, these methods are often physically implausible, potentially resulting in augmented data that fails to preserve the original meaning of the activity labels. In this study, we introduce and systematically characterize Physically Plausible Data Augmentation (PPDA) enabled by physics simulation. PPDA leverages human body movement data from motion capture or video-based pose estimation and incorporates various realistic variabilities through physics simulation, including modifying body movements, sensor placements, and hardware-related effects. We compare the performance of PPDAs with traditional STDAs on three public datasets of daily activities and fitness workouts. First, we evaluate each augmentation method individually, directly comparing PPDAs to their STDA counterparts. Next, we assess how combining multiple PPDAs can reduce the need for initial data collection by varying the number of subjects used for training. Experiments show consistent benefits of PPDAs, improving macro F1 scores by an average of 3.7 pp (up to 13 pp) and achieving competitive performance with up to 60% fewer training subjects than STDAs. As the first systematic study of PPDA in sensor-based HAR, these results highlight the advantages of pursuing physical plausibility in data augmentation and the potential of physics simulation for generating synthetic Inertial Measurement Unit data for training deep learning HAR models. This cost-effective and scalable approach therefore helps address the annotation scarcity challenge in HAR.",
    "url": "https://arxiv.org/abs/2508.13284",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces Physically Plausible Data Augmentation (PPDA) for Human Activity Recognition (HAR) using physics simulation to create realistic variations in training data. The study shows that PPDA outperforms traditional Signal Transformation-based Data Augmentation (STDA) methods, improving model performance by an average of 3.7 percentage points and requiring up to 60% fewer training subjects. By addressing the scarcity of labeled data in sensor-based HAR, PPDA offers a cost-effective and scalable solution for enhancing model generalization across real-world scenarios."
  },
  {
    "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards Geospatial AI Agents for Visual Inquiries",
    "abstract": "Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work.",
    "url": "https://arxiv.org/abs/2508.15752",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the development of Geo-Visual Agents, AI agents that can understand and respond to visual-spatial inquiries about the world by analyzing geospatial images from various sources. This new approach aims to address the limitations of traditional interactive digital maps that rely on structured GIS data. The study outlines the vision for Geo-Visual Agents, presents three examples, and highlights key challenges and opportunities for future research in this area."
  },
  {
    "title": "Demystifying Reward Design in Reinforcement Learning for Upper Extremity Interaction: Practical Guidelines for Biomechanical Simulations in HCI",
    "abstract": "Designing effective reward functions is critical for reinforcement learning-based biomechanical simulations, yet HCI researchers and practitioners often waste (computation) time with unintuitive trial-and-error tuning. This paper demystifies reward function design by systematically analyzing the impact of effort minimization, task completion bonuses, and target proximity incentives on typical HCI tasks such as pointing, tracking, and choice reaction. We show that proximity incentives are essential for guiding movement, while completion bonuses ensure task success. Effort terms, though optional, help refine motion regularity when appropriately scaled. We perform an extensive analysis of how sensitive task success and completion time depend on the weights of these three reward components. From these results we derive practical guidelines to create plausible biomechanical simulations without the need for reinforcement learning expertise, which we then validate on remote control and keyboard typing tasks. This paper advances simulation-based interaction design and evaluation in HCI by improving the efficiency and applicability of biomechanical user modeling for real-world interface development.",
    "url": "https://arxiv.org/abs/2508.15727",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores the impact of different reward components such as effort minimization, task completion bonuses, and target proximity incentives on biomechanical simulations in HCI tasks. The study shows that proximity incentives are crucial for guiding movement, completion bonuses ensure task success, and effort terms can refine motion regularity. The findings provide practical guidelines for creating effective biomechanical simulations without the need for reinforcement learning expertise, ultimately improving the efficiency and applicability of user modeling in interface development."
  },
  {
    "title": "Foundation Models for Cross-Domain EEG Analysis Application: A Survey",
    "abstract": "Electroencephalography (EEG) analysis stands at the forefront of neuroscience and artificial intelligence research, where foundation models are reshaping the traditional EEG analysis paradigm by leveraging their powerful representational capacity and cross-modal generalization. However, the rapid proliferation of these techniques has led to a fragmented research landscape, characterized by diverse model roles, inconsistent architectures, and a lack of systematic categorization. To bridge this gap, this study presents the first comprehensive modality-oriented taxonomy for foundation models in EEG analysis, systematically organizing research advances based on output modalities of the native EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal frameworks. We rigorously analyze each category's research ideas, theoretical foundations, and architectural innovations, while highlighting open challenges such as model interpretability, cross-domain generalization, and real-world applicability in EEG-based systems. By unifying this dispersed field, our work not only provides a reference framework for future methodology development but accelerates the translation of EEG foundation models into scalable, interpretable, and online actionable solutions.",
    "url": "https://arxiv.org/abs/2508.15716",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores the use of foundation models in EEG analysis, highlighting their potential to reshape traditional paradigms by leveraging their powerful representational capacity and cross-modal generalization. The research presents a comprehensive taxonomy for organizing these models based on output modalities, such as EEG decoding, EEG-text, EEG-vision, and EEG-audio, as well as broader multimodal frameworks. By addressing challenges such as model interpretability and real-world applicability, this work aims to provide a reference framework for future methodology development and accelerate the translation of EEG foundation models into scalable and actionable solutions."
  },
  {
    "title": "Spatio-Temporal Mixed and Augmented Reality Experience Description for Interactive Playback",
    "abstract": "We propose the Spatio-Temporal Mixed and Augmented Reality Experience Description (MAR-ED), a novel framework to standardize the representation of past events for interactive and adaptive playback in a user's present physical space. While current spatial media technologies have primarily focused on capturing or replaying content as static assets, often disconnected from the viewer's environment or offering limited interactivity, the means to describe an experience's underlying semantic and interactive structure remains underexplored. We propose a descriptive framework called MAR-ED based on three core primitives: 1) Event Primitives for semantic scene graph representation, 2) Keyframe Primitives for efficient and meaningful data access, and 3) Playback Primitives for user-driven adaptive interactive playback of recorded MAR experience. The proposed flowchart of the three-stage process of the proposed MAR-ED framework transforms a recorded experience into a unique adaptive MAR experience during playback, where its spatio-temporal structure dynamically conforms to a new environment and its narrative can be altered by live user input. Drawing on this framework, personal digital memories and recorded events can evolve beyond passive 2D/3D videos into immersive, spatially-integrated group experiences, opening new paradigms for training, cultural heritage, and interactive storytelling without requiring complex, per-user adaptive rendering.",
    "url": "https://arxiv.org/abs/2508.15258",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces a framework called MAR-ED to standardize the representation of past events for interactive playback in a user's physical space. This framework allows for dynamic adaptation of the spatio-temporal structure of recorded experiences, enabling immersive and interactive storytelling experiences that can be altered by user input. The proposed framework has implications for training, cultural heritage preservation, and interactive storytelling, offering a new way to transform digital memories and recorded events into spatially-integrated group experiences."
  },
  {
    "title": "Visualization on Smart Wristbands: Results from an In-situ Design Workshop with Four Scenarios",
    "abstract": "We present the results of an in-situ ideation workshop for designing data visualizations on smart wristbands that can show data around the entire wrist of a wearer. Wristbands pose interesting challenges because the visibility of different areas of the band depends on the wearer's arm posture. We focused on four usage scenarios that lead to different postures: office work, leisurely walks, cycling, and driving. As the technology for smart wristbands is not yet commercially available, we conducted a paper-based ideation exercise that showed how spatial layout and visualization design on smart wristbands may need to vary depending on the types of data items of interest and arm postures. Participants expressed a strong preference for responsive visualization designs that could adapt to the movement of wearers' arms. Supplemental material from the study is available here: this https URL.",
    "url": "https://arxiv.org/abs/2508.15249",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the design of data visualizations on smart wristbands, taking into account the challenges posed by the visibility of different areas of the band depending on the wearer's arm posture. Four usage scenarios were considered, and participants preferred responsive visualization designs that could adapt to the movement of wearers' arms. The study highlights the importance of considering spatial layout and visualization design in the development of smart wristbands, especially as the technology is not yet commercially available."
  },
  {
    "title": "GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design",
    "abstract": "Environment designers in the entertainment industry create imaginative 2D and 3D scenes for games, films, and television, requiring both fine-grained control of specific details and consistent global coherence. Designers have increasingly integrated generative AI into their workflows, often relying on large language models (LLMs) to expand user prompts for text-to-image generation, then iteratively refining those prompts and applying inpainting. However, our formative study with 10 designers surfaced two key challenges: (1) the lengthy LLM-generated prompts make it difficult to understand and isolate the keywords that must be revised for specific visual elements; and (2) while inpainting supports localized edits, it can struggle with global consistency and correctness. Based on these insights, we present GenTune, an approach that enhances human--AI collaboration by clarifying how AI-generated prompts map to image content. Our GenTune system lets designers select any element in a generated image, trace it back to the corresponding prompt labels, and revise those labels to guide precise yet globally consistent image refinement. In a summative study with 20 designers, GenTune significantly improved prompt--image comprehension, refinement quality, and efficiency, and overall satisfaction (all $p < .01$) compared to current practice. A follow-up field study with two studios further demonstrated its effectiveness in real-world settings.",
    "url": "https://arxiv.org/abs/2508.15227",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the challenges faced by environment designers when using generative AI models for image refinement in entertainment design. The study introduces GenTune, a tool that improves prompt-image comprehension and refinement quality by allowing designers to trace AI-generated prompts back to specific image elements and revise them for precise yet globally consistent refinement. The results show that GenTune significantly enhances collaboration between humans and AI, leading to improved efficiency, satisfaction, and overall quality in image refinement for entertainment design."
  },
  {
    "title": "Evaluating an Immersive Analytics Application at an Enterprise Business Intelligence Customer Conference",
    "abstract": "We reflect on an evaluation of an immersive analytics application (Tableau for visionOS) conducted at a large enterprise business intelligence (BI) conference. Conducting a study in such a context offered an opportunistic setting to gather diverse feedback. However, this setting also highlighted the challenge of evaluating usability while also assessing potential utility, as feedback straddled between the novelty of the experience and the practicality of the application in participants' analytical workflows. This formative evaluation with 22 participants allowed us to gather insights with respect to the usability of Tableau for visionOS, along with broader perspectives on the potential for head-mounted displays (HMDs) to promote new ways to engage with BI data. Our experience suggests a need for new evaluation considerations that integrate qualitative and quantitative measures and account for unique interaction patterns with 3D representations and interfaces accessible via an HMD. Overall, we contribute an enterprise perspective on evaluation methodologies for immersive analytics.",
    "url": "https://arxiv.org/abs/2508.15152",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research evaluated the immersive analytics application Tableau for visionOS at an enterprise business intelligence conference, gathering feedback on usability and utility. The study highlighted the challenge of assessing both the novelty and practicality of the application in participants' analytical workflows. The findings suggest the need for new evaluation considerations that integrate qualitative and quantitative measures for 3D representations and interfaces accessible via head-mounted displays, contributing to the understanding of immersive analytics in an enterprise setting."
  },
  {
    "title": "ReviseMate: Exploring Contextual Support for Digesting STEM Paper Reviews",
    "abstract": "Effectively assimilating and integrating reviewer feedback is crucial for researchers seeking to refine their papers and handle potential rebuttal phases in academic venues. However, traditional review digestion processes present challenges such as time consumption, reading fatigue, and the requisite for comprehensive analytical skills. Prior research on review analysis often provides theoretical guidance with limited targeted support. Additionally, general text comprehension tools overlook the intricate nature of comprehensively understanding reviews and lack contextual assistance. To bridge this gap, we formulated research questions to explore the authors' concerns and methods for enhancing comprehension during the review digestion phase. Through interviews and the creation of storyboards, we developed ReviseMate, an interactive system designed to address the identified challenges. A controlled user study (N=31) demonstrated the superiority of ReviseMate over baseline methods, with positive feedback regarding user interaction. Subsequent field deployment (N=6) further validated the effectiveness of ReviseMate in real-world review digestion scenarios. These findings underscore the potential of interactive tools to significantly enhance the assimilation and integration of reviewer feedback during the manuscript review process.",
    "url": "https://arxiv.org/abs/2508.15148",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the challenges researchers face in assimilating and integrating reviewer feedback in academic venues, highlighting the limitations of traditional review digestion processes. The study introduces ReviseMate, an interactive system designed to enhance comprehension during the review digestion phase, which was found to be superior to baseline methods in a controlled user study. The findings suggest that interactive tools like ReviseMate have the potential to significantly improve the assimilation and integration of reviewer feedback in the manuscript review process."
  },
  {
    "title": "QueryGenie: Making LLM-Based Database Querying Transparent and Controllable",
    "abstract": "Conversational user interfaces powered by large language models (LLMs) have significantly lowered the technical barriers to database querying. However, existing tools still encounter several challenges, such as misinterpretation of user intent, generation of hallucinated content, and the absence of effective mechanisms for human feedback-all of which undermine their reliability and practical utility. To address these issues and promote a more transparent and controllable querying experience, we proposed QueryGenie, an interactive system that enables users to monitor, understand, and guide the LLM-driven query generation process. Through incremental reasoning, real-time validation, and responsive interaction mechanisms, users can iteratively refine query logic and ensure alignment with their intent.",
    "url": "https://arxiv.org/abs/2508.15146",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces QueryGenie, a system designed to improve the transparency and control of database querying using large language models (LLMs) in conversational user interfaces. The system addresses challenges such as misinterpretation of user intent and generation of incorrect information by allowing users to monitor, understand, and guide the query generation process. Through incremental reasoning, real-time validation, and responsive interaction mechanisms, users can iteratively refine query logic to ensure it aligns with their intent, ultimately improving the reliability and practical utility of LLM-based database querying."
  },
  {
    "title": "Understanding Accessibility Needs of Blind Authors on CMS-Based Websites",
    "abstract": "This paper addresses the limited attention given to blind users as content creators in Content Management Systems (CMS), a gap that remains under-explored in web accessibility research. For blind authors, effective interaction with CMS platforms requires more than technical compliance; it demands interfaces designed with semantic clarity, predictable navigation, and meaningful feedback for screen reader users. This study investigates the accessibility barriers blind users face when performing key tasks, such as page creation, menu editing, and image publishing, using CMS platforms. A two-fold evaluation was conducted using automated tools and manual usability testing with three blind and one sighted participant, complemented by expert analysis based on the Barrier Walkthrough method. Results showed that block-based interfaces were particularly challenging, often marked as accessible by automated tools but resulting in critical usability issues during manual evaluation. The use of a text-based editor, the integration of AI-generated image descriptions, and training aligned with screen reader workflows, significantly improved usability and autonomy. These findings underscore the limitations of automated assessments and highlight the importance of user-centered design practices. Enhancing CMS accessibility requires consistent navigation structures, reduced reliance on visual interaction patterns, and the integration of AI tools that support blind content authors throughout the content creation process.",
    "url": "https://arxiv.org/abs/2508.15045",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research examines the accessibility needs of blind authors using Content Management Systems (CMS) and highlights the importance of user-centered design practices. The study found that blind users face barriers when performing tasks such as page creation and image publishing on CMS platforms, particularly with block-based interfaces. The integration of AI tools, text-based editors, and training aligned with screen reader workflows significantly improved usability and autonomy for blind content creators."
  },
  {
    "title": "LitForager: Exploring Multimodal Literature Foraging Strategies in Immersive Sensemaking",
    "abstract": "Exploring and comprehending relevant academic literature is a vital yet challenging task for researchers, especially given the rapid expansion in research publications. This task fundamentally involves sensemaking - interpreting complex, scattered information sources to build understanding. While emerging immersive analytics tools have shown cognitive benefits like enhanced spatial memory and reduced mental load, they predominantly focus on information synthesis (e.g., organizing known documents). In contrast, the equally important information foraging phase - discovering and gathering relevant literature - remains underexplored within immersive environments, hindering a complete sensemaking workflow. To bridge this gap, we introduce LitForager, an interactive literature exploration tool designed to facilitate information foraging of research literature within an immersive sensemaking workflow using network-based visualizations and multimodal interactions. Developed with WebXR and informed by a formative study with researchers, LitForager supports exploration guidance, spatial organization, and seamless transition through a 3D literature network. An observational user study with 15 researchers demonstrated LitForager's effectiveness in supporting fluid foraging strategies and spatial sensemaking through its multimodal interface.",
    "url": "https://arxiv.org/abs/2508.15043",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the challenges researchers face in comprehending academic literature and introduces LitForager, an interactive tool designed to facilitate the information foraging phase within an immersive sensemaking workflow. LitForager utilizes network-based visualizations and multimodal interactions to support fluid foraging strategies and spatial sensemaking, as demonstrated in an observational user study with 15 researchers. This research highlights the importance of addressing the information foraging phase in immersive environments to enhance researchers' ability to explore and gather relevant literature effectively."
  },
  {
    "title": "Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle",
    "abstract": "This paper argues that a techno-philosophical reading of the EU AI Act provides insight into the long-term dynamics of data in AI systems, specifically, how the lifecycle from ingestion to deployment generates recursive value chains that challenge existing frameworks for Responsible AI. We introduce a conceptual tool to frame the AI pipeline, spanning data, training regimes, architectures, feature stores, and transfer learning. Using cross-disciplinary methods, we develop a technically grounded and philosophically coherent analysis of regulatory blind spots. Our central claim is that what remains absent from policymaking is an account of the dynamic of becoming that underpins both the technical operation and economic logic of AI. To address this, we advance a formal reading of AI inspired by Simondonian philosophy of technology, reworking his concept of individuation to model the AI lifecycle, including the pre-individual milieu, individuation, and individuated AI. To translate these ideas, we introduce futurity: the self-reinforcing lifecycle of AI, where more data enhances performance, deepens personalisation, and expands application domains. Futurity highlights the recursively generative, non-rivalrous nature of data, underpinned by infrastructures like feature stores that enable feedback, adaptation, and temporal recursion. Our intervention foregrounds escalating power asymmetries, particularly the tech oligarchy whose infrastructures of capture, training, and deployment concentrate value and decision-making. We argue that effective regulation must address these infrastructural and temporal dynamics, and propose measures including lifecycle audits, temporal traceability, feedback accountability, recursion transparency, and a right to contest recursive reuse.",
    "url": "https://arxiv.org/abs/2508.15680",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores the long-term dynamics of data in AI systems, focusing on the lifecycle from ingestion to deployment and how it challenges existing frameworks for Responsible AI. The study introduces the concept of futurity to highlight the self-reinforcing nature of AI, where more data enhances performance and deepens personalization. The paper emphasizes the need for regulation to address escalating power asymmetries and proposes measures such as lifecycle audits and feedback accountability to ensure transparency and accountability in the AI lifecycle."
  },
  {
    "title": "\\textit{adder-viz}: Real-Time Visualization Software for Transcoding Event Video",
    "abstract": "Recent years have brought about a surge in neuromorphic ``event'' video research, primarily targeting computer vision applications. Event video eschews video frames in favor of asynchronous, per-pixel intensity samples. While much work has focused on a handful of representations for specific event cameras, these representations have shown limitations in flexibility, speed, and compressibility. We previously proposed the unified AD{\\Delta}ER representation to address these concerns. This paper introduces numerous improvements to the \\textit{adder-viz} software for visualizing real-time event transcode processes and applications in-the-loop. The MIT-licensed software is available from a centralized repository at \\href{this https URL}{this https URL}.",
    "url": "https://arxiv.org/abs/2508.14996",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research focuses on improving the visualization software \\textit{adder-viz} for real-time event video transcoding, which is crucial for neuromorphic event video research in computer vision applications. The unified AD{\\Delta}ER representation proposed in the study addresses limitations in flexibility, speed, and compressibility of existing representations for event cameras. The software enhancements enable better visualization of event transcode processes and facilitate applications in-the-loop, making it a valuable tool for researchers in the field."
  },
  {
    "title": "Human Feedback Driven Dynamic Speech Emotion Recognition",
    "abstract": "This work proposes to explore a new area of dynamic speech emotion recognition. Unlike traditional methods, we assume that each audio track is associated with a sequence of emotions active at different moments in time. The study particularly focuses on the animation of emotional 3D avatars. We propose a multi-stage method that includes the training of a classical speech emotion recognition model, synthetic generation of emotional sequences, and further model improvement based on human feedback. Additionally, we introduce a novel approach to modeling emotional mixtures based on the Dirichlet distribution. The models are evaluated based on ground-truth emotions extracted from a dataset of 3D facial animations. We compare our models against the sliding window approach. Our experimental results show the effectiveness of Dirichlet-based approach in modeling emotional mixtures. Incorporating human feedback further improves the model quality while providing a simplified annotation procedure.",
    "url": "https://arxiv.org/abs/2508.14920",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores dynamic speech emotion recognition by associating audio tracks with sequences of emotions over time. The study focuses on animating emotional 3D avatars and proposes a multi-stage method that includes training a speech emotion recognition model, generating emotional sequences synthetically, and improving the model based on human feedback. The use of a novel Dirichlet-based approach for modeling emotional mixtures shows effectiveness, with the incorporation of human feedback leading to improved model quality and simplified annotation procedures."
  },
  {
    "title": "From Passive Tool to Socio-cognitive Teammate: A Conceptual Framework for Agentic AI in Human-AI Collaborative Learning",
    "abstract": "The role of Artificial Intelligence (AI) in education is undergoing a rapid transformation, moving beyond its historical function as an instructional tool towards a new potential as an active participant in the learning process. This shift is driven by the emergence of agentic AI, autonomous systems capable of proactive, goal-directed action. However, the field lacks a robust conceptual framework to understand, design, and evaluate this new paradigm of human-AI interaction in learning. This paper addresses this gap by proposing a novel conceptual framework (the APCP framework) that charts the transition from AI as a tool to AI as a collaborative partner. We present a four-level model of escalating AI agency within human-AI collaborative learning: (1) the AI as an Adaptive Instrument, (2) the AI as a Proactive Assistant, (3) the AI as a Co-Learner, and (4) the AI as a Peer Collaborator. Grounded in sociocultural theories of learning and Computer-Supported Collaborative Learning (CSCL), this framework provides a structured vocabulary for analysing the shifting roles and responsibilities between human and AI agents. The paper further engages in a critical discussion of the philosophical underpinnings of collaboration, examining whether an AI, lacking genuine consciousness or shared intentionality, can be considered a true collaborator. We conclude that while AI may not achieve authentic phenomenological partnership, it can be designed as a highly effective functional collaborator. This distinction has significant implications for pedagogy, instructional design, and the future research agenda for AI in education, urging a shift in focus towards creating learning environments that harness the complementary strengths of both human and AI.",
    "url": "https://arxiv.org/abs/2508.14825",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores the evolving role of Artificial Intelligence (AI) in education, transitioning from a passive tool to an active participant in the learning process. The authors propose a new conceptual framework (the APCP framework) that outlines four levels of escalating AI agency in human-AI collaborative learning. The paper also discusses the philosophical implications of AI as a collaborator and suggests that while AI may not achieve genuine partnership, it can be designed as a functional collaborator with significant implications for pedagogy and instructional design."
  },
  {
    "title": "Challenges and Opportunities for Participatory Design of Conversational Agents for Young People's Wellbeing",
    "abstract": "This paper outlines the challenges and opportunities of research on conversational agents with children and young people across four countries, exploring the ways AI technologies can support children's well-being across social and cultural contexts.",
    "url": "https://arxiv.org/abs/2508.14787",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper discusses the challenges and opportunities of designing conversational agents for young people's well-being in different countries. The study highlights the potential of AI technologies to support children's well-being in various social and cultural settings, emphasizing the importance of participatory design in creating effective and culturally sensitive conversational agents for young people."
  },
  {
    "title": "Topology-Aware Volume Fusion for Spectral Computed Tomography via Histograms and Extremum Graph",
    "abstract": "Photon-Counting Computed Tomography (PCCT) is a novel imaging modality that simultaneously acquires volumetric data at multiple X-ray energy levels, generating separate volumes that capture energy-dependent attenuation properties. Attenuation refers to the reduction in X-ray intensity as it passes through different tissues or materials. This spectral information enhances tissue and material differentiation, enabling more accurate diagnosis and analysis. However, the resulting multivolume datasets are often complex and redundant, making visualization and interpretation challenging. To address these challenges, we propose a method for fusing spectral PCCT data into a single representative volume that enables direct volume rendering and segmentation by leveraging both shared and complementary information across different channels. Our approach starts by computing 2D histograms between pairs of volumes to identify those that exhibit prominent structural features. These histograms reveal relationships and variations that may be difficult to discern from individual volumes alone. Next, we construct an extremum graph from the 2D histogram of two minimally correlated yet complementary volumes-selected to capture both shared and distinct features-thereby maximizing the information content. The graph captures the topological distribution of histogram extrema. By extracting prominent structure within this graph and projecting each grid point in histogram space onto it, we reduce the dimensionality to one, producing a unified volume. This representative volume retains key structural and material characteristics from the original spectral data while significantly reducing the analysis scope from multiple volumes to one. The result is a topology-aware, information-rich fusion of multi-energy CT datasets that facilitates more effective visualization and segmentation.",
    "url": "https://arxiv.org/abs/2508.14719",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research focuses on fusing spectral Photon-Counting Computed Tomography (PCCT) data into a single representative volume to improve visualization and segmentation. By analyzing 2D histograms between pairs of volumes and constructing an extremum graph, the method identifies shared and complementary information across different channels to create a unified volume. This approach reduces the complexity of multivolume datasets while retaining key structural and material characteristics, ultimately enhancing the effectiveness of visualization and segmentation in PCCT imaging."
  },
  {
    "title": "Towards AI-based Sustainable and XR-based human-centric manufacturing: Implementation of ISO 23247 for digital twins of production systems",
    "abstract": "Since the introduction of Industry 4.0, digital twin technology has significantly evolved, laying the groundwork for a transition toward Industry 5.0 principles centered on human-centricity, sustainability, and resilience. Through digital twins, real-time connected production systems are anticipated to be more efficient, resilient, and sustainable, facilitating communication and connectivity between digital and physical systems. However, environmental performance and integration with virtual reality (VR) and artificial intelligence (AI) of such systems remain challenging. Further exploration of digital twin technologies is needed to validate the real-world impact and benefits. This paper investigates these challenges by implementing a real-time digital twin based on the ISO 23247 standard, connecting the physical factory and simulation software with VR capabilities. This digital twin system provides cognitive assistance and a user-friendly interface for operators, thereby improving cognitive ergonomics. The connection of the Internet of Things (IoT) platform allows the digital twin to have real-time bidirectional communication, collaboration, monitoring, and assistance. A lab-scale drone factory was used as the digital twin application to test and evaluate the ISO 23247 standard and its potential benefits. Additionally, AI integration and environmental performance Key Performance Indicators (KPIs) have been considered as the next stages in improving VR-integrated digital twins. With a solid theoretical foundation and a demonstration of the VR-integrated digital twins, this paper addresses integration issues between various technologies and advances the framework of digital twins based on ISO 23247.",
    "url": "https://arxiv.org/abs/2508.14580",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores the implementation of a real-time digital twin system based on the ISO 23247 standard, connecting physical factory systems with virtual reality (VR) capabilities to improve cognitive ergonomics for operators. The study demonstrates the potential benefits of integrating AI and environmental performance Key Performance Indicators (KPIs) in VR-integrated digital twins, paving the way for more efficient, resilient, and sustainable production systems in Industry 5.0. The findings highlight the importance of further exploring digital twin technologies to validate their real-world impact and advance the framework of digital twins in manufacturing."
  },
  {
    "title": "Detecting Reading-Induced Confusion Using EEG and Eye Tracking",
    "abstract": "Humans regularly navigate an overwhelming amount of information via text media, whether reading articles, browsing social media, or interacting with chatbots. Confusion naturally arises when new information conflicts with or exceeds a reader's comprehension or prior knowledge, posing a challenge for learning. In this study, we present a multimodal investigation of reading-induced confusion using EEG and eye tracking. We collected neural and gaze data from 11 adult participants as they read short paragraphs sampled from diverse, real-world sources. By isolating the N400 event-related potential (ERP), a well-established neural marker of semantic incongruence, and integrating behavioral markers from eye tracking, we provide a detailed analysis of the neural and behavioral correlates of confusion during naturalistic reading. Using machine learning, we show that multimodal (EEG + eye tracking) models improve classification accuracy by 4-22% over unimodal baselines, reaching an average weighted participant accuracy of 77.3% and a best accuracy of 89.6%. Our results highlight the dominance of the brain's temporal regions in these neural signatures of confusion, suggesting avenues for wearable, low-electrode brain-computer interfaces (BCI) for real-time monitoring. These findings lay the foundation for developing adaptive systems that dynamically detect and respond to user confusion, with potential applications in personalized learning, human-computer interaction, and accessibility.",
    "url": "https://arxiv.org/abs/2508.14442",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study investigates reading-induced confusion using EEG and eye tracking data. By analyzing the N400 event-related potential and integrating behavioral markers from eye tracking, the researchers were able to improve classification accuracy of confusion during naturalistic reading. The results suggest potential applications in personalized learning, human-computer interaction, and accessibility, laying the foundation for developing adaptive systems that can detect and respond to user confusion in real-time."
  },
  {
    "title": "NoteIt: A System Converting Instructional Videos to Interactable Notes Through Multimodal Video Understanding",
    "abstract": "Users often take notes for instructional videos to access key knowledge later without revisiting long videos. Automated note generation tools enable users to obtain informative notes efficiently. However, notes generated by existing research or off-the-shelf tools fail to preserve the information conveyed in the original videos comprehensively, nor can they satisfy users' expectations for diverse presentation formats and interactive features when using notes digitally. In this work, we present NoteIt, a system, which automatically converts instructional videos to interactable notes using a novel pipeline that faithfully extracts hierarchical structure and multimodal key information from videos. With NoteIt's interface, users can interact with the system to further customize the content and presentation formats of the notes according to their preferences. We conducted both a technical evaluation and a comparison user study (N=36). The solid performance in objective metrics and the positive user feedback demonstrated the effectiveness of the pipeline and the overall usability of NoteIt. Project website: this https URL",
    "url": "https://arxiv.org/abs/2508.14395",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces NoteIt, a system that converts instructional videos into interactive notes by extracting hierarchical structure and key information from the videos. The system allows users to customize the content and presentation formats of the notes according to their preferences. The study showed that NoteIt performs well in objective metrics and received positive feedback from users, demonstrating its effectiveness and usability in generating interactive notes from videos."
  },
  {
    "title": "Exploring Organizational Strategies in Immersive Computational Notebooks",
    "abstract": "Computational notebooks, which integrate code, documentation, tags, and visualizations into a single document, have become increasingly popular for data analysis tasks. With the advent of immersive technologies, these notebooks have evolved into a new paradigm, enabling more interactive and intuitive ways to perform data analysis. An immersive computational notebook, which integrates computational notebooks within an immersive environment, significantly enhances navigation performance with embodied interactions. However, despite recognizing the significance of organizational strategies in the immersive data science process, the organizational strategies for using immersive notebooks remain largely unexplored. In response, our research aims to deepen our understanding of organizations, especially focusing on spatial structures for computational notebooks, and to examine how various execution orders can be visualized in an immersive context. Through an exploratory user study, we found participants preferred organizing notebooks in half-cylindrical structures and engaged significantly more in non-linear analysis. Notably, as the scale of the notebooks increased (i.e., more code cells), users increasingly adopted multiple, concurrent non-linear analytical approaches.",
    "url": "https://arxiv.org/abs/2508.14346",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the organizational strategies for immersive computational notebooks, which integrate computational notebooks within an immersive environment for data analysis tasks. The study found that organizing notebooks in half-cylindrical structures and engaging in non-linear analysis were preferred by participants, especially as the scale of the notebooks increased. These findings highlight the importance of spatial structures and execution orders in enhancing navigation performance and user engagement in immersive data science processes."
  },
  {
    "title": "\"They Aren't Built For Me\": An Exploratory Study of Strategies for Measurement of Graphical Primitives in Tactile Graphics",
    "abstract": "Advancements in accessibility technologies such as low-cost swell form printers or refreshable tactile displays promise to allow blind or low-vision (BLV) people to analyze data by transforming visual representations directly to tactile representations. However, it is possible that design guidelines derived from experiments on the visual perception system may not be suited for the tactile perception system. We investigate the potential mismatch between familiar visual encodings and tactile perception in an exploratory study into the strategies employed by BLV people to measure common graphical primitives converted to tactile representations. First, we replicate the Cleveland and McGill study on graphical perception using swell form printing with eleven BLV subjects. Then, we present results from a group interview in which we describe the strategies used by our subjects to read four common chart types. While our results suggest that familiar encodings based on visual perception studies can be useful in tactile graphics, our subjects also expressed a desire to use encodings designed explicitly for BLV people. Based on this study, we identify gaps between the perceptual expectations of common charts and the perceptual tools available in tactile perception. Then, we present a set of guidelines for the design of tactile graphics that accounts for these gaps. Supplemental material is available at this https URL.",
    "url": "https://arxiv.org/abs/2508.14289",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the strategies used by blind or low-vision individuals to measure graphical primitives in tactile representations, highlighting a potential mismatch between visual and tactile perception systems. The study replicates a previous experiment on graphical perception using swell form printing with BLV subjects and identifies gaps in perceptual expectations between common charts and tactile tools. The findings suggest that while familiar visual encodings can be useful in tactile graphics, there is a need for encodings specifically designed for BLV individuals, leading to the development of guidelines for designing more effective tactile graphics."
  },
  {
    "title": "Using Real Names of Disabled Participant-Contributors to Practice Citational Justice in Accessibility",
    "abstract": "In accessibility research involving human subjects, researchers conventionally anonymize their research participants to protect privacy. However, a lack of intentionality about who to publicly acknowledge for intellectual contributions to research can lead to the erasure of disabled individuals' work and knowledge. In this paper, I propose identifying disabled research participants by name (with consent) as a practice of citational justice. I share observations from examples of this practice in accessible visualization research, and offer considerations for when it may be appropriate to de-anonymize. Intentional practices of citation offer researchers an opportunity to acknowledge the expertise and intellectual contributions of disabled people in our communities.",
    "url": "https://arxiv.org/abs/2508.14257",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper argues for the use of real names of disabled research participants in accessibility research to ensure their intellectual contributions are acknowledged and not erased. The practice of citational justice is proposed as a way to recognize the expertise and knowledge of disabled individuals in the research community. By intentionally citing and acknowledging disabled participants by name, researchers can promote inclusivity and give credit where it is due in the field of accessibility research."
  },
  {
    "title": "BioSonix: Can Physics-Based Sonification Perceptualize Tissue Deformations From Tool Interactions?",
    "abstract": "Perceptualizing tool interactions with deformable structures in surgical procedures remains challenging, as unimodal visualization techniques often fail to capture the complexity of these interactions due to constraints such as occlusion and limited depth perception. This paper presents a novel approach to augment tool navigation in mixed reality environments by providing auditory representations of tool-tissue dynamics, particularly for interactions with soft tissue. BioSonix, a physics-informed design framework, utilizes tissue displacements in 3D space to compute excitation forces for a sound model encoding tissue properties such as stiffness and density. Biomechanical simulations were employed to model particle displacements resulting from tool-tissue interactions, establishing a robust foundation for the method. An optimization approach was used to define configurations for capturing diverse interaction scenarios with varying tool trajectories. Experiments were conducted to validate the accuracy of the sound-displacement mappings. Additionally, two user studies were performed: the first involved two clinical professionals (a neuroradiologist and a cardiologist), who confirmed the method's impact and achieved high task accuracy; the second included 22 biomedical experts, who demonstrated high discrimination accuracy in tissue differentiation and targeting tasks. The results revealed a strong correlation between tool-tissue dynamics and their corresponding auditory profiles, highlighting the potential of these sound representations to enhance the intuitive understanding of complex interactions.",
    "url": "https://arxiv.org/abs/2508.14688",
    "journal": "arXiv cs.HC",
    "ai_summary": "This paper introduces BioSonix, a novel approach that uses auditory representations to perceptualize tool interactions with soft tissue in surgical procedures. By utilizing physics-informed design and biomechanical simulations, BioSonix accurately captures tool-tissue dynamics and provides sound profiles that enhance the intuitive understanding of complex interactions. The method was validated through experiments and user studies, demonstrating high task accuracy and discrimination accuracy in tissue differentiation and targeting tasks, indicating the potential of sound representations to augment tool navigation in mixed reality environments."
  },
  {
    "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs",
    "abstract": "Recent advances in large language models (LLMs) and reasoning frameworks have opened new possibilities for improving the perspective -taking capabilities of autonomous agents. However, tasks that involve active perception, collaborative reasoning, and perspective taking (understanding what another agent can see or knows) pose persistent challenges for current LLM-based systems. This study investigates the potential of structured examples derived from transformed solution graphs generated by the Fast Downward planner to improve the performance of LLM-based agents within a ReAct framework. We propose a structured solution-processing pipeline that generates three distinct categories of examples: optimal goal paths (G-type), informative node paths (E-type), and step-by-step optimal decision sequences contrasting alternative actions (L-type). These solutions are further converted into ``thought-action'' examples by prompting an LLM to explicitly articulate the reasoning behind each decision. While L-type examples slightly reduce clarification requests and overall action steps, they do not yield consistent improvements. Agents are successful in tasks requiring basic attentional filtering but struggle in scenarios that required mentalising about occluded spaces or weighing the costs of epistemic actions. These findings suggest that structured examples alone are insufficient for robust perspective-taking, underscoring the need for explicit belief tracking, cost modelling, and richer environments to enable socially grounded collaboration in LLM-based agents.",
    "url": "https://arxiv.org/abs/2508.14564",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores the use of structured examples derived from transformed solution graphs to improve the performance of LLM-based agents in tasks involving perspective-taking and reasoning. While the structured examples slightly reduce clarification requests and overall action steps, they do not consistently improve agent performance. The findings suggest that explicit belief tracking, cost modeling, and richer environments are necessary for robust perspective-taking and collaboration in LLM-based agents."
  },
  {
    "title": "Documenting Deployment with Fabric: A Repository of Real-World AI Governance",
    "abstract": "Artificial intelligence (AI) is increasingly integrated into society, from financial services and traffic management to creative writing. Academic literature on the deployment of AI has mostly focused on the risks and harms that result from the use of AI. We introduce Fabric, a publicly available repository of deployed AI use cases to outline their governance mechanisms. Through semi-structured interviews with practitioners, we collect an initial set of 20 AI use cases. In addition, we co-design diagrams of the AI workflow with the practitioners. We discuss the oversight mechanisms and guardrails used in practice to safeguard AI use. The Fabric repository includes visual diagrams of AI use cases and descriptions of the deployed systems. Using the repository, we surface gaps in governance and find common patterns in human oversight of deployed AI systems. We intend for Fabric to serve as an extendable, evolving tool for researchers to study the effectiveness of AI governance.",
    "url": "https://arxiv.org/abs/2508.14119",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Fabric, a repository of real-world AI use cases that highlights the governance mechanisms in place for these deployments. Through interviews with practitioners, the study identifies oversight mechanisms and guardrails used to safeguard AI systems. The repository aims to help researchers study the effectiveness of AI governance and identify common patterns in human oversight of deployed AI systems."
  },
  {
    "title": "Federated Action Recognition for Smart Worker Assistance Using FastPose",
    "abstract": "In smart manufacturing environments, accurate and real-time recognition of worker actions is essential for productivity, safety, and human-machine collaboration. While skeleton-based human activity recognition (HAR) offers robustness to lighting, viewpoint, and background variations, most existing approaches rely on centralized datasets, which are impractical in privacy-sensitive industrial scenarios. This paper presents a federated learning (FL) framework for pose-based HAR using a custom skeletal dataset of eight industrially relevant upper-body gestures, captured from five participants and processed using a modified FastPose model. Two temporal backbones, an LSTM and a Transformer encoder, are trained and evaluated under four paradigms: centralized, local (per-client), FL with weighted federated averaging (FedAvg), and federated ensemble learning (FedEnsemble). On the global test set, the FL Transformer improves over centralized training by +12.4 percentage points, with FedEnsemble delivering a +16.3 percentage points gain. On an unseen external client, FL and FedEnsemble exceed centralized accuracy by +52.6 and +58.3 percentage points, respectively. These results demonstrate that FL not only preserves privacy but also substantially enhances cross-user generalization, establishing it as a practical solution for scalable, privacy-aware HAR in heterogeneous industrial settings.",
    "url": "https://arxiv.org/abs/2508.14113",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper introduces a federated learning framework for pose-based human activity recognition in smart manufacturing environments, using a custom skeletal dataset and a modified FastPose model. The study compares different training paradigms and shows that federated learning improves accuracy over centralized training, with federated ensemble learning delivering the highest gains. The results suggest that federated learning not only protects privacy but also enhances cross-user generalization, making it a practical solution for scalable and privacy-aware human activity recognition in diverse industrial settings."
  },
  {
    "title": "Activity Coefficient-based Channel Selection for Electroencephalogram: A Task-Independent Approach",
    "abstract": "Electroencephalogram (EEG) signals have gained widespread adoption in brain-computer interface (BCI) applications due to their non-invasive, low-cost, and relatively simple acquisition process. The demand for higher spatial resolution, particularly in clinical settings, has led to the development of high-density electrode arrays. However, increasing the number of channels introduces challenges such as cross-channel interference and computational overhead. To address these issues, modern BCI systems often employ channel selection algorithms. Existing methods, however, are typically task-specific and require re-optimization for each new application. This work proposes a task-agnostic channel selection method, Activity Coefficient-based Channel Selection (ACCS), which uses a novel metric called the Channel Activity Coefficient (CAC) to quantify channel utility based on activity levels. By selecting the top 16 channels ranked by CAC, ACCS achieves up to 34.97% improvement in multi-class classification accuracy. Unlike traditional approaches, ACCS identifies a reusable set of informative channels independent of the downstream task or model, making it highly adaptable for diverse EEG-based applications.",
    "url": "https://arxiv.org/abs/2508.14060",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a novel task-agnostic channel selection method, Activity Coefficient-based Channel Selection (ACCS), for EEG signals in BCI applications. By using the Channel Activity Coefficient (CAC) metric to rank channel utility based on activity levels, ACCS can improve multi-class classification accuracy by up to 34.97% when selecting the top 16 channels. This approach is significant as it provides a reusable set of informative channels independent of the specific task or model, making it adaptable for various EEG-based applications."
  },
  {
    "title": "Learning to Use AI for Learning: How Can We Effectively Teach and Measure Prompting Literacy for K-12 Students?",
    "abstract": "As Artificial Intelligence (AI) becomes increasingly integrated into daily life, there is a growing need to equip the next generation with the ability to apply, interact with, evaluate, and collaborate with AI systems responsibly. Prior research highlights the urgent demand from K-12 educators to teach students the ethical and effective use of AI for learning. To address this need, we designed an Large-Language Model (LLM)-based module to teach prompting literacy. This includes scenario-based deliberate practice activities with direct interaction with intelligent LLM agents, aiming to foster secondary school students' responsible engagement with AI chatbots. We conducted two iterations of classroom deployment in 11 authentic secondary education classrooms, and evaluated 1) AI-based auto-grader's capability; 2) students' prompting performance and confidence changes towards using AI for learning; and 3) the quality of learning and assessment materials. Results indicated that the AI-based auto-grader could grade student-written prompts with satisfactory quality. In addition, the instructional materials supported students in improving their prompting skills through practice and led to positive shifts in their perceptions of using AI for learning. Furthermore, data from Study 1 informed assessment revisions in Study 2. Analyses of item difficulty and discrimination in Study 2 showed that True/False and open-ended questions could measure prompting literacy more effectively than multiple-choice questions for our target learners. These promising outcomes highlight the potential for broader deployment and highlight the need for broader studies to assess learning effectiveness and assessment design.",
    "url": "https://arxiv.org/abs/2508.13962",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on teaching K-12 students how to responsibly engage with AI systems, specifically through prompting literacy. The study implemented an AI-based module in secondary education classrooms, which showed that students were able to improve their prompting skills and confidence in using AI for learning. The results suggest that the instructional materials and assessment methods used were effective in promoting responsible engagement with AI, indicating the potential for broader deployment and further studies on learning effectiveness and assessment design."
  },
  {
    "title": "Prompt Orchestration Markup Language",
    "abstract": "Large Language Models (LLMs) require sophisticated prompting, yet current practices face challenges in structure, data integration, format sensitivity, and tooling. Existing methods lack comprehensive solutions for organizing complex prompts involving diverse data types (documents, tables, images) or managing presentation variations systematically. To address these gaps, we introduce POML (Prompt Orchestration Markup Language). POML employs component-based markup for logical structure (roles, tasks, examples), specialized tags for seamless data integration, and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration. We validate POML through two case studies demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA), as well as a user study assessing its effectiveness in real-world development scenarios.",
    "url": "https://arxiv.org/abs/2508.13948",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Prompt Orchestration Markup Language (POML) as a solution to challenges faced in structuring and integrating data for Large Language Models (LLMs). POML utilizes component-based markup, specialized tags for data integration, and a CSS-like styling system to improve prompt organization and reduce formatting sensitivity. The study validates POML through case studies showing its impact on application integration and accuracy performance, as well as a user study demonstrating its effectiveness in real-world development scenarios."
  },
  {
    "title": "LLM-Powered Virtual Patient Agents for Interactive Clinical Skills Training with Automated Feedback",
    "abstract": "Objective Structured Clinical Examinations (OSCEs) are essential for medical training, but they require significant resources, including professional actors and expert medical feedback. Although Large Language Models (LLMs) have introduced text-based virtual patients for communication practice, these simulations often lack the capability for richer, non-textual interactions. This paper presents a novel framework that significantly enhances LLM-based simulated patients by equipping them with action spaces, thereby enabling more realistic and dynamic patient behaviors that extend beyond text. Furthermore, our system incorporates virtual tutors that provide students with instant, personalized feedback on their performance at any time during these simulated encounters. We have conducted a rigorous evaluation of the framework's real-time performance, including system latency and component accuracy. Preliminary evaluations with medical experts assessed the naturalness and coherence of the simulated patients, as well as the usefulness and appropriateness of the virtual tutor's assessments. This innovative system provides medical students with a low-cost, accessible platform for personalized OSCE preparation at home.",
    "url": "https://arxiv.org/abs/2508.13943",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces a novel framework that enhances text-based virtual patient simulations by adding action spaces for more realistic interactions. The system also includes virtual tutors that provide instant feedback to medical students during simulated encounters. Preliminary evaluations show that this innovative system offers a low-cost and accessible platform for personalized OSCE preparation at home, potentially reducing the need for professional actors and expert feedback in medical training."
  },
  {
    "title": "Mind & Motion: Opportunities and Applications of Integrating Biomechanics and Cognitive Models in HCI",
    "abstract": "Computational models of how users perceive and act within a virtual or physical environment offer enormous potential for the understanding and design of user interactions. Cognition models have been used to understand the role of attention and individual preferences and beliefs on human decision making during interaction, while biomechanical simulations have been successfully applied to analyse and predict physical effort, fatigue, and discomfort. The next frontier in HCI lies in connecting these models to enable robust, diverse, and representative simulations of different user groups. These embodied user simulations could predict user intents, strategies, and movements during interaction more accurately, benchmark interfaces and interaction techniques in terms of performance and ergonomics, and guide adaptive system design. This UIST workshop explores ideas for integrating computational models into HCI and discusses use cases such as UI/UX design, automated system testing, and personalised adaptive interfaces. It brings researchers from relevant disciplines together to identify key opportunities and challenges as well as feasible next steps for bridging mind and motion to simulate interactive user behaviour.",
    "url": "https://arxiv.org/abs/2508.13788",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the potential of integrating biomechanics and cognitive models in HCI to enhance user interactions. By connecting cognition models with biomechanical simulations, researchers can predict user intents, strategies, and movements more accurately, benchmark interfaces for performance and ergonomics, and guide adaptive system design. This integration could lead to more robust and representative simulations of different user groups, offering opportunities for UI/UX design, automated system testing, and personalized adaptive interfaces."
  },
  {
    "title": "Bend It, Aim It, Tap It: Designing an On-Body Disambiguation Mechanism for Curve Selection in Mixed Reality",
    "abstract": "Object selection in Mixed Reality (MR) becomes particularly challenging in dense or occluded environments, where traditional mid-air ray-casting often leads to ambiguity and reduced precision. We present two complementary techniques: (1) a real-time Bezier Curve selection paradigm guided by finger curvature, enabling expressive one-handed trajectories, and (2) an on-body disambiguation mechanism that projects the four nearest candidates onto the user's forearm via proximity-based mapping. Together, these techniques combine flexible, user-controlled selection with tactile, proprioceptive disambiguation. We evaluated their independent and joint effects in a 2x2 within-subjects study (N = 24), crossing interaction paradigm (Bezier Curve vs. Linear Ray) with interaction medium (Mid-air vs. On-body). Results show that on-body disambiguation significantly reduced selection errors and physical demand while improving perceived performance, hedonic quality, and user preference. Bezier input provided effective access to occluded targets but incurred longer task times and greater effort under some conditions. We conclude with design implications for integrating curved input and on-body previews to support precise, adaptive selection in immersive environments.",
    "url": "https://arxiv.org/abs/2508.13748",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces two techniques for improving object selection in Mixed Reality (MR) environments: a Bezier Curve selection paradigm guided by finger curvature and an on-body disambiguation mechanism that projects candidate selections onto the user's forearm. The study found that the on-body disambiguation significantly reduced errors and physical demand while improving perceived performance and user preference. The findings suggest that integrating curved input and on-body previews can enhance precise and adaptive selection in immersive environments."
  },
  {
    "title": "`My Dataset of Love': A Preliminary Mixed-Method Exploration of Human-AI Romantic Relationships",
    "abstract": "Human-AI romantic relationships have gained wide popularity among social media users in China. The technological impact on romantic relationships and its potential applications have long drawn research attention to topics such as relationship preservation and negativity mitigation. Media and communication studies also explore the practices in romantic para-social relationships. Nonetheless, this emerging human-AI romantic relationship, whether the relations fall into the category of para-social relationship together with its navigation pattern, remains unexplored, particularly in the context of relational stages and emotional attachment. This research thus seeks to fill this gap by presenting a mixed-method approach on 1,766 posts and 60,925 comments from Xiaohongshu, as well as the semi-structured interviews with 23 participants, of whom one of them developed her relationship with self-created AI for three years. The findings revealed that the users' willingness to self-disclose to AI companions led to increased positivity without social stigma. The results also unveiled the reciprocal nature of these interactions, the dominance of 'self', and raised concerns about language misuse, bias, and data security in AI communication.",
    "url": "https://arxiv.org/abs/2508.13655",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the emerging phenomenon of human-AI romantic relationships, particularly in the context of relational stages and emotional attachment. Through a mixed-method approach involving analysis of social media posts and comments, as well as interviews with participants, the study found that users' willingness to self-disclose to AI companions led to increased positivity without social stigma. The findings also highlighted concerns about language misuse, bias, and data security in AI communication, shedding light on the complexities of these relationships."
  }
]