# arXiv cs.AI Summary â€“ 2026-02-23

## AI-Wrapped: Participatory, Privacy-Preserving Measurement of Longitudinal LLM Use In-the-Wild
**URL:** https://arxiv.org/abs/2602.18415

**Abstract:** Alignment research on large language models (LLMs) increasingly depends on understanding how these systems are used in everyday contexts. yet naturalistic interaction data is difficult to access due to privacy constraints and platform control. We present AI-Wrapped, a prototype workflow for collecting naturalistic LLM usage data while providing participants with an immediate ``wrapped''-style report on their usage statistics, top topics, and safety-relevant behavioral patterns. We report findings from an initial deployment with 82 U.S.-based adults across 48,495 conversations from their 2025 histories. Participants used LLMs for both instrumental and reflective purposes, including creative work, professional tasks, and emotional or existential themes. Some usage patterns were consistent with potential over-reliance or perfectionistic refinement, while heavier users showed comparatively more reflective exchanges than primarily transactional ones. Methodologically, even with zero data retention and PII removal, participants may remain hesitant to share chat data due to perceived privacy and judgment risks, underscoring the importance of trust, agency, and transparent design when building measurement infrastructure for alignment research.

**AI Summary:** The study introduces AI-Wrapped, a method for collecting naturalistic data on the use of large language models (LLMs) while providing participants with immediate feedback on their usage statistics. The findings suggest that participants use LLMs for various purposes, including creative work, professional tasks, and emotional themes, with some showing potential over-reliance or perfectionistic behavior. The study highlights the importance of trust, agency, and transparent design in building measurement infrastructure for alignment research, as participants may be hesitant to share data due to privacy concerns.

---

## "How Do I ...?": Procedural Questions Predominate Student-LLM Chatbot Conversations
**URL:** https://arxiv.org/abs/2602.18372

**Abstract:** Providing scaffolding through educational chatbots built on Large Language Models (LLM) has potential risks and benefits that remain an open area of research. When students navigate impasses, they ask for help by formulating impasse-driven questions. Within interactions with LLM chatbots, such questions shape the user prompts and drive the pedagogical effectiveness of the chatbot's response. This paper focuses on such student questions from two datasets of distinct learning contexts: formative self-study, and summative assessed coursework. We analysed 6,113 messages from both learning contexts, using 11 different LLMs and three human raters to classify student questions using four existing schemas. On the feasibility of using LLMs as raters, results showed moderate-to-good inter-rater reliability, with higher consistency than human raters. The data showed that 'procedural' questions predominated in both learning contexts, but more so when students prepare for summative assessment. These results provide a basis on which to use LLMs for classification of student questions. However, we identify clear limitations in both the ability to classify with schemas and the value of doing so: schemas are limited and thus struggle to accommodate the semantic richness of composite prompts, offering only partial understanding the wider risks and benefits of chatbot integration. In the future, we recommend an analysis approach that captures the nuanced, multi-turn nature of conversation, for example, by applying methods from conversation analysis in discursive psychology.

**AI Summary:** This research paper explores how students ask for help from educational chatbots built on Large Language Models (LLMs) by analyzing over 6,000 messages from two different learning contexts. The study found that 'procedural' questions were the most common type of questions asked by students, especially when preparing for summative assessments. The results suggest the potential of using LLMs to classify student questions, but also highlight limitations in current classification schemas and the need for more nuanced analysis methods in future research.

---

## Qualitative Coding Analysis through Open-Source Large Language Models: A User Study and Design Recommendations
**URL:** https://arxiv.org/abs/2602.18352

**Abstract:** Qualitative data analysis is labor-intensive, yet the privacy risks associated with commercial Large Language Models (LLMs) often preclude their use in sensitive research. To address this, we introduce ChatQDA, an on-device framework powered by open-source LLMs designed for privacy-preserving open coding. Our mixed-methods user study reveals that while participants rated the system highly for usability and perceived efficiency, they exhibited "conditional trust", valuing the tool for surface-level extraction while questioning its interpretive nuance and consistency. Furthermore, despite the technical security of local deployment, participants reported epistemic uncertainty regarding data protection, suggesting that invisible security measures are insufficient to foster trust. We conclude with design recommendations for local-first analysis tools that prioritize verifiable privacy and methodological rigor.

**AI Summary:** The study introduces ChatQDA, an on-device framework using open-source Large Language Models for qualitative data analysis. Participants in the user study rated the system highly for usability and efficiency but had "conditional trust" in its interpretive nuance and consistency. The findings highlight the importance of verifiable privacy and methodological rigor in the design of local-first analysis tools.

---

## Aurora: Neuro-Symbolic AI Driven Advising Agent
**URL:** https://arxiv.org/abs/2602.17999

**Abstract:** Academic advising in higher education is under severe strain, with advisor-to-student ratios commonly exceeding 300:1. These structural bottlenecks limit timely access to guidance, increase the risk of delayed graduation, and contribute to inequities in student support. We introduce Aurora, a modular neuro-symbolic advising agent that unifies retrieval-augmented generation (RAG), symbolic reasoning, and normalized curricular databases to deliver policy-compliant, verifiable recommendations at scale. Aurora integrates three components: (i) a Boyce-Codd Normal Form (BCNF) catalog schema for consistent program rules, (ii) a Prolog engine for prerequisite and credit enforcement, and (iii) an instruction-tuned large language model for natural-language explanations of its recommendations. To assess performance, we design a structured evaluation suite spanning common and edge-case advising scenarios, including short-term scheduling, long-term roadmapping, skill-aligned pathways, and out-of-scope requests. Across this diverse set, Aurora improves semantic alignment with expert-crafted answers from 0.68 (Raw LLM baseline) to 0.93 (+36%), achieves perfect precision and recall in nearly half of in-scope cases, and consistently produces correct fallbacks for unanswerable prompts. On commodity hardware, Aurora delivers sub-second mean latency (0.71s across 20 queries), approximately 83X faster than a Raw LLM baseline (59.2s). By combining symbolic rigor with neural fluency, Aurora advances a paradigm for accurate, explainable, and scalable AI-driven advising.

**AI Summary:** The research introduces Aurora, a neuro-symbolic advising agent designed to address the challenges of academic advising in higher education, such as high advisor-to-student ratios and delayed graduation. Aurora combines retrieval-augmented generation (RAG), symbolic reasoning, and normalized curricular databases to provide policy-compliant recommendations at scale. The agent performs well in various advising scenarios, improving semantic alignment with expert-crafted answers, achieving high precision and recall, and delivering fast response times compared to baseline models, highlighting its potential to enhance accurate, explainable, and scalable AI-driven advising.

---

## DuoTouch: Passive Two-Footprint Attachments Using Binary Sequences to Extend Touch Interaction
**URL:** https://arxiv.org/abs/2602.17961

**Abstract:** DuoTouch is a passive attachment for capacitive touch panels that adds tangible input while minimizing content occlusion and loss of input area. It uses two contact footprints and two traces to encode motion as binary sequences and runs on unmodified devices through standard touch APIs. We present two configurations with paired decoders: an aligned configuration that maps fixed-length codes to discrete commands and a phase-shifted configuration that estimates direction and distance from relative timing. To characterize the system's reliability, we derive a sampling-limited bound that links actuation speed, internal trace width, and device touch sampling rate. Through technical evaluations on a smartphone and a touchpad, we report performance metrics that describe the relationship between these parameters and decoding accuracy. Finally, we demonstrate the versatility of DuoTouch by embedding the mechanism into various form factors, including a hand strap, a phone ring holder, and touchpad add-ons.

**AI Summary:** DuoTouch is a passive attachment for touch panels that enhances touch interaction by using binary sequences to encode motion. The system offers two configurations for decoding commands and estimating direction and distance, and can be integrated into various form factors. Technical evaluations on smartphones and touchpads show the system's reliability and performance metrics, highlighting its potential for extending touch interaction capabilities.

---

## How Well Can 3D Accessibility Guidelines Support XR Development? An Interview Study with XR Practitioners in Industry
**URL:** https://arxiv.org/abs/2602.17939

**Abstract:** While accessibility (a11y) guidelines exist for 3D games and virtual worlds, their applicability to extended reality (XR)'s unique interaction paradigms (e.g., spatial tracking, kinesthetic interactions) remains unexplored. XR practitioners need practical guidance to successfully implement a11y guidelines under real-world constraints. We present the first evaluation of existing 3D a11y guidelines applied to XR development through semi-structured interviews with 25 XR practitioners across diverse organization contexts. We assessed 20 commonly-agreed a11y guidelines from six major resources across visual, motor, cognitive, speech, and hearing domains, comparing practitioners' development practices against guideline applicability to XR. Our investigation reveals that guidelines can be highly effective when designed as transformation catalysts rather than compliance checklists, but fundamental mismatches exist between existing 3D guidelines and XR requirements, creating both implementation barriers and design gaps. This work provides foundational insights towards developing a11y guidelines and support tools that address XR's distinct characteristics.

**AI Summary:** This research evaluates the applicability of existing 3D accessibility guidelines to extended reality (XR) development through interviews with 25 XR practitioners. The study finds that while guidelines can be effective as transformation catalysts, there are fundamental mismatches between existing guidelines and XR requirements, leading to implementation barriers and design gaps. The findings highlight the need for developing new accessibility guidelines and support tools that address the unique characteristics of XR technology.

---

## Growing With the Condition: Co-Designing Pediatric Technologies that Adapt Across Developmental Stages
**URL:** https://arxiv.org/abs/2602.17925

**Abstract:** Children with chronic conditions face evolving challenges in daily activities, peer relationships, and clinical care. Younger children often rely on parental support, while older ones seek independence. Prior studies on chronic conditions explored proxy-based, family-centered, and playful approaches to support children's health, but most technologies treat children as a homogeneous group rather than adapting to their developmental differences. To address this gap, we conducted four co-design workshops with 69 children with congenital heart disease (CHD) at a medically supported camp, spanning elementary, middle, and high school groups. Our analysis reveals distinct coping strategies: elementary children relied on comfort objects and reassurance, middle schoolers used mediated communication and selective disclosure, and high schoolers emphasized agency and direct engagement with peers and providers. Through child-centered participatory design, we contribute empirical insights into how children's management of chronic conditions evolves and propose design implications for pediatric health technologies that adapt across developmental trajectories.

**AI Summary:** This research explores how children with chronic conditions, specifically congenital heart disease, adapt and cope with their condition as they grow older. The study found that younger children rely on parental support, while older children seek independence and direct engagement with peers and providers. By conducting co-design workshops with children at different developmental stages, the researchers highlight the importance of designing pediatric health technologies that can adapt to the evolving needs of children with chronic conditions.

---

## Games That Teach, Chats That Convince: Comparing Interactive and Static Formats for Persuasive Learning
**URL:** https://arxiv.org/abs/2602.17905

**Abstract:** Interactive systems such as chatbots and games are increasingly used to persuade and educate on sustainability-related topics, yet it remains unclear how different delivery formats shape learning and persuasive outcomes when content is held constant. Grounding on identical arguments and factual content across conditions, we present a controlled user study comparing three modes of information delivery: static essays, conversational chatbots, and narrative text-based games. Across subjective measures, the chatbot condition consistently outperformed the other modes and increased perceived importance of the topic. However, perceived learning did not reliably align with objective outcomes: participants in the text-based game condition reported learning less than those reading essays, yet achieved higher scores on a delayed (24-hour) knowledge quiz. Additional exploratory analyses further suggest that common engagement proxies, such as verbosity and interaction length, are more closely related to subjective experience than to actual learning. These findings highlight a dissociation between how persuasive experiences feel and what participants retain, and point to important design trade-offs between interactivity, realism, and learning in persuasive systems and serious games.

**AI Summary:** This research compares different modes of information delivery (static essays, conversational chatbots, and narrative text-based games) for persuasive learning on sustainability-related topics. The study found that chatbots were the most effective in increasing perceived importance of the topic, but objective learning outcomes did not always align with subjective perceptions. The findings emphasize the importance of considering design trade-offs between interactivity, realism, and learning in persuasive systems and serious games.

---

## HookLens: Visual Analytics for Understanding React Hooks Structures
**URL:** https://arxiv.org/abs/2602.17891

**Abstract:** Maintaining and refactoring React web applications is challenging, as React code often becomes complex due to its core API called Hooks. For example, Hooks often lead developers to create complex dependencies among components, making code behavior unpredictable and reducing maintainability, i.e., anti-patterns. To address this challenge, we present HookLens, an interactive visual analytics system that helps developers understand howHooks define dependencies and data flows between components. Informed by an iterative design process with experienced React developers, HookLens supports users to efficiently understand the structure and dependencies between components and to identify anti-patterns. A quantitative user study with 12 React developers demonstrates that HookLens significantly improves participants' accuracy in detecting anti-patterns compared to conventional code editors. Moreover, a comparative study with state-of-the-art LLM-based coding assistants confirms that these improvements even surpass the capabilities of such coding assistants on the same task.

**AI Summary:** The research introduces HookLens, a visual analytics system designed to help developers understand the complex dependencies and data flows created by React Hooks in web applications. Through an iterative design process and user study, it was found that HookLens significantly improves developers' ability to detect anti-patterns compared to traditional code editors and even outperforms state-of-the-art coding assistants. This research is significant as it provides a valuable tool for developers to maintain and refactor React code effectively, ultimately improving code quality and maintainability.

---

## Exploring The Impact Of Proactive Generative AI Agent Roles In Time-Sensitive Collaborative Problem-Solving Tasks
**URL:** https://arxiv.org/abs/2602.17864

**Abstract:** Collaborative problem-solving under time pressure is common but difficult, as teams must generate ideas quickly, coordinate actions, and track progress. Generative AI offers new opportunities to assist, but we know little about how proactive agents affect the dynamics of real-time, co-located teamwork. We studied two forms of proactive support in digital escape rooms: a facilitator agent that offered summaries and group structures, and a peer agent that proposed ideas and answered queries. In a within-subjects study with 24 participants, we compared group performance and processes across three conditions: no AI, peer, and facilitator. Results show that the peer agent occasionally enhanced problem-solving by offering timely hints and memory support; however, it also disrupted flow, increased workload, and created over-reliance. In comparison, the facilitator agent provided light scaffolding but had a limited impact on outcomes. We provide design considerations for proactive generative AI agents based on our findings.

**AI Summary:** This research explores the impact of proactive generative AI agents in time-sensitive collaborative problem-solving tasks. The study found that a peer agent offering hints and memory support occasionally enhanced problem-solving but also disrupted flow and increased workload. In comparison, a facilitator agent providing light scaffolding had a limited impact on outcomes. These findings offer valuable insights for designing proactive AI agents to assist in real-time teamwork.

---

## Mind the Style: Impact of Communication Style on Human-Chatbot Interaction
**URL:** https://arxiv.org/abs/2602.17850

**Abstract:** Conversational agents increasingly mediate everyday digital interactions, yet the effects of their communication style on user experience and task success remain unclear. Addressing this gap, we describe the results of a between-subject user study where participants interact with one of two versions of a chatbot called NAVI which assists users in an interactive map-based 2D navigation task. The two chatbot versions differ only in communication style: one is friendly and supportive, while the other is direct and task-focused. Our results show that the friendly style increases subjective satisfaction and significantly improves task completion rates among female participants only, while no baseline differences between female and male participants were observed in a control condition without the chatbot. Furthermore, we find little evidence of users mimicking the chatbot's style, suggesting limited linguistic accommodation. These findings highlight the importance of user- and task-sensitive conversational agents and support that communication style personalization can meaningfully enhance interaction quality and performance.

**AI Summary:** This research study explores the impact of communication style on user experience and task success in human-chatbot interactions. The results show that a friendly and supportive communication style improves subjective satisfaction and task completion rates, particularly among female participants. The findings emphasize the importance of personalized communication styles in conversational agents to enhance interaction quality and performance.

---

## Lost Before Translation: Social Information Transmission and Survival in AI-AI Communication
**URL:** https://arxiv.org/abs/2602.17674

**Abstract:** When AI systems summarize and relay information, they inevitably transform it. But how? We introduce an experimental paradigm based on the telephone game to study what happens when AI talks to AI. Across five studies tracking content through AI transmission chains, we find three consistent patterns. The first is convergence, where texts differing in certainty, emotional intensity, and perspectival balance collapse toward a shared default of moderate confidence, muted affect, and analytical structure. The second is selective survival, where narrative anchors persist while the texture of evidence, hedges, quotes, and attributions is stripped away. The third is competitive filtering, where strong arguments survive while weaker but valid considerations disappear when multiple viewpoints coexist. In downstream experiments, human participants rated AI-transmitted content as more credible and polished. Importantly, however, humans also showed degraded factual recall, reduced perception of balance, and diminished emotional resonance. We show that the properties that make AI-mediated content appear authoritative may systematically erode the cognitive and affective diversity on which informed judgment depends.

**AI Summary:** This research explores how AI systems transform information when communicating with each other, finding that AI-mediated content tends to converge towards a default of moderate confidence, muted affect, and analytical structure. Additionally, the study shows that certain elements of the original information, such as evidence and attributions, are selectively stripped away during AI transmission. While human participants rated AI-transmitted content as more credible and polished, they also experienced degraded factual recall, reduced perception of balance, and diminished emotional resonance, highlighting the potential negative impact of AI-mediated communication on informed judgment.

---

## Digital self-Efficacy as a foundation for a generative AI usage framework in faculty's professional practices
**URL:** https://arxiv.org/abs/2602.17673

**Abstract:** This research explores the role of digital self-efficacy in the appropriation of generative artificial intelligence (GAI) by higher education faculty. Drawing on Bandura's sociocognitive theory and Flichy's concept of usage framework, our study examines the relationships between levels of digital self-efficacy and GAI usage profiles. A survey of 265 faculty members identified three user profiles (Engaged, Reflective Reserved, Critical Resisters) and validated a three-dimensional digital self-efficacy scale. Results reveal a significant association between self-efficacy profiles and GAI appropriation patterns. Based on these findings, we propose a differentiated usage framework integrating four sociotechnical configurations, appropriation trajectories adapted to self-efficacy profiles, and personalized institutional support mechanisms.

**AI Summary:** This research investigates the impact of digital self-efficacy on the adoption of generative artificial intelligence (GAI) by higher education faculty. By identifying three user profiles and validating a digital self-efficacy scale, the study found a significant relationship between self-efficacy levels and GAI usage patterns. The findings suggest the importance of tailored support mechanisms and appropriation trajectories based on individual self-efficacy profiles in promoting the effective integration of GAI in professional practices.

---

## Assessing LLM Response Quality in the Context of Technology-Facilitated Abuse
**URL:** https://arxiv.org/abs/2602.17672

**Abstract:** Technology-facilitated abuse (TFA) is a pervasive form of intimate partner violence (IPV) that leverages digital tools to control, surveil, or harm survivors. While tech clinics are one of the reliable sources of support for TFA survivors, they face limitations due to staffing constraints and logistical barriers. As a result, many survivors turn to online resources for assistance. With the growing accessibility and popularity of large language models (LLMs), and increasing interest from IPV organizations, survivors may begin to consult LLM-based chatbots before seeking help from tech clinics.
In this work, we present the first expert-led manual evaluation of four LLMs - two widely used general-purpose non-reasoning models and two domain-specific models designed for IPV contexts - focused on their effectiveness in responding to TFA-related questions. Using real-world questions collected from literature and online forums, we assess the quality of zero-shot single-turn LLM responses generated with a survivor safety-centered prompt on criteria tailored to the TFA domain. Additionally, we conducted a user study to evaluate the perceived actionability of these responses from the perspective of individuals who have experienced TFA.
Our findings, grounded in both expert assessment and user feedback, provide insights into the current capabilities and limitations of LLMs in the TFA context and may inform the design, development, and fine-tuning of future models for this domain. We conclude with concrete recommendations to improve LLM performance for survivor support.

**AI Summary:** This research evaluates the effectiveness of large language models (LLMs) in responding to questions related to technology-facilitated abuse (TFA) in intimate partner violence (IPV) contexts. The study compares general-purpose and domain-specific LLMs in providing survivor safety-centered responses to TFA-related questions, using both expert assessment and user feedback. The findings highlight the potential of LLMs in supporting survivors of TFA and provide recommendations for improving LLM performance in this critical domain.

---

## AI Hallucination from Students' Perspective: A Thematic Analysis
**URL:** https://arxiv.org/abs/2602.17671

**Abstract:** As students increasingly rely on large language models, hallucinations pose a growing threat to learning. To mitigate this, AI literacy must expand beyond prompt engineering to address how students should detect and respond to LLM hallucinations. To support this, we need to understand how students experience hallucinations, how they detect them, and why they believe they occur. To investigate these questions, we asked university students three open-ended questions about their experiences with AI hallucinations, their detection strategies, and their mental models of why hallucinations occur. Sixty-three students responded to the survey. Thematic analysis of their responses revealed that reported hallucination issues primarily relate to incorrect or fabricated citations, false information, overconfident but misleading responses, poor adherence to prompts, persistence in incorrect answers, and sycophancy. To detect hallucinations, students rely either on intuitive judgment or on active verification strategies, such as cross-checking with external sources or re-prompting the model. Students' explanations for why hallucinations occur reflected several mental models, including notable misconceptions. Many described AI as a research engine that fabricates information when it cannot locate an answer in its "database." Others attributed hallucinations to issues with training data, inadequate prompting, or the model's inability to understand or verify information. These findings illuminate vulnerabilities in AI-supported learning and highlight the need for explicit instruction in verification protocols, accurate mental models of generative AI, and awareness of behaviors such as sycophancy and confident delivery that obscure inaccuracy. The study contributes empirical evidence for integrating hallucination awareness and mitigation into AI literacy curricula.

**AI Summary:** The study explores how university students experience and detect hallucinations in large language models (LLMs) used for learning. Findings show that students encounter issues such as incorrect citations, false information, and misleading responses from LLMs. Students use intuitive judgment or verification strategies to detect hallucinations and have various misconceptions about why they occur, including viewing AI as a research engine that fabricates information. The study highlights the need for explicit instruction in verification protocols and accurate mental models of generative AI to address vulnerabilities in AI-supported learning.

---

## The Dark Side of Dark Mode -- User behaviour rebound effects and consequences for digital energy consumption
**URL:** https://arxiv.org/abs/2602.17670

**Abstract:** User devices are the largest contributor to media related global emissions. For web content, dark mode has been widely recommended as an energy-saving measure for certain display types. However, the energy savings achieved by dark mode may be undermined by user behaviour. This pilot study investigates the unintended consequences of dark mode adoption, revealing a rebound effect wherein users may increase display brightness when interacting with dark-themed web pages. This behaviour may negate the potential energy savings that dark mode offers. Our findings suggest that the energy efficiency benefits of dark mode are not as straightforward as commonly believed for display energy, and the interplay between content colourscheme and user behaviour must be carefully considered in sustainability guidelines and interventions.

**AI Summary:** This pilot study explores the unintended consequences of dark mode adoption, revealing a rebound effect where users may increase display brightness when interacting with dark-themed web pages, potentially negating the energy savings offered by dark mode. The findings suggest that the energy efficiency benefits of dark mode are not as straightforward as commonly believed, emphasizing the importance of considering the interplay between content colorscheme and user behavior in sustainability guidelines and interventions for digital energy consumption.

---

## Evaluating Text-based Conversational Agents for Mental Health: A Systematic Review of Metrics, Methods and Usage Contexts
**URL:** https://arxiv.org/abs/2602.17669

**Abstract:** Text-based conversational agents (CAs) are increasingly used in mental health, yet evaluation practices remain fragmented. We conducted a PRISMA-guided systematic review (May-June 2024) across ACM Digital Library, Scopus, and PsycINFO. From 613 records, 132 studies were included, with dual-coder extraction achieving substantial agreement (Cohen's kappa = 0.77-0.92). We synthesized evaluation approaches across three dimensions: metrics, methods, and usage contexts. Metrics were classified into CA-centric attributes (e.g., reliability, safety, empathy) and user-centric outcomes (experience, knowledge, psychological state, health behavior). Methods included automated analyses, standardized psychometric scales, and qualitative inquiry. Temporal designs ranged from momentary to follow-up assessments. Findings show reliance on Western-developed scales, limited cultural adaptation, predominance of small and short-term samples, and weak links between automated performance metrics and user well-being. We argue for methodological triangulation, temporal rigor, and equity in measurement. This review offers a structured foundation for reliable, safe, and user-centered evaluation of mental health CAs.

**AI Summary:** This systematic review evaluated the use of text-based conversational agents (CAs) in mental health, highlighting fragmented evaluation practices. The study found that there is a reliance on Western-developed scales, limited cultural adaptation, and weak links between automated performance metrics and user well-being. The review emphasizes the need for methodological triangulation, temporal rigor, and equity in measurement to ensure reliable, safe, and user-centered evaluation of mental health CAs.

---

## Visual Interface Workflow Management System Strengthening Data Integrity and Project Tracking in Complex Processes
**URL:** https://arxiv.org/abs/2602.17668

**Abstract:** Manual notes and scattered messaging applications used in managing business processes compromise data integrity and abstract project tracking. In this study, an integrated system that works simultaneously on web and mobile platforms has been developed to enable individual users and teams to manage their workflows with concrete data. The system architecture integrates MongoDB, which stores data in JSON format, this http URL this http URL on the server side, this http URL on the web interface, and React Native technologies on the mobile side. The system interface is designed around visual dashboards that track the status of tasks (To Do-In Progress-Done). The urgency of tasks is distinguished by color-coded labels, and dynamic graphics (Dashboard) have been created for managers to monitor team performance. The usability of the system was tested with a heterogeneous group of 10 people consisting of engineers, engineering students, public employees, branch managers, and healthcare personnel. In analyses conducted using a 5-point Likert scale, the organizational efficiency provided by the system compared to traditional methods was rated 4.90, while the visual dashboards achieved a perfect score of 5.00 with zero variance. Additionally, the ease of interface use was rated 4.65, and overall user satisfaction was calculated as 4.60. The findings show that the developed system simplifies complex work processes and provides a traceable digital working environment for Small and Medium-sized Enterprises and project teams.

**AI Summary:** This research study developed an integrated system for managing workflows on web and mobile platforms, using MongoDB and React Native technologies. The system features visual dashboards for tracking task status and performance monitoring, which received high ratings for organizational efficiency and user satisfaction. The findings suggest that the system simplifies complex work processes and provides a traceable digital working environment for Small and Medium-sized Enterprises and project teams.

---

## Robo-Saber: Generating and Simulating Virtual Reality Players
**URL:** https://arxiv.org/abs/2602.18319

**Abstract:** We present the first motion generation system for playtesting virtual reality (VR) games. Our player model generates VR headset and handheld controller movements from in-game object arrangements, guided by style exemplars and aligned to maximize simulated gameplay score. We train on the large BOXRR-23 dataset and apply our framework on the popular VR game Beat Saber. The resulting model Robo-Saber produces skilled gameplay and captures diverse player behaviors, mirroring the skill levels and movement patterns specified by input style exemplars. Robo-Saber demonstrates promise in synthesizing rich gameplay data for predictive applications and enabling a physics-based whole-body VR playtesting agent.

**AI Summary:** The research introduces Robo-Saber, a motion generation system for playtesting VR games that generates movements based on in-game object arrangements and style exemplars. The model, trained on a large dataset, successfully produces skilled gameplay and diverse player behaviors in the popular VR game Beat Saber. This framework shows potential for generating rich gameplay data for predictive applications and creating a physics-based VR playtesting agent.

---

## Visual Anthropomorphism Shifts Evaluations of Gendered AI Managers
**URL:** https://arxiv.org/abs/2602.17919

**Abstract:** This research examines whether competence cues can reduce gender bias in evaluations of AI managers and whether these effects depend on how the AI is represented. Across two preregistered experiments (N = 2,505), each employing a 2 x 2 x 3 design manipulating AI gender, competence, and decision outcome, we compared text-based descriptions of AI managers with visually generated AI faces created using a reverse-correlation paradigm. In the text condition, evaluations were driven by competence rather than gender. When participants received unfavourable decisions, high-competence AI managers were judged as fairer, more competent, and better leaders than low-competence managers, regardless of AI gender. In contrast, when the AI manager was visually represented, competence cues had attenuated influence once facial information was present. Instead, participants showed systematic gender-differentiated responses to AI faces, with feminine-appearing managers evaluated as more competent and more trustworthy than masculine-appearing managers, particularly when delivering favourable outcomes. These gender effects were largely absent when outcomes were unfavourable, suggesting that negative feedback attenuates the influence of both competence information and facial cues. Taken together, these findings show that competence information can mitigate negative reactions to AI managers in text-based interactions, whereas facial anthropomorphism elicits gendered perceptual biases not observed in text-only settings. The results highlight that representational modality plays a critical role in determining when gender stereotypes are activated in evaluations of AI systems and underscore that design choices are consequential for AI governance in evaluative contexts.

**AI Summary:** This research explores how competence cues and visual representations of AI managers can affect gender bias in evaluations. The study found that in text-based descriptions, evaluations were driven by competence rather than gender, but when AI managers were visually represented, gendered biases were present, with feminine-appearing managers being perceived as more competent and trustworthy. The results suggest that design choices, such as visual anthropomorphism, can impact how gender stereotypes are activated in evaluations of AI systems, highlighting the importance of considering representational modality in AI governance.

---

## Stop Saying "AI"
**URL:** https://arxiv.org/abs/2602.17729

**Abstract:** Across academia, industry, and government, ``AI'' has become central in research and development, regulatory debates, and promises of ever faster and more capable decision-making and action. In numerous domains, especially safety-critical ones, there are significant concerns over how ``AI'' may affect decision-making, responsibility, or the likelihood of mistakes (to name only a few categories of critique). However, for most critiques, the target is generally ``AI'', a broad term admitting many (types of) systems used for a variety of tasks and each coming with its own set of limitations, challenges, and potential use cases. In this article, we focus on the military domain as a case study and present both a loose enumerative taxonomy of systems captured under the umbrella term ``military AI'', as well as discussion of the challenges of each. In doing so, we highlight that critiques of one (type of) system will not always transfer to other (types of) systems. Building on this, we argue that in order for debates to move forward fruitfully, it is imperative that the discussions be made more precise and that ``AI'' be excised from debates to the extent possible. Researchers, developers, and policy-makers should make clear exactly what systems they have in mind and what possible benefits and risks attend the deployment of those particular systems. While we focus on AI in the military as an exemplar for the overall trends in discussions of ``AI'', the argument's conclusions are broad and have import for discussions of AI across a host of domains.

**AI Summary:** The abstract discusses the need for more precise discussions around AI, particularly in the military domain. The authors argue that the term "AI" is too broad and encompasses a wide range of systems with different limitations and challenges. They suggest that in order to have fruitful debates, it is important to specify the type of system being discussed and the potential benefits and risks associated with it. This research highlights the importance of moving away from generalizations about AI and focusing on the specific characteristics and implications of individual systems.

---

## Closing Africa's Early Warning Gap: AI Weather Forecasting for Disaster Prevention
**URL:** https://arxiv.org/abs/2602.17726

**Abstract:** In January 2026, torrential rains killed 200-300 people across Southern Africa, exposing a critical reality: 60% of the continent lacks effective early warning systems due to infrastructure costs. Traditional radar stations exceed USD 1 million each, leaving Africa with an 18x coverage deficit compared to the US and EU. We present a production-grade architecture for deploying NVIDIA Earth-2 AI weather models at USD 1,430-1,730/month for national-scale deployment - enabling coverage at 2,000-4,545x lower cost than radar. The system generates 15-day global atmospheric forecasts, cached in PostgreSQL to enable user queries under 200 milliseconds without real-time inference.
Deployed in South Africa in February 2026, our system demonstrates three technical contributions: (1) a ProcessPoolExecutor-based event loop isolation pattern that resolves aiobotocore session lifecycle conflicts in async Python applications; (2) a database-backed serving architecture where the GPU writes global forecasts directly to PostgreSQL, eliminating HTTP transfer bottlenecks for high-resolution tensors; and (3) an automated coordinate management pattern for multi-step inference across 61 timesteps. Forecasts are delivered via WhatsApp, leveraging 80%+ market penetration. This architecture makes continent-scale early warning systems economically viable, supporting UNDRR findings that such systems reduce disaster death rates by 6x. All architectural details are documented inline for full reproducibility.

**AI Summary:** The research addresses the lack of effective early warning systems in Africa due to high infrastructure costs, presenting a cost-effective AI weather forecasting system that can provide national-scale coverage at a significantly lower cost than traditional radar stations. The system, deployed in South Africa, demonstrates technical advancements in event loop isolation, database-backed serving architecture, and automated coordinate management, with forecasts delivered via WhatsApp. This architecture makes continent-scale early warning systems economically viable and has the potential to significantly reduce disaster death rates in Africa.

---

## The Effectiveness of a Virtual Reality-Based Training Program for Improving Body Awareness in Children with Attention Deficit and Hyperactivity Disorder
**URL:** https://arxiv.org/abs/2602.17649

**Abstract:** This study investigates the effectiveness of a Virtual Reality (VR)-based training program in improving body awareness among children with Attention Deficit Hyperactivity Disorder (ADHD). Utilizing a quasi-experimental design, the research sample consisted of 10 children aged 4 to 7 years, with IQ scores ranging from 90 to 110. Participants were divided into an experimental group and a control group, with the experimental group receiving a structured VR intervention over three months, totaling 36 sessions. Assessment tools included the Stanford-Binet Intelligence Scale (5th Edition), the Conners Test for ADHD, and a researcher-prepared Body Awareness Scale.
The results indicated statistically significant differences between pre-test and post-test scores for the experimental group, demonstrating the program's efficacy in enhancing spatial awareness, body part identification, and motor expressions. Furthermore, follow-up assessments conducted one month after the intervention revealed no significant differences from the post-test results, confirming the sustainability and continuity of the program's effects over time. The findings suggest that immersive VR environments provide a safe, engaging, and effective therapeutic medium for addressing psychomotor deficits in early childhood ADHD.

**AI Summary:** This study explored the effectiveness of a Virtual Reality (VR)-based training program in improving body awareness in children with ADHD. The results showed significant improvements in spatial awareness, body part identification, and motor expressions after the VR intervention, with sustained effects one month later. The findings highlight the potential of immersive VR environments as a safe and engaging therapeutic tool for addressing psychomotor deficits in children with ADHD.

---

## What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data
**URL:** https://arxiv.org/abs/2602.17483

**Abstract:** Large language models (LLMs), and conversational agents based on them, are exposed to personal data (PD) during pre-training and during user interactions. Prior work shows that PD can resurface, yet users lack insight into how strongly models associate specific information to their identity. We audit PD across eight LLMs (3 open-source; 5 API-based, including GPT-4o), introduce LMP2 (Language Model Privacy Probe), a human-centered, privacy-preserving audit tool refined through two formative studies (N=20), and run two studies with EU residents to capture (i) intuitions about LLM-generated PD (N1=155) and (ii) reactions to tool output (N2=303). We show empirically that models confidently generate multiple PD categories for well-known individuals. For everyday users, GPT-4o generates 11 features with 60% or more accuracy (e.g., gender, hair color, languages). Finally, 72% of participants sought control over model-generated associations with their name, raising questions about what counts as PD and whether data privacy rights should extend to LLMs.

**AI Summary:** This research examines how large language models (LLMs) associate personal data with individuals' names, finding that models can confidently generate multiple personal data categories for well-known individuals. The study introduces a privacy-preserving audit tool, LMP2, and shows that everyday users' names can be associated with features such as gender, hair color, and languages with high accuracy. The findings raise concerns about data privacy rights and the need for individuals to have control over the information generated by LLMs.

---

## ShadAR: LLM-driven shader generation to transform visual perception in Augmented Reality
**URL:** https://arxiv.org/abs/2602.17481

**Abstract:** Augmented Reality (AR) can simulate various visual perceptions, such as how individuals with colorblindness see the world. However, these simulations require developers to predefine each visual effect, limiting flexibility. We present ShadAR, an AR application enabling real-time transformation of visual perception through shader generation using large language models (LLMs). ShadAR allows users to express their visual intent via natural language, which is interpreted by an LLM to generate corresponding shader code. This shader is then compiled real-time to modify the AR headset viewport. We present our LLM-driven shader generation pipeline and demonstrate its ability to transform visual perception for inclusiveness and creativity.

**AI Summary:** The research introduces ShadAR, an AR application that uses large language models to generate shader code in real-time, allowing users to express their visual intent through natural language. This approach enables the transformation of visual perception in AR, promoting inclusiveness and creativity by providing flexibility in simulating various visual effects. The study demonstrates the effectiveness of the LLM-driven shader generation pipeline in enhancing visual perception in AR applications.

---

## Do Hackers Dream of Electric Teachers?: A Large-Scale, In-Situ Evaluation of Cybersecurity Student Behaviors and Performance with AI Tutors
**URL:** https://arxiv.org/abs/2602.17448

**Abstract:** To meet the ever-increasing demands of the cybersecurity workforce, AI tutors have been proposed for personalized, scalable education. But, while AI tutors have shown promise in introductory programming courses, no work has evaluated their use in hands-on exploration and exploitation of systems (e.g., ``capture-the-flag'') commonly used to teach cybersecurity. Thus, despite growing interest and need, no work has evaluated how students use AI tutors or whether they benefit from their presence in real, large-scale cybersecurity courses. To answer this, we conducted a semester-long observational study on the use of an embedded AI tutor with 309 students in an upper-division introductory cybersecurity course. By analyzing 142,526 student queries sent to the AI tutor across 396 cybersecurity challenges spanning 9 core cybersecurity topics and an accompanying set of post-semester surveys, we find (1) what queries and conversational strategies students use with AI tutors, (2) how these strategies correlate with challenge completion, and (3) students' perceptions of AI tutors in cybersecurity education. In particular, we identify three broad AI tutor conversational styles among users: Short (bounded, few-turn exchanges), Reactive (repeatedly submitting code and errors), and Proactive (driving problem-solving through targeted inquiry). We also find that the use of these styles significantly predicts challenge completion, and that this effect increases as materials become more advanced. Furthermore, students valued the tutor's availability but reported that it became less useful for harder material. Based on this, we provide suggestions for security educators and developers on practical AI tutor use.

**AI Summary:** This research evaluates the use of AI tutors in a large-scale cybersecurity course, analyzing student queries and performance. The study finds that students use three main conversational styles with AI tutors, which significantly predict challenge completion, especially in more advanced materials. While students value the tutor's availability, they report that it becomes less useful for harder material, suggesting practical implications for security educators and developers.

---

## PersonaMail: Learning and Adapting Personal Communication Preferences for Context-Aware Email Writing
**URL:** https://arxiv.org/abs/2602.17340

**Abstract:** LLM-assisted writing has seen rapid adoption in interpersonal communication, yet current systems often fail to capture the subtle tones essential for effectiveness. Email writing exemplifies this challenge: effective messages require careful alignment with intent, relationship, and context beyond mere fluency. Through formative studies, we identified three key challenges: articulating nuanced communicative intent, making modifications at multiple levels of granularity, and reusing effective tone strategies across messages. We developed PersonaMail, a system that addresses these gaps through structured communication factor exploration, granular editing controls, and adaptive reuse of successful strategies. Our evaluation compared PersonaMail against standard LLM interfaces, and showed improved efficiency in both immediate and repeated use, alongside higher user satisfaction. We contribute design implications for AI-assisted communication systems that prioritize interpersonal nuance over generic text generation.

**AI Summary:** The research focuses on improving AI-assisted email writing by addressing challenges in capturing subtle tones essential for effective communication. The study identified key challenges in articulating communicative intent, making granular modifications, and reusing successful tone strategies. The developed system, PersonaMail, demonstrated improved efficiency and user satisfaction compared to standard LLM interfaces, highlighting the importance of prioritizing interpersonal nuance in AI-assisted communication systems.

---

## NotebookRAG: Retrieving Multiple Notebooks to Augment the Generation of EDA Notebooks for Crowd-Wisdom
**URL:** https://arxiv.org/abs/2602.17215

**Abstract:** High-quality exploratory data analysis (EDA) is essential in the data science pipeline, but remains highly dependent on analysts' expertise and effort. While recent LLM-based approaches partially reduce this burden, they struggle to generate effective analysis plans and appropriate insights and visualizations when user intent is abstract. Meanwhile, a vast collection of analysis notebooks produced across platforms and organizations contains rich analytical knowledge that can potentially guide automated EDA. Retrieval-augmented generation (RAG) provides a natural way to leverage such corpora, but general methods often treat notebooks as static documents and fail to fully exploit their potential knowledge for automating EDA. To address these limitations, we propose NotebookRAG, a method that takes user intent, datasets, and existing notebooks as input to retrieve, enhance, and reuse relevant notebook content for automated EDA generation. For retrieval, we transform code cells into context-enriched executable components, which improve retrieval quality and enable rerun with new data to generate updated visualizations and reliable insights. For generation, an agent leverages enhanced retrieval content to construct effective EDA plans, derive insights, and produce appropriate visualizations. Evidence from a user study with 24 participants confirms the superiority of our method in producing high-quality and intent-aligned EDA notebooks.

**AI Summary:** The research introduces NotebookRAG, a method that leverages existing analysis notebooks to automate the generation of exploratory data analysis (EDA) notebooks. By transforming code cells into context-enriched executable components, the method improves retrieval quality and enables the generation of updated visualizations and insights. A user study with 24 participants confirms the superiority of NotebookRAG in producing high-quality and intent-aligned EDA notebooks, highlighting its potential to reduce the burden on analysts and enhance automated EDA processes.

---

## The Bots of Persuasion: Examining How Conversational Agents' Linguistic Expressions of Personality Affect User Perceptions and Decisions
**URL:** https://arxiv.org/abs/2602.17185

**Abstract:** Large Language Model-powered conversational agents (CAs) are increasingly capable of projecting sophisticated personalities through language, but how these projections affect users is unclear. We thus examine how CA personalities expressed linguistically affect user decisions and perceptions in the context of charitable giving. In a crowdsourced study, 360 participants interacted with one of eight CAs, each projecting a personality composed of three linguistic aspects: attitude (optimistic/pessimistic), authority (authoritative/submissive), and reasoning (emotional/rational). While the CA's composite personality did not affect participants' decisions, it did affect their perceptions and emotional responses. Particularly, participants interacting with pessimistic CAs felt lower emotional state and lower affinity towards the cause, perceived the CA as less trustworthy and less competent, and yet tended to donate more toward the charity. Perceptions of trust, competence, and situational empathy significantly predicted donation decisions. Our findings emphasize the risks CAs pose as instruments of manipulation, subtly influencing user perceptions and decisions.

**AI Summary:** This research examines how conversational agents' projected personalities through language affect user perceptions and decisions in the context of charitable giving. The study found that while the composite personality of the CA did not impact participants' decisions, it did affect their perceptions and emotional responses. Participants interacting with pessimistic CAs felt lower emotional state and affinity towards the cause, perceived the CA as less trustworthy and competent, yet tended to donate more. The findings highlight the potential risks of CAs as tools of manipulation, subtly influencing user perceptions and decisions.

---

## Understanding Nature Engagement Experiences of Blind People
**URL:** https://arxiv.org/abs/2602.17093

**Abstract:** Nature plays a crucial role in human health and well-being, but little is known about how blind people experience and relate to it. We conducted a survey of nature relatedness with blind (N=20) and sighted (N=20) participants, along with in-depth interviews with 16 blind participants, to examine how blind people engage with nature and the factors shaping this engagement. Our survey results revealed lower levels of nature relatedness among blind participants compared to sighted peers. Our interview study further highlighted: 1) current practices and challenges of nature engagement, 2) attitudes and values that shape engagement, and 3) expectations for assistive technologies that support safe and meaningful engagement. We also provide design implications to guide future technologies that support nature engagement for blind people. Overall, our findings illustrate how blind people experience nature beyond vision and lay a foundation for technologies that support inclusive nature engagement.

**AI Summary:** This research explores how blind individuals engage with nature and the factors influencing their experiences. The study found that blind participants had lower levels of nature relatedness compared to sighted individuals. The findings highlight the current practices, challenges, attitudes, and values that shape nature engagement for blind individuals, as well as the potential for assistive technologies to support their meaningful engagement with nature. The study provides design implications for future technologies aimed at promoting inclusive nature experiences for blind individuals.

---

## Rememo: A Research-through-Design Inquiry Towards an AI-in-the-loop Therapist's Tool for Dementia Reminiscence
**URL:** https://arxiv.org/abs/2602.17083

**Abstract:** Reminiscence therapy (RT) is a common non-pharmacological intervention in dementia care. Recent technology-mediated interventions have largely focused on people with dementia through solutions that replace human facilitators with conversational agents. However, the relational work of facilitation is critical in the effectiveness of RT. Hence, we developed Rememo, a therapist-oriented tool that integrates Generative AI to support and enrich human facilitation in RT. Our tool aims to support the infrastructural and cultural challenges that therapists in Singapore face. In this research, we contribute the Rememo system as a therapist's tool for personalized RT developed through sociotechnically-aware research-through-design. Through studying this system in-situ, our research extends our understanding of human-AI collaboration for care work. We discuss the implications of designing AI-enabled systems that respect the relational dynamics in care contexts, and argue for a rethinking of synthetic imagery as a therapeutic support for memory rahter than a record of truth.

**AI Summary:** The research focused on developing a therapist-oriented tool called Rememo that integrates Generative AI to enhance reminiscence therapy for dementia patients. The study highlights the importance of human facilitation in the effectiveness of RT and aims to address the challenges faced by therapists in Singapore. The research emphasizes the significance of designing AI-enabled systems that respect relational dynamics in care contexts and suggests rethinking synthetic imagery as a therapeutic support for memory rather than a record of truth.

---

## StoryLensEdu: Personalized Learning Report Generation through Narrative-Driven Multi-Agent Systems
**URL:** https://arxiv.org/abs/2602.17067

**Abstract:** Personalized feedback plays an important role in self-regulated learning (SRL), helping students track progress and refine their strategies. However, current common solutions, such as text-based reports or learning analytics dashboards, often suffer from poor interpretability, monotonous presentation, and limited explainability. To overcome these challenges, we present StoryLensEdu, a narrative-driven multi-agent system that automatically generates intuitive, engaging, and interactive learning reports. StoryLensEdu integrates three agents: a Data Analyst that extracts data insights based on a learning objective centered structure, a Teacher that ensures educational relevance and offers actionable suggestions, and a Storyteller that organizes these insights using the Heroes Journey narrative framework. StoryLensEdu supports post-generation interactive question answering to improve explainability and user engagement. We conducted a formative study in a real high school and iteratively developed StoryLensEdu in collaboration with an e-learning team to inform our design. Evaluation with real users shows that StoryLensEdu enhances engagement and promotes a deeper understanding of the learning process.

**AI Summary:** The research introduces StoryLensEdu, a personalized learning report generation system that utilizes a narrative-driven multi-agent approach to create intuitive and engaging reports for students. By integrating data analysis, educational relevance, and storytelling elements, StoryLensEdu aims to provide actionable insights and promote deeper understanding of the learning process. Evaluation with real users in a high school setting showed that StoryLensEdu enhances engagement and improves the overall learning experience.

---

## "It's like a pet...but my pet doesn't collect data about me": Multi-person Households' Privacy Design Preferences for Household Robots
**URL:** https://arxiv.org/abs/2602.16975

**Abstract:** Household robots boasting mobility, more sophisticated sensors, and powerful processing models have become increasingly prevalent in the commercial market. However, these features may expose users to unwanted privacy risks, including unsolicited data collection and unauthorized data sharing. While security and privacy researchers thus far have explored people's privacy concerns around household robots, literature investigating people's preferred privacy designs and mitigation strategies is still limited. Additionally, the existing literature has not yet accounted for multi-user perspectives on privacy design and household robots. We aimed to fill this gap by conducting in-person participatory design sessions with 15 households to explore how they would design a privacy-aware household robot based on their concerns and expectations. We found that participants did not trust that robots, or their respective manufacturers, would respect the data privacy of household members or operate in a multi-user ecosystem without jeopardizing users' personal data. Based on these concerns, they generated designs that gave them authority over their data, contained accessible controls and notification systems, and could be customized and tailored to suit the needs and preferences of each user over time. We synthesize our findings into actionable design recommendations for robot manufacturers and developers.

**AI Summary:** This research explores the privacy design preferences of multi-person households for household robots, as existing literature on this topic is limited. Through participatory design sessions with 15 households, it was found that participants had concerns about data privacy and wanted control over their data, accessible controls and notifications, and customizable features. The findings provide actionable design recommendations for robot manufacturers and developers to address privacy concerns in household robots.

---

## Nudging Attention to Workplace Meeting Goals: A Large-Scale, Preregistered Field Experiment
**URL:** https://arxiv.org/abs/2602.16939

**Abstract:** Ineffective meetings are pervasive. Thinking ahead explicitly about meeting goals may improve effectiveness, but current collaboration platforms lack integrated support. We tested a lightweight goal-reflection intervention in a preregistered field experiment in a global technology company (361 employees, 7196 meetings). Over two weeks, workers in the treatment group completed brief pre-meeting surveys in their collaboration platform, nudging attention to goals for upcoming meetings. To measure impact, both treatment and control groups completed post-meeting surveys about meeting effectiveness. While the intervention impact on meeting effectiveness was not statistically significant, mixed-methods findings revealed improvements in self-reported awareness and behaviour across both groups, with post-meeting surveys unintentionally functioning as an intervention. We highlight the promise of supporting goal reflection, while noting challenges of evaluating and supporting workplace reflection for meetings, including workflow and collaboration norms, and attitudes and behaviours around meeting preparation. We conclude with implications for designing technological support for meeting intentionality.

**AI Summary:** This study conducted a field experiment in a global technology company to test the impact of a goal-reflection intervention on meeting effectiveness. While the intervention did not have a statistically significant impact on meeting effectiveness, it did lead to improvements in self-reported awareness and behavior in both the treatment and control groups. The findings suggest the potential benefits of supporting goal reflection in meetings, but also highlight the challenges of evaluating and supporting workplace reflection, as well as the importance of designing technological support for meeting intentionality.

---

## Say It My Way: Exploring Control in Conversational Visual Question Answering with Blind Users
**URL:** https://arxiv.org/abs/2602.16930

**Abstract:** Prompting and steering techniques are well established in general-purpose generative AI, yet assistive visual question answering (VQA) tools for blind users still follow rigid interaction patterns with limited opportunities for customization. User control can be helpful when system responses are misaligned with their goals and contexts, a gap that becomes especially consequential for blind users that may rely on these systems for access. We invite 11 blind users to customize their interactions with a real-world conversational VQA system. Drawing on 418 interactions, reflections, and post-study interviews, we analyze prompting-based techniques participants adopted, including those introduced in the study and those developed independently in real-world settings. VQA interactions were often lengthy: participants averaged 3 turns, sometimes up to 21, with input text typically tenfold shorter than the responses they heard. Built on state-of-the-art LLMs, the system lacked verbosity controls, was limited in estimating distance in space and time, relied on inaccessible image framing, and offered little to no camera guidance. We discuss how customization techniques such as prompt engineering can help participants work around these limitations. Alongside a new publicly available dataset, we offer insights for interaction design at both query and system levels.

**AI Summary:** This research explores the importance of user control in conversational visual question answering (VQA) systems for blind users, as current systems lack customization options. The study involved 11 blind users customizing their interactions with a VQA system, revealing that interactions were often lengthy and the system had limitations in verbosity, estimating distance, image framing, and camera guidance. The findings highlight the need for customization techniques like prompt engineering to address these limitations and offer insights for improving interaction design in VQA systems for blind users.

---

## Evidotes: Integrating Scientific Evidence and Anecdotes to Support Uncertainties Triggered by Peer Health Posts
**URL:** https://arxiv.org/abs/2602.16900

**Abstract:** Peer health posts surface new uncertainties, such as questions and concerns for readers. Prior work focused primarily on improving relevance and accuracy fails to address users' diverse information needs and emotions triggered. Instead, we propose directly addressing these by information augmentation. We introduce Evidotes, an information support system that augments individual posts with relevant scientific and anecdotal information retrieved using three user-selectable lenses (dive deeper, focus on positivity, and big picture). In a mixed-methods study with 17 chronic illness patients, Evidotes improved self-reported information satisfaction (3.2->4.6) and reduced self-reported emotional cost (3.4->1.9) compared to participants' baseline browsing. Moreover, by co-presenting sources, Evidotes unlocked information symbiosis: anecdotes made research accessible and contextual, while research helped filter and generalize peer stories. Our work enables an effective integration of scientific evidence and human anecdotes to help users better manage health uncertainty.

**AI Summary:** The research introduces Evidotes, a system that augments peer health posts with scientific and anecdotal information to address users' diverse information needs and emotions. A study with chronic illness patients showed that Evidotes improved information satisfaction and reduced emotional cost compared to baseline browsing. The system enables the integration of scientific evidence and human anecdotes to help users better manage health uncertainty.

---

## Connecting the Dots: Surfacing Structure in Documents through AI-Generated Cross-Modal Links
**URL:** https://arxiv.org/abs/2602.16895

**Abstract:** Understanding information-dense documents like recipes and scientific papers requires readers to find, interpret, and connect details scattered across text, figures, tables, and other visual elements. These documents are often long and filled with specialized terminology, hindering the ability to locate relevant information or piece together related ideas. Existing tools offer limited support for synthesizing information across media types. As a result, understanding complex material remains cognitively demanding. This paper presents a framework for fine-grained integration of information in complex documents. We instantiate the framework in an augmented reading interface, which populates a scientific paper with clickable points on figures, interactive highlights in the body text, and a persistent reference panel for accessing consolidated details without manual scrolling. In a controlled between-subjects study, we find that participants who read the paper with our tool achieved significantly higher scores on a reading quiz without evidence of increased time to completion or cognitive load. Fine-grained integration provides a systematic way of revealing relationships within a document, supporting engagement with complex, information-dense materials.

**AI Summary:** This research introduces a framework for integrating information across different media types in complex documents, such as scientific papers, to improve understanding and synthesis of information. The researchers developed an augmented reading interface that enhances the reading experience by providing clickable points on figures, interactive highlights in the text, and a reference panel for accessing consolidated details. The study showed that participants using this tool achieved higher scores on a reading quiz without experiencing increased time to completion or cognitive load, highlighting the significance of fine-grained integration in supporting engagement with information-dense materials.

---

## CalmReminder: A Design Probe for Parental Engagement with Children with Hyperactivity, Augmented by Real-Time Motion Sensing with a Watch
**URL:** https://arxiv.org/abs/2602.16893

**Abstract:** Families raising children with ADHD often experience heightened stress and reactive parenting. While digital interventions promise personalization, many remain one-size-fits-all and fail to reflect parents' lived practices. We present CalmReminder, a watch-based system that detects children's calm moments and delivers just-in-time prompts to parents. Through a four-week deployment with 16 families (twelve completed) of children with ADHD, we compared notification strategies ranging from hourly to random to only when the child was inferred to be calm. Our sensing-based notifications were frequently perceived as arriving during calm moments. More importantly, parents adopted the system in diverse ways: using notifications for praise, mindfulness, activity planning, or conversation. These findings show that parents are not passive recipients but active designers, reshaping interventions to fit their parenting styles. We contribute a calm detection pipeline, empirical insights into families' flexible appropriation of notifications, and design implications for intervention systems that foster agency.

**AI Summary:** The study introduces CalmReminder, a watch-based system designed to help parents of children with ADHD by detecting their calm moments and providing timely prompts. Through a four-week deployment with 16 families, the study found that parents adopted the system in various ways, using notifications for praise, mindfulness, activity planning, or conversation. The findings suggest that parents are active designers who reshape interventions to fit their parenting styles, highlighting the importance of personalized and flexible digital interventions for families raising children with ADHD.

---

## "My body is not your Porn": Identifying Trends of Harm and Oppression through a Sociotechnical Genealogy of Digital Sexual Violence in South Korea
**URL:** https://arxiv.org/abs/2602.16853

**Abstract:** Ever since the introduction of internet technologies in South Korea, digital sexual violence (DSV) has been a persistent and pervasive problem. Evolving alongside digital technologies, the severity and scale of violence have grown consistently, leading to widespread public concern. In this paper, we present four eras of image-based DSV in South Korea, spanning from the early internet era of the 1990s to the deepfake scandals in the mid-2020s. Drawing from media coverage, legal documents, and academic literature, we elucidate forms and characteristics of DSV cases in each era, tracing how entrenched misogyny is reconfigured and amplified through evolving technologies, alongside shifting legislative measures. Taking a genealogical approach to read prominent cases of different eras, our analysis identifies three constitutive and interconnected dimensions of DSV: (1) the homo-social fabrication of "obscenity", wherein victims' imagery becomes collectively framed as obscene through participatory practices in male-dominant networks; (2) the increasing imperceptibility of violence, as technologies foreclose victims' ability to perceive harm; and (3) the commercialization of abuse through decentralized economic infrastructures. We suggest future directions for CSCW research, and further reflect on the value of the genealogical method in enabling non-linear understanding of DSV as dynamically evolving sociotechnical configurations of harm.

**AI Summary:** This research paper delves into the issue of digital sexual violence (DSV) in South Korea, tracing its evolution from the early internet era to the present day. The study highlights the interconnected dimensions of DSV, including the social construction of "obscenity", the increasing invisibility of harm due to technology, and the commercialization of abuse. The findings underscore the need for further research in the field of Computer-Supported Cooperative Work (CSCW) and emphasize the importance of understanding DSV as a complex sociotechnical phenomenon.

---

## Overseeing Agents Without Constant Oversight: Challenges and Opportunities
**URL:** https://arxiv.org/abs/2602.16844

**Abstract:** To enable human oversight, agentic AI systems often provide a trace of reasoning and action steps. Designing traces to have an informative, but not overwhelming, level of detail remains a critical challenge. In three user studies on a Computer User Agent, we investigate the utility of basic action traces for verification, explore three alternatives via design probes, and test a novel interface's impact on error finding in question-answering tasks. As expected, we find that current practices are cumbersome, limiting their efficacy. Conversely, our proposed design reduced the time participants spent finding errors. However, although participants reported higher levels of confidence in their decisions, their final accuracy was not meaningfully improved. To this end, our study surfaces challenges for human verification of agentic systems, including managing built-in assumptions, users' subjective and changing correctness criteria, and the shortcomings, yet importance, of communicating the agent's process.

**AI Summary:** This research explores the challenges and opportunities of enabling human oversight of agentic AI systems through the design of informative action traces. The study found that current practices are cumbersome and proposed a novel interface design that reduced the time spent finding errors, although it did not significantly improve final accuracy. The research highlights the importance of effectively communicating the agent's process and managing users' subjective correctness criteria in overseeing AI systems.

---

## AI-Mediated Feedback Improves Student Revisions: A Randomized Trial with FeedbackWriter in a Large Undergraduate Course
**URL:** https://arxiv.org/abs/2602.16820

**Abstract:** Despite growing interest in using LLMs to generate feedback on students' writing, little is known about how students respond to AI-mediated versus human-provided feedback. We address this gap through a randomized controlled trial in a large introductory economics course (N=354), where we introduce and deploy FeedbackWriter - a system that generates AI suggestions to teaching assistants (TAs) while they provide feedback on students' knowledge-intensive essays. TAs have the full capacity to adopt, edit, or dismiss the suggestions. Students were randomly assigned to receive either handwritten feedback from TAs (baseline) or AI-mediated feedback where TAs received suggestions from FeedbackWriter. Students revise their drafts based on the feedback, which is further graded. In total, 1,366 essays were graded using the system. We found that students receiving AI-mediated feedback produced significantly higher-quality revisions, with gains increasing as TAs adopted more AI suggestions. TAs found the AI suggestions useful for spotting gaps and clarifying rubrics.

**AI Summary:** This study investigated the impact of AI-mediated feedback on student revisions in a large undergraduate economics course. The researchers found that students who received AI-mediated feedback produced higher-quality revisions compared to those who received handwritten feedback from teaching assistants. The study also showed that the effectiveness of the AI suggestions increased as TAs adopted more of them, indicating the potential for AI to enhance the feedback process in educational settings.

---

## Exploring the Design and Impact of Interactive Worked Examples for Learners with Varying Prior Knowledge
**URL:** https://arxiv.org/abs/2602.16806

**Abstract:** Tutoring systems improve learning through tailored interventions, such as worked examples, but often suffer from the aptitude-treatment interaction effect where low prior knowledge learners benefit more. We applied the ICAP learning theory to design two new types of worked examples, Buggy (students fix bugs), and Guided (students complete missing rules), requiring varying levels of cognitive engagement, and investigated their impact on learning in a controlled experiment with 155 undergraduate students in a logic problem solving tutor. Students in the Buggy and Guided examples groups performed significantly better on the posttest than those receiving passive worked examples. Buggy problems helped high prior knowledge learners whereas Guided problems helped low prior knowledge learners. Behavior analysis showed that Buggy produced more exploration-revision cycles, while Guided led to more help-seeking and fewer errors. This research contributes to the design of interventions in logic problem solving for varied levels of learner knowledge and a novel application of behavior analysis to compare learner interactions with the tutor.

**AI Summary:** This research explores the impact of interactive worked examples on learning for students with varying levels of prior knowledge. The study found that the new types of worked examples, Buggy and Guided, led to improved learning outcomes compared to passive examples, with Buggy examples benefiting high prior knowledge learners and Guided examples benefiting low prior knowledge learners. The research highlights the importance of tailored interventions in logic problem solving and introduces behavior analysis as a tool to compare learner interactions with tutoring systems.

---

## Modeling Distinct Human Interaction in Web Agents
**URL:** https://arxiv.org/abs/2602.17588

**Abstract:** Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.

**AI Summary:** This research focuses on modeling human intervention in web agents to improve collaborative task execution. By analyzing real-user web navigation data, the researchers identified four distinct patterns of user interaction with agents. By training language models to anticipate when users are likely to intervene, they were able to improve intervention prediction accuracy and increase user-rated agent usefulness by 26.5%. This structured modeling of human intervention leads to more adaptive and collaborative web agents.

---

## Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers
**URL:** https://arxiv.org/abs/2602.17469

**Abstract:** The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% "Sentiment Inversion Rate," fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including "Asymmetric Empathy" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a "Modern Bias" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate "Affective Stability" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.

**AI Summary:** This research focuses on the challenges of ensuring accurate understanding of human intent by AI systems across different languages, specifically Bengali and English. The study benchmarks four transformer architectures and identifies significant issues such as sentiment inversion and dialect representation failures. The findings highlight the importance of culturally grounded alignment in AI systems to maintain emotional fidelity and trust between humans and AI.

---

## IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents
**URL:** https://arxiv.org/abs/2602.17049

**Abstract:** Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

**AI Summary:** The research introduces IntentCUA, a framework for multi-agent computer-use agents that improves long-horizon execution by aligning plans with user intent. By abstracting raw interaction traces into intent representations and reusable skills, the system reduces redundant re-planning and error propagation across desktop applications. IntentCUA outperformed RL-based and trajectory-centric approaches in task success rate and step efficiency, demonstrating the importance of system-level intent abstraction and memory-grounded coordination for reliable and efficient desktop automation in dynamic environments.

---

## Wink: Recovering from Misbehaviors in Coding Agents
**URL:** https://arxiv.org/abs/2602.17037

**Abstract:** Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories.
To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.

**AI Summary:** This research paper introduces a system called Wink that aims to automatically recover from misbehaviors in coding agents powered by large language models. The study identifies three primary categories of misbehaviors and shows that these issues occur in about 30% of all agent trajectories. Wink successfully resolves 90% of misbehaviors with a single intervention, leading to a significant reduction in Tool Call Failures and improving overall productivity in the software development workflow.

---

## CreateAI Insights from an NSF Workshop on K12 Students, Teachers, and Families as Designers of Artificial Intelligence and Machine Learning Applications
**URL:** https://arxiv.org/abs/2602.16894

**Abstract:** In response to the exponential growth in the use of artificial intelligence and machine learning applications, educators, researchers and policymakers have taken steps to integrate artificial intelligence applications into K-12 education. Among these efforts, one equally important approach has received little, if any attention: What if students and teachers were not just learning to be competent users of AI but also its creators? This question is at the heart of CreateAI in which K12 educators, researchers, and learning scientists addressed the following questions: (1) What tools, skills, and knowledge will empower students and teachers to build their own AI/ML applications? (2) How can we integrate these approaches into classrooms? and (3) What new possibilities for learning emerge when students and teachers become innovators and creators? In the report we provide recommendations for what tools designed for creating AI/ML applications should address in terms of design features, and learner progression in investigations. To promote effective learning and teaching of creating AI applications, we also need to help students and teachers select appropriate tools. We outline how we need to develop a better understanding of learning practices and funds of knowledge to support youth as they create and evaluate AI/ML applications. This also includes engaging youth in learning about ethics and critically that is authentic, empowering, and relevant throughout the design process. Here we advocate for the integration of ethics in the curriculum. We also address what teachers need to know and how assessments can help establish baselines, include different instruments, and promote students as responsible creators of AI. Together, these recommendations provide important insights for preparing students to engage thoughtfully and critically with these technologies.

**AI Summary:** The research explores the idea of empowering K-12 students and teachers to not only use AI applications but also create their own. The study highlights the importance of providing the necessary tools, skills, and knowledge for students and teachers to build AI/ML applications, as well as integrating these approaches into classrooms. The findings emphasize the need for developing a better understanding of learning practices, ethics, and assessments to support students as responsible creators of AI, ultimately preparing them to engage thoughtfully and critically with these technologies.

---

## Expanding the Scope of Computational Thinking in Artificial Intelligence for K-12 Education
**URL:** https://arxiv.org/abs/2602.16890

**Abstract:** The introduction of generative artificial intelligence applications to the public has led to heated discussions about its potential impacts and risks for K-12 education. One particular challenge has been to decide what students should learn about AI, and how this relates to computational thinking, which has served as an umbrella for promoting and introducing computing education in schools. In this paper, we situate in which ways we should expand computational thinking to include artificial intelligence and machine learning technologies. Furthermore, we discuss how these efforts can be informed by lessons learned from the last decade in designing instructional programs, integrating computing with other subjects, and addressing issues of algorithmic bias and justice in teaching computing in schools.

**AI Summary:** This research paper explores the integration of artificial intelligence and machine learning technologies into K-12 education by expanding the scope of computational thinking. The authors discuss the challenges of determining what students should learn about AI and how it relates to computational thinking. They also highlight the importance of addressing issues such as algorithmic bias and justice in teaching computing in schools.

---

## "Hello, I'm Delivering. Let Me Pass By": Navigating Public Pathways with Walk-along with Robots in Crowded City Streets
**URL:** https://arxiv.org/abs/2602.16861

**Abstract:** As the presence of autonomous robots in public spaces increases-whether navigating campus walkways or neighborhood sidewalks-understanding how to carefully study these robots becomes critical. While HRI research has conducted field studies in public spaces, these are often limited to controlled experiments with prototype robots or structured observational methods, such as the Wizard of Oz technique. However, the autonomous mobile robots we encounter today, particularly delivery robots, operate beyond the control of researchers, navigating dynamic routes and unpredictable environments. To address this challenge, a more deliberate approach is required. Drawing inspiration from public realm ethnography in urban studies, geography, and sociology, this paper proposes the Walk-Along with Robots (WawR) methodology. We outline the key features of this method, the steps we applied in our study, the unique insights it offers, and the ways it can be evaluated. We hope this paper stimulates further discussion on research methodologies for studying autonomous robots in public spaces.

**AI Summary:** This research paper introduces the Walk-Along with Robots (WawR) methodology to study autonomous robots, particularly delivery robots, navigating public spaces. The methodology draws inspiration from public realm ethnography and aims to provide unique insights into how these robots operate in dynamic and unpredictable environments. By proposing a more deliberate approach to studying autonomous robots in public spaces, this research contributes to the development of effective research methodologies in the field of human-robot interaction.

---

## Wearable AR for Restorative Breaks: How Interactive Narrative Experiences Support Relaxation for Young Adults
**URL:** https://arxiv.org/abs/2602.16323

**Abstract:** Young adults often take breaks from screen-intensive work by consuming digital content on mobile phones, which undermines rest through visual fatigue and inactivity. We introduce a design framework that embeds light break activities into media content on AR smart glasses, balancing engagement and recovery. The framework employs three strategies: (1) seamlessly guiding users by embedding activity cues aligned with media elements; (2) transitioning to audio-centric formats to reduce visual load while sustaining immersion; and (3) structuring sessions with "rise-peak-closure" pacing for smooth transitions. In a within-subjects study (N = 16) comparing passive viewing, reminder-based breaks, and non-narrative activities, InteractiveBreak instantiated from our framework seamlessly guided activities, sustained engagement, and enhanced break quality. These findings demonstrate wearable AR's potential to support restorative relaxation by transforming breaks into engaging and meaningful experiences.

**AI Summary:** This research explores the use of wearable AR technology to provide restorative breaks for young adults, who often experience visual fatigue and inactivity when consuming digital content on mobile phones. The study introduces a design framework that incorporates light break activities into media content on AR smart glasses, guiding users through interactive narrative experiences to enhance relaxation. Results from a study comparing different break methods show that InteractiveBreak, based on the framework, effectively guides activities, sustains engagement, and improves break quality, highlighting the potential of wearable AR to transform breaks into engaging and meaningful experiences for relaxation.

---

