[
  {
    "title": "Understanding Workplace Relatedness Support among Healthcare Professionals: A Four-Layer Model and Implications for Technology Design",
    "abstract": "Healthcare professionals (HCPs) face increasing occupational stress and burnout. Supporting HCPs need for relatedness is fundamental to their psychological wellbeing and resilience. However, how technologies could support HCPs relatedness in the workplace remains less explored. This study incorporated semi-structured interviews (n = 15) and co-design workshops (n = 21) with HCPs working in the UK National Health Service (NHS), to explore their current practices and preferences for workplace relatedness support, and how technology could be utilized to benefit relatedness. Qualitative analysis yielded a four-layer model of HCPs relatedness need, which includes Informal Interactions, Camaraderie and Bond, Community and Organizational Care, and Shared Identity. Workshops generated eight design concepts (e.g., Playful Encounter, Collocated Action, and Memories and Stories) that operationalize the four relatedness need layers. We conclude by highlighting the theoretical relevance, practical design implications, and the necessity to strengthen relatedness support for HCPs in the era of digitalization and artificial intelligence.",
    "url": "https://arxiv.org/abs/2602.06916",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores the importance of supporting healthcare professionals' need for relatedness in the workplace to reduce stress and burnout. Through interviews and workshops, a four-layer model of relatedness needs was identified, including Informal Interactions, Camaraderie and Bond, Community and Organizational Care, and Shared Identity. The study also generated design concepts for technology that could help meet these relatedness needs, emphasizing the importance of enhancing support for healthcare professionals in the digital age."
  },
  {
    "title": "Directing Space: Rehearsing Architecture as Performer with Explainable AI",
    "abstract": "As AI systems increasingly become embedded in interactive and im-mersive artistic environments, artists and technologists are discovering new opportunities to engage with their interpretive and autonomous capacities as creative collaborators in live performance. The focus of this work-in-progress is on outlining conceptual and technical foundations under which performance-makers and interactive architecture can collaborate within rehearsal settings. It introduces a rehearsal-oriented prototype system for shaping and testing AI-mediated environments within creative practice. This approach treats interactive architecture as a performative agent that senses spatial behaviour and speech, interprets these signals through a large language model, and generates real-time environmental adaptations. Designed for deployment in physical performance spaces, the system employs virtual blueprints to support iterative experimentation and creative dialogue between artists and AI agents, using reasoning traces to inform architectural interaction design grounded in dramaturgical principles.",
    "url": "https://arxiv.org/abs/2602.06915",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the collaboration between performance-makers and interactive architecture using AI systems in rehearsal settings. The prototype system allows for real-time environmental adaptations based on spatial behavior and speech, creating a creative dialogue between artists and AI agents. By incorporating reasoning traces and dramaturgical principles, this approach aims to enhance the creative process in physical performance spaces."
  },
  {
    "title": "Redundant is Not Redundant: Automating Efficient Categorical Palette Design Unifying Color & Shape Encodings with CatPAW",
    "abstract": "Colors and shapes are commonly used to encode categories in multi-class scatterplots. Designers often combine the two channels to create redundant encodings, aiming to enhance class distinctions. However, evidence for the effectiveness of redundancy remains conflicted, and guidelines for constructing effective combinations are limited. This paper presents four crowdsourced experiments evaluating redundant color-shape encodings and identifying high-performing configurations across different category numbers. Results show that redundancy significantly improves accuracy in assessing class-level correlations, with the strongest benefits for 5-8 categories. We also find pronounced interaction effects between colors and shapes, underscoring the need for careful pairing in designing redundant encodings. Drawing on these findings, we introduce a categorical palette design tool that enables designers to construct empirically grounded palettes for effective categorical visualization. Our work advances understanding of categorical perception in data visualization by systematically identifying effective redundant color-shape combinations and embedding these insights into a practical palette design tool.",
    "url": "https://arxiv.org/abs/2602.06792",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the effectiveness of using redundant color and shape encodings in multi-class scatterplots to enhance class distinctions. The study found that redundancy significantly improves accuracy in assessing class-level correlations, especially for 5-8 categories. The results highlight the importance of carefully pairing colors and shapes in designing effective redundant encodings and introduce a tool for creating empirically grounded palettes for categorical visualization."
  },
  {
    "title": "ClassAid: A Real-time Instructor-AI-Student Orchestration System for Classroom Programming Activities",
    "abstract": "Generative AI is reshaping education, but it also raises concerns about instability and overreliance. In programming classrooms, we aim to leverage its feedback capabilities while reinforcing the educator's role in guiding student-AI interactions. We developed ClassAid, a real-time orchestration system that integrates TA Agents to provide personalized support and an AI-driven dashboard that visualizes student-AI interactions, enabling instructors to dynamically adjust TA Agent modes. Instructors can configure the Agent to provide technical feedback (direct coding solutions), heuristic feedback (hint-based guidance), automatic feedback (autonomously selecting technical or heuristic support), or silent operation (no AI support). We evaluated ClassAid through three aspects: (1) the TA Agents' performance, (2) feedback from 54 students and one instructor during a classroom deployment, and (3) interviews with eight educators. Results demonstrate that dynamic instructor control over AI supports effective real-time personalized feedback and provides design implications for integrating AI into authentic educational settings.",
    "url": "https://arxiv.org/abs/2602.06734",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces ClassAid, a real-time instructor-AI-student orchestration system for programming classrooms. The system allows instructors to adjust TA Agent modes to provide personalized support, including technical, heuristic, automatic, or silent operation feedback. Evaluation results show that dynamic instructor control over AI can effectively enhance real-time personalized feedback in educational settings, offering valuable design implications for integrating AI into classrooms."
  },
  {
    "title": "PrefIx: Understand and Adapt to User Preference in Human-Agent Interaction",
    "abstract": "LLM-based agents can complete tasks correctly yet still frustrate users through poor interaction patterns, such as excessive confirmations, opaque reasoning, or misaligned pacing. Current benchmarks evaluate task accuracy but overlook how agents interact: whether they infer preferences from implicit cues, adapt dynamically, or maintain fine-grained interaction quality. We introduce Prefix, a configurable environment that evaluates both what agents accomplish and how they interact. Central to Prefix is the Interaction-as-a-Tool (IaaT) paradigm, which treats interaction behaviors as structured tool calls, unifying them with existing evaluation frameworks. We define 31 preference settings across 14 attributes and formalize user experience (UX) as a core metric alongside task accuracy. A composite LLM-as-a-Judge mechanism across seven UX dimensions achieves strong aggregate reliability (ICC > 0.79), high internal consistency (alpha = 0.943), and human correlation (rho = 0.52-0.78). Preference-aware agents show 7.6% average UX improvement and 18.5% gain in preference alignment. Our work is openly accessible.",
    "url": "https://arxiv.org/abs/2602.06714",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Prefix, a tool to evaluate both task completion and interaction quality of LLM-based agents in human-agent interactions. By defining preference settings and formalizing user experience as a core metric, the study shows that preference-aware agents can significantly improve user experience and alignment with user preferences. The research highlights the importance of considering interaction quality in addition to task accuracy in evaluating AI agents."
  },
  {
    "title": "Beyond Judgment: Exploring LLM as a Support System for Maternal Mental Health",
    "abstract": "In the age of Large Language Models (LLMs), much work has already been done on how LLMs support medication advice and serve as information providers; however, how mothers use these tools for emotional and informational support to avoid social judgment remains underexplored. In this study, we have conducted a 10-day mixed-methods exploratory survey (N=107) to investigate how mothers use LLMs as a non-judgmental resource for emotional support and regulation, as well as situational reassurance. Our findings show that mothers are asking LLMs various questions about childcare to reassure themselves and avoid judgment, particularly around childcare decisions, maternal guilt, and late-night caregiving. Open-ended responses also show that mothers are comfortable with LLMs because they do not have to think about social consequences or judgment. Although they use LLMs for quick information or reassurance to avoid judgment, the results also show that more than half of the participants value human warmth over LLMs; however, a significant minority, especially those who live in a joint family, consider LLMs to avoid human judgment. These findings help us understand how we can frame LLMs as low-risk interaction support rather than as a replacement for human support, and highlight the role of social context in shaping emotional technology use.",
    "url": "https://arxiv.org/abs/2602.06678",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study explores how mothers use Large Language Models (LLMs) as a non-judgmental resource for emotional support and reassurance in childcare decisions, maternal guilt, and late-night caregiving. The findings suggest that while mothers value human warmth over LLMs, a significant minority, particularly those in joint families, use LLMs to avoid human judgment. This research highlights the importance of framing LLMs as low-risk interaction support rather than a replacement for human support, and emphasizes the impact of social context on emotional technology use."
  },
  {
    "title": "Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging",
    "abstract": "Digital behaviour change systems increasingly rely on repeated, system-initiated messages to support users in everyday contexts. LLMs enable these messages to be personalised consistently across interactions, yet it remains unclear whether such personalisation improves individual messages or instead shapes users' perceptions through patterns of exposure. We explore this question in the context of LLM-generated JITAIs, which are short, context-aware messages delivered at moments deemed appropriate to support behaviour change, using physical activity as an application domain. In a controlled retrospective study, 90 participants evaluated messages generated using four LLM strategies: baseline prompting, few-shot prompting, fine-tuned models, and retrieval augmented generation, each implemented with and without Big Five Personality Traits to produce personality-aligned communication across multiple scenarios. Using ordinal multilevel models with within-between decomposition, we distinguish trial-level effects, whether personality information improves evaluations of individual messages, from person-level exposure effects, whether participants receiving higher proportions of personality-informed messages exhibit systematically different overall perceptions. Results showed no trial-level associations, but participants who received higher proportions of BFPT-informed messages rated the messages as more personalised, appropriate, and reported less negative affect. We use Communication Accommodation Theory for post-hoc analysis. These results suggest that personality-based personalisation in behaviour change systems may operate primarily through aggregate exposure rather than per-message optimisation, with implications for how adaptive systems are designed and evaluated in sustained human-AI interaction. In-situ longitudinal studies are needed to validate these findings in real-world contexts.",
    "url": "https://arxiv.org/abs/2602.06596",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the impact of using Big Five Personality Traits in personalized messaging for behavior change systems, specifically in the context of physical activity. The study found that while there were no individual message-level associations, participants who received a higher proportion of personality-informed messages rated them as more personalized and appropriate, and reported less negative affect. This suggests that personality-based personalization may have a greater impact through aggregate exposure rather than per-message optimization, highlighting the importance of considering user perceptions in designing and evaluating adaptive systems in human-AI interactions."
  },
  {
    "title": "Designing Computational Tools for Exploring Causal Relationships in Qualitative Data",
    "abstract": "Exploring causal relationships for qualitative data analysis in HCI and social science research enables the understanding of user needs and theory building. However, current computational tools primarily characterize and categorize qualitative data; the few systems that analyze causal relationships either inadequately consider context, lack credibility, or produce overly complex outputs. We first conducted a formative study with 15 participants interested in using computational tools for exploring causal relationships in qualitative data to understand their needs and derive design guidelines. Based on these findings, we designed and implemented QualCausal, a system that extracts and illustrates causal relationships through interactive causal network construction and multi-view visualization. A feedback study (n = 15) revealed that participants valued our system for reducing the analytical burden and providing cognitive scaffolding, yet navigated how such systems fit within their established research paradigms, practices, and habits. We discuss broader implications for designing computational tools that support qualitative data analysis.",
    "url": "https://arxiv.org/abs/2602.06506",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the need for computational tools to explore causal relationships in qualitative data analysis in HCI and social science research. Current tools lack credibility and produce complex outputs, leading to the development of QualCausal, a system that extracts and illustrates causal relationships through interactive network construction and visualization. Feedback from participants showed that the system reduced analytical burden and provided cognitive support, but also highlighted the challenge of integrating such tools into existing research practices. The study emphasizes the importance of designing computational tools that support qualitative data analysis."
  },
  {
    "title": "Simulating Word Suggestion Usage in Mobile Typing to Guide Intelligent Text Entry Design",
    "abstract": "Intelligent text entry (ITE) methods, such as word suggestions, are widely used in mobile typing, yet improving ITE systems is challenging because the cognitive mechanisms behind suggestion use remain poorly understood, and evaluating new systems often requires long-term user studies to account for behavioral adaptation. We present WSTypist, a reinforcement learning-based model that simulates how typists integrate word suggestions into typing. It builds on recent hierarchical control models of typing, but focuses on the cognitive mechanisms that underlie the high-level decision-making for effectively integrating word suggestions into manual typing: assessing efficiency gains, considering orthographic uncertainties, and including personal reliance on AI support. Our evaluations show that WSTypist simulates diverse human-like suggestion-use strategies, reproduces individual differences, and generalizes across different systems. Importantly, we demonstrate on four design cases how computational rationality models can be used to inform what-if analyses during the design process, by simulating how users might adapt to changes in the UI or in the algorithmic support, reducing the need for long-term user studies.",
    "url": "https://arxiv.org/abs/2602.06489",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research introduces WSTypist, a model that simulates how typists use word suggestions in mobile typing. The model focuses on the cognitive mechanisms involved in decision-making when integrating word suggestions, such as efficiency gains, orthographic uncertainties, and personal reliance on AI support. The evaluations show that WSTypist replicates human-like suggestion-use strategies, individual differences, and can inform design decisions by simulating user adaptation to changes in UI or algorithmic support, reducing the need for long-term user studies."
  },
  {
    "title": "InterFlow: Designing Unobtrusive AI to Empower Interviewers in Semi-Structured Interviews",
    "abstract": "Semi-structured interviews are a common method in qualitative research. However, conducting high-quality interviews is challenging, as it requires interviewers to actively listen to participants, adapt their plans as the conversation unfolds, and probe effectively. We propose InterFlow, an AI-powered visual scaffold that helps interviewers manage the interview flow and facilitates real-time data sensemaking. The system dynamically adapts the interview script to the ongoing conversation and provides a visual timer to track interview progress and conversational balance. It further supports information capture with three levels of automation: manual entry, AI-assisted summary with user-specified focus, and a co-interview agent that proactively surfaces potential follow-up points. A within-subject user study (N = 12) indicates that InterFlow reduces interviewers' cognitive load and facilitates the interview process. Based on the user study findings, we provide design implications for unobtrusive and agency-preserving AI assistance under time-sensitive and cognitively demanding situations.",
    "url": "https://arxiv.org/abs/2602.06396",
    "journal": "arXiv cs.HC",
    "ai_summary": "The study introduces InterFlow, an AI-powered tool designed to assist interviewers in conducting semi-structured interviews by providing visual cues, adaptive interview scripts, and automated data capture. The system was found to reduce cognitive load and improve the interview process in a user study. The research highlights the potential of unobtrusive AI assistance in enhancing qualitative research methods and provides design implications for similar tools in time-sensitive and cognitively demanding situations."
  },
  {
    "title": "How Do Human Creators Embrace Human-AI Co-Creation? A Perspective on Human Agency of Screenwriters",
    "abstract": "Generative AI has greatly transformed creative work in various domains, such as screenwriting. To understand this transformation, prior research often focused on capturing a snapshot of human-AI co-creation practice at a specific moment, with less attention to how humans mobilize, regulate, and reflect to form the practice gradually. Motivated by Bandura's theory of human agency, we conducted a two-week study with 19 professional screenwriters to investigate how they embraced AI in their creation process. Our findings revealed that screenwriters not only mindfully planned, foresaw, and responded to AI usage, but, more importantly, through reflections on practice, they developed themselves and human-AI co-creation paradigms, such as cognition, strategies, and workflows. They also expressed various expectations for how future AI should better support their agency. Based on our findings, we conclude this paper with extensive discussion and actionable suggestions to screenwriters, tool developers, and researchers for sustainable human-AI co-creation.",
    "url": "https://arxiv.org/abs/2602.06327",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research study explores how professional screenwriters embrace AI in their creative process, focusing on how they mobilize, regulate, and reflect on their use of AI over time. The findings show that screenwriters actively plan, anticipate, and adapt to AI usage, leading to the development of new paradigms for human-AI co-creation. The study highlights the importance of human agency in the creative process and offers actionable suggestions for screenwriters, tool developers, and researchers to promote sustainable human-AI co-creation practices."
  },
  {
    "title": "Reimagining Legal Fact Verification with GenAI: Toward Effective Human-AI Collaboration",
    "abstract": "Fact verification is a critical yet underexplored component of non-litigation legal practice. While existing research has examined automation in legal workflow and human-AI collaboration in high-stakes domains, little is known about how GenAI can support fact verification, a task that demands prudent judgment and strict accountability. To address this, we conducted semi-structured interviews with 18 lawyers to understand their current verification practices, attitudes toward GenAI adoption, and expectations for future systems. We found that while lawyers use GenAI for low-risk tasks like drafting and language optimization, concerns over accuracy, confidentiality, and liability are currently limiting its adoption for fact verification. These concerns translate into core design requirements for AI systems that are trustworthy and accountable. Based on these, we contribute design insights for human-AI collaboration in legal fact verification, emphasizing the development of auditable systems that balance efficiency with professional judgment and uphold ethical and legal accountability in high-stakes practice.",
    "url": "https://arxiv.org/abs/2602.06305",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of AI in legal fact verification, a crucial aspect of non-litigation legal practice. Through interviews with lawyers, the study found that while AI is used for low-risk tasks like drafting, concerns over accuracy, confidentiality, and liability hinder its adoption for fact verification. The findings highlight the need for AI systems in legal practice to be trustworthy, accountable, and uphold ethical and legal standards, emphasizing the importance of human-AI collaboration in high-stakes domains."
  },
  {
    "title": "Rethinking External Communication of Autonomous Vehicles: Is the Field Converging, Diverging, or Stalling?",
    "abstract": "As autonomous vehicles enter public spaces, external human-machine interfaces are proposed to support communication with external road users. A decade of research has produced hundreds of studies and reviews, yet it remains unclear whether the field is converging on shared principles or diverging across approaches. We present a multi-dimensional analysis of 620 publications, complemented by industry deployments and regulatory documents, to track research evolution and identify convergence. The analysis reveals several field-level patterns. First, convergence on a safety-first core: simple visual cues that clarify intent. Second, sustained divergence in necessity and implementation. Third, a progressive filtering funnel: broad exploration in research and concepts narrows in deployment and is codified by regulation into a minimal set of permitted signals. These insights point to a shift in emphasis for future work, from producing new prototypes toward consolidating evidence, clarifying points of contention, and developing frameworks that can adapt across contexts.",
    "url": "https://arxiv.org/abs/2602.06278",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research examines the evolution of external communication interfaces for autonomous vehicles, finding that the field is converging on the use of simple visual cues to clarify intent for safety. However, there is divergence in the necessity and implementation of these interfaces, with regulations ultimately narrowing down the permitted signals. The study suggests a shift towards consolidating evidence, clarifying points of contention, and developing adaptable frameworks for future research in this area."
  },
  {
    "title": "Secure and Private Spatial Sharing for Mixed Reality Remote Collaboration in Enterprise Settings",
    "abstract": "Mixed Reality (MR) technologies are increasingly adopted by enterprises to enhance remote collaboration, enabling users to share real-time views of their physical environments through head-mounted displays (HMDs). While MR spatial sharing offers significant benefits, it introduces complex security and privacy risks, particularly in balancing employee collaboration needs with enterprise data protection requirements across office and personal spaces. This paper investigates these challenges through formative interviews with employees and expert consultations with professionals in cybersecurity, IoT, technology risk, and corporate legal domains. We present a conceptual framework for secure MR spatial sharing in enterprise contexts and identify critical concerns and requirements for system design. Based on our findings, we offer actionable recommendations to guide the development of secure and privacy-preserving MR spatial sharing solutions for future enterprise deployments.",
    "url": "https://arxiv.org/abs/2602.06254",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores the security and privacy risks associated with using Mixed Reality (MR) technologies for remote collaboration in enterprise settings. Through interviews with employees and consultations with cybersecurity experts, the study identifies key concerns and requirements for designing secure MR spatial sharing systems. The findings provide actionable recommendations for developing privacy-preserving solutions to ensure the safe implementation of MR technology in the workplace."
  },
  {
    "title": "Personagram: Bridging Personas and Product Design for Creative Ideation with Multimodal LLMs",
    "abstract": "Product designers often begin their design process with handcrafted personas. While personas are intended to ground design decisions in consumer preferences, they often fall short in practice by remaining abstract, expensive to produce, and difficult to translate into actionable design features. As a result, personas risk serving as static reference points rather than tools that actively shape design outcomes. To address these challenges, we built Personagram, an interactive system powered by multimodal large language models (MLLMs) that helps designers explore detailed census-based personas, extract product features inferred from persona attributes, and recombine them for specific customer segments. In a study with 12 professional designers, we show that Personagram facilitates more actionable ideation workflows by structuring multimodal thinking from persona attributes to product design features, achieving higher engagement with personas, perceived transparency, and satisfaction compared to a chat-based baseline. We discuss implications of integrating AI-generated personas into product design workflows.",
    "url": "https://arxiv.org/abs/2602.06197",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces Personagram, an interactive system utilizing multimodal large language models to help designers generate detailed personas based on census data and extract design features for specific customer segments. The study with professional designers demonstrated that Personagram facilitates more actionable ideation workflows, leading to higher engagement with personas, perceived transparency, and satisfaction compared to a chat-based baseline. The integration of AI-generated personas into product design workflows could potentially improve the effectiveness and efficiency of the design process."
  },
  {
    "title": "Knowledge Synthesis Graph: An LLM-Based Approach for Modeling Student Collaborative Discourse",
    "abstract": "Asynchronous, text-based discourse-such as students' posts in discussion forums-is widely used to support collaborative learning. However, the distributed and evolving nature of such discourse often makes it difficult to see how ideas connect, develop, and build on one another over time. As a result, learners may struggle to recognize relationships among ideas-a process that is critical for idea advancement in productive collaborative discourse. To address this challenge, we explore how large language models (LLMs) can provide representational guidance by modeling student discourse as a Knowledge Synthesis Graph (KSG). The KSG identifies ideas from student discourse and visualizes their epistemic relationships, externalizing the current state of collaborative knowledge in a form that can support further inquiry and idea advancement. In this study, we present the design of the KSG and evaluate the LLM-based approach for constructing KSGs from authentic student discourse data. Through multi-round human-expert coding and prompt iteration, our results demonstrate the feasibility of using our approach to construct reliable KSGs across different models. This work provides a technical foundation for modeling collaborative discourse with LLMs and offers pedagogical implications for augmenting complex knowledge work in collaborative learning environments.",
    "url": "https://arxiv.org/abs/2602.06194",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores using large language models (LLMs) to create a Knowledge Synthesis Graph (KSG) that visualizes the relationships among ideas in student collaborative discourse. By representing the evolving nature of discourse, the KSG can help learners better understand and build on ideas over time. The study shows that the LLM-based approach is feasible for constructing reliable KSGs, offering a technical foundation for modeling collaborative discourse and enhancing complex knowledge work in collaborative learning environments."
  },
  {
    "title": "Generics in science communication: Misaligned interpretations across laypeople, scientists, and large language models",
    "abstract": "Scientists often use generics, that is, unquantified statements about whole categories of people or phenomena, when communicating research findings (e.g., \"statins reduce cardiovascular events\"). Large language models (LLMs), such as ChatGPT, frequently adopt the same style when summarizing scientific texts. However, generics can prompt overgeneralizations, especially when they are interpreted differently across audiences. In a study comparing laypeople, scientists, and two leading LLMs (ChatGPT-5 and DeepSeek), we found systematic differences in interpretation of generics. Compared to most scientists, laypeople judged scientific generics as more generalizable and credible, while LLMs rated them even higher. These mismatches highlight significant risks for science communication. Scientists may use generics and incorrectly assume laypeople share their interpretation, while LLMs may systematically overgeneralize scientific findings when summarizing research. Our findings underscore the need for greater attention to language choices in both human and LLM-mediated science communication.",
    "url": "https://arxiv.org/abs/2602.06190",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research study explores how generics, unquantified statements used in science communication, are interpreted differently by laypeople, scientists, and large language models (LLMs). The study found that laypeople tend to overgeneralize and find scientific generics more generalizable and credible compared to scientists, while LLMs rated them even higher. These findings highlight the risks of miscommunication in science communication and emphasize the importance of considering language choices in both human and LLM-mediated communication."
  },
  {
    "title": "DataCrumb: A Physical Probe for Reflections on Background Web Tracking",
    "abstract": "Cookie banners and privacy settings attempt to give users a sense of control over how their personal data is collected and used, but background tracking of personal information often continues unnoticed. To explore how such invisible data collection might be made more perceptible, we present DataCrumb, a physical probe that reacts in real-time to data tracking with visual and auditory feedback. Using a research-through-design approach, we deployed the artifact in three households and studied participants' responses. Instead of providing details about what data was being tracked, the artifact introduced subtle disruptions that made background data flows harder to ignore. Participants described new forms of awareness, contradiction, and fatigue. Our findings show how sensory feedback can support reflection by drawing attention to tracking data flows that are usually hidden. We argue for designing systems that foster awareness and interpretation, especially when the users' control and understanding are limited.",
    "url": "https://arxiv.org/abs/2602.06177",
    "journal": "arXiv cs.HC",
    "ai_summary": "The researchers developed DataCrumb, a physical probe that provides real-time visual and auditory feedback in response to background data tracking. By deploying this artifact in households, they found that it increased users' awareness of hidden data collection and prompted reflection on their privacy. This study highlights the importance of designing systems that make data tracking more perceptible to users in order to promote awareness and understanding."
  },
  {
    "title": "The Eye-Head Mover Spectrum: Modelling Individual and Population Head Movement Tendencies in Virtual Reality",
    "abstract": "People differ in how much they move their head versus their eyes when shifting gaze, yet such tendencies remain largely unexplored in HCI. We introduce head movement tendencies as a fundamental dimension of individual difference in VR and provide a quantitative account of their population-level distribution. Using a 360° video free-viewing dataset (N=87), we model head contributions to gaze shifts with a hinge-based parametric function, revealing a spectrum of strategies from eye-movers to head-movers. We then conduct a user study (N=28) combining 360° video viewing with a short controlled task using gaze targets. While parameter values differ across tasks, individuals show partial alignment in their relative positions within the population, indicating that tendencies are meaningful but shaped by context. Our findings establish head movement tendencies as an important concept for VR and highlight implications for adaptive systems such as foveated rendering, viewport alignment, and multi-user experience design.",
    "url": "https://arxiv.org/abs/2602.06164",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores individual differences in head and eye movement tendencies in virtual reality (VR) environments, using a dataset of 360° video free-viewing and a user study with gaze targets. The study reveals a spectrum of strategies from eye-movers to head-movers and shows that individuals exhibit consistent tendencies across tasks but are influenced by context. These findings suggest that head movement tendencies are a crucial concept for VR design, with implications for adaptive systems and multi-user experience design."
  },
  {
    "title": "Hear You in Silence: Designing for Active Listening in Human Interaction with Conversational Agents Using Context-Aware Pacing",
    "abstract": "In human conversation, empathic dialogue requires nuanced temporal cues indicating whether the conversational partner is paying attention. This type of \"active listening\" is overlooked in the design of Conversational Agents (CAs), which use the same pacing for one conversation. To model the temporal cues in human conversation, we need CAs that dynamically adjust response pacing according to user input. We qualitatively analyzed ten cases of active listening to distill five context-aware pacing strategies: Reflective Silence, Facilitative Silence, Empathic Silence, Holding Space, and Immediate Response. In a between-subjects study (N=50) with two conversational scenarios (relationship and career-support), the context-aware agent scored higher than static-pacing control on perceived human-likeness, smoothness, and interactivity, supporting deeper self-disclosure and higher engagement. In the career support scenario, the CA yielded higher perceived listening quality and affective trust. This work shows how insights from human conversation like context-aware pacing can empower the design of more empathic human-AI communication.",
    "url": "https://arxiv.org/abs/2602.06134",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the importance of active listening in human interaction with Conversational Agents (CAs) and proposes context-aware pacing strategies to improve user experience. By dynamically adjusting response pacing based on user input, CAs can enhance perceived human-likeness, smoothness, and interactivity, leading to deeper self-disclosure and higher engagement. The study demonstrates that incorporating context-aware pacing in CAs can improve listening quality and build affective trust, ultimately enhancing human-AI communication."
  },
  {
    "title": "Git for Sketches: An Intelligent Tracking System for Capturing Design Evolution",
    "abstract": "During product conceptualization, capturing the non-linear history and cognitive intent is crucial. Traditional sketching tools often lose this context. We introduce DIMES (Design Idea Management and Evolution capture System), a web-based environment featuring sGIT (SketchGit), a custom visual version control architecture, and Generative AI. sGIT includes AEGIS, a module using hybrid Deep Learning and Machine Learning models to classify six stroke types. The system maps Git primitives to design actions, enabling implicit branching and multi-modal commits (stroke data + voice intent). In a comparative study, experts using DIMES demonstrated a 160% increase in breadth of concept exploration. Generative AI modules generated narrative summaries that enhanced knowledge transfer; novices achieved higher replication fidelity (Neural Transparency-based Cosine Similarity: 0.97 vs. 0.73) compared to manual summaries. AI-generated renderings also received higher user acceptance (Purchase Likelihood: 4.2 vs 3.1). This work demonstrates that intelligent version control bridges creative action and cognitive documentation, offering a new paradigm for design education.",
    "url": "https://arxiv.org/abs/2602.06047",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces DIMES, a system that combines visual version control architecture and Generative AI to capture design evolution and cognitive intent during product conceptualization. Experts using DIMES showed a significant increase in concept exploration, and AI-generated narrative summaries improved knowledge transfer and replication fidelity. The study suggests that intelligent version control can enhance design education by bridging creative action and cognitive documentation."
  },
  {
    "title": "Revisiting Emotions Representation for Recognition in the Wild",
    "abstract": "Facial emotion recognition has been typically cast as a single-label classification problem of one out of six prototypical emotions. However, that is an oversimplification that is unsuitable for representing the multifaceted spectrum of spontaneous emotional states, which are most often the result of a combination of multiple emotions contributing at different intensities. Building on this, a promising direction that was explored recently is to cast emotion recognition as a distribution learning problem. Still, such approaches are limited in that research datasets are typically annotated with a single emotion class. In this paper, we contribute a novel approach to describe complex emotional states as probability distributions over a set of emotion classes. To do so, we propose a solution to automatically re-label existing datasets by exploiting the result of a study in which a large set of both basic and compound emotions is mapped to probability distributions in the Valence-Arousal-Dominance (VAD) space. In this way, given a face image annotated with VAD values, we can estimate the likelihood of it belonging to each of the distributions, so that emotional states can be described as a mixture of emotions, enriching their description, while also accounting for the ambiguous nature of their perception. In a preliminary set of experiments, we illustrate the advantages of this solution and a new possible direction of investigation. Data annotations are available at this https URL.",
    "url": "https://arxiv.org/abs/2602.06778",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the limitations of traditional single-label emotion classification in facial emotion recognition and proposes a new approach using probability distributions to represent complex emotional states. By automatically re-labeling datasets based on Valence-Arousal-Dominance (VAD) values, emotional states can be described as mixtures of emotions, providing a more nuanced and accurate representation. The preliminary experiments show the advantages of this approach and suggest a new direction for future research in emotion recognition."
  },
  {
    "title": "\"Tab, Tab, Bug'': Security Pitfalls of Next Edit Suggestions in AI-Integrated IDEs",
    "abstract": "Modern AI-integrated IDEs are shifting from passive code completion to proactive Next Edit Suggestions (NES). Unlike traditional autocompletion, NES is designed to construct a richer context from both recent user interactions and the broader codebase to suggest multi-line, cross-line, or even cross-file modifications. This evolution significantly streamlines the programming workflow into a tab-by-tab interaction and enhances developer productivity. Consequently, NES introduces a more complex context retrieval mechanism and sophisticated interaction patterns. However, existing studies focus almost exclusively on the security implications of standalone LLM-based code generation, ignoring the potential attack vectors posed by NES in modern AI-integrated IDEs. The underlying mechanisms of NES remain under-explored, and their security implications are not yet fully understood.\nIn this paper, we conduct the first systematic security study of NES systems. First, we perform an in-depth dissection of the NES mechanisms to understand the newly introduced threat vectors. It is found that NES retrieves a significantly expanded context, including inputs from imperceptible user actions and global codebase retrieval, which increases the attack surfaces. Second, we conduct a comprehensive in-lab study to evaluate the security implications of NES. The evaluation results reveal that NES is susceptible to context poisoning and is sensitive to transactional edits and human-IDE interactions. Third, we perform a large-scale online survey involving over 200 professional developers to assess the perceptions of NES security risks in real-world development workflows. The survey results indicate a general lack of awareness regarding the potential security pitfalls associated with NES, highlighting the need for increased education and improved security countermeasures in AI-integrated IDEs.",
    "url": "https://arxiv.org/abs/2602.06759",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the security implications of Next Edit Suggestions (NES) in AI-integrated IDEs, which go beyond traditional code completion to suggest multi-line, cross-line, or cross-file modifications. The study reveals that NES systems have increased attack surfaces due to their expanded context retrieval mechanisms, making them susceptible to context poisoning and sensitive to user interactions. The findings emphasize the need for improved security education and countermeasures in AI-integrated IDEs to mitigate potential security risks."
  },
  {
    "title": "CytoCrowd: A Multi-Annotator Benchmark Dataset for Cytology Image Analysis",
    "abstract": "High-quality annotated datasets are crucial for advancing machine learning in medical image analysis. However, a critical gap exists: most datasets either offer a single, clean ground truth, which hides real-world expert disagreement, or they provide multiple annotations without a separate gold standard for objective evaluation. To bridge this gap, we introduce CytoCrowd, a new public benchmark for cytology analysis. The dataset features 446 high-resolution images, each with two key components: (1) raw, conflicting annotations from four independent pathologists, and (2) a separate, high-quality gold-standard ground truth established by a senior expert. This dual structure makes CytoCrowd a versatile resource. It serves as a benchmark for standard computer vision tasks, such as object detection and classification, using the ground truth. Simultaneously, it provides a realistic testbed for evaluating annotation aggregation algorithms that must resolve expert disagreements. We provide comprehensive baseline results for both tasks. Our experiments demonstrate the challenges presented by CytoCrowd and establish its value as a resource for developing the next generation of models for medical image analysis.",
    "url": "https://arxiv.org/abs/2602.06674",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces CytoCrowd, a benchmark dataset for cytology image analysis that addresses the lack of multiple annotations with a separate gold standard for evaluation. The dataset features conflicting annotations from four pathologists and a high-quality gold standard established by a senior expert, making it a versatile resource for computer vision tasks and evaluating annotation aggregation algorithms. The experiments highlight the challenges presented by CytoCrowd and establish its value for developing advanced models for medical image analysis."
  },
  {
    "title": "DAVE: Distribution-aware Attribution via ViT Gradient Decomposition",
    "abstract": "Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-level explanations, causing many existing methods to rely on coarse patch-level attributions. We introduce DAVE \\textit{(\\underline{D}istribution-aware \\underline{A}ttribution via \\underline{V}iT Gradient D\\underline{E}composition)}, a mathematically grounded attribution method for ViTs based on a structured decomposition of the input gradient. By exploiting architectural properties of ViTs, DAVE isolates locally equivariant and stable components of the effective input--output mapping. It separates these from architecture-induced artifacts and other sources of instability.",
    "url": "https://arxiv.org/abs/2602.06613",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces DAVE, a new attribution method for Vision Transformers (ViTs) that addresses the challenge of producing stable and high-resolution attribution maps. By decomposing the input gradient and isolating locally equivariant and stable components of the input-output mapping, DAVE is able to separate these from structured artifacts introduced by architectural components like patch embeddings and attention routing. This method allows for more accurate pixel-level explanations for ViTs, improving the interpretability of these models in computer vision tasks."
  },
  {
    "title": "MicroBi-ConvLSTM: An Ultra-Lightweight Efficient Model for Human Activity Recognition on Resource Constrained Devices",
    "abstract": "Human Activity Recognition (HAR) on resource constrained wearables requires models that balance accuracy against strict memory and computational budgets. State of the art lightweight architectures such as TinierHAR (34K parameters) and TinyHAR (55K parameters) achieve strong accuracy, but exceed memory budgets of microcontrollers with limited SRAM once operating system overhead is considered. We present MicroBi-ConvLSTM, an ultra-lightweight convolutional-recurrent architecture achieving 11.4K parameters on average through two stage convolutional feature extraction with 4x temporal pooling and a single bidirectional LSTM layer. This represents 2.9x parameter reduction versus TinierHAR and 11.9x versus DeepConvLSTM while preserving linear O(N) complexity. Evaluation across eight diverse HAR benchmarks shows that MicroBi-ConvLSTM maintains competitive performance within the ultra-lightweight regime: 93.41% macro F1 on UCI-HAR, 94.46% on SKODA assembly gestures, and 88.98% on Daphnet gait freeze detection. Systematic ablation reveals task dependent component contributions where bidirectionality benefits episodic event detection, but provides marginal gains on periodic locomotion. INT8 post training quantization incurs only 0.21% average F1-score degradation, yielding a 23.0 KB average deployment footprint suitable for memory constrained edge devices.",
    "url": "https://arxiv.org/abs/2602.06523",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces a new ultra-lightweight model, MicroBi-ConvLSTM, for human activity recognition on resource-constrained devices, with only 11.4K parameters. The model outperforms existing lightweight architectures like TinierHAR and TinyHAR while maintaining competitive performance on various benchmarks, making it suitable for deployment on memory-constrained edge devices. Additionally, the study shows that bidirectionality in the model benefits episodic event detection but provides marginal gains on periodic locomotion, and post-training quantization incurs minimal F1-score degradation."
  },
  {
    "title": "A High-Fidelity Robotic Manipulator Teleoperation Framework for Human-Centered Augmented Reality Evaluation",
    "abstract": "Validating Augmented Reality (AR) tracking and interaction models requires precise, repeatable ground-truth motion. However, human users cannot reliably perform consistent motion due to biomechanical variability. Robotic manipulators are promising to act as human motion proxies if they can mimic human movements. In this work, we design and implement ARBot, a real-time teleoperation platform that can effectively capture natural human motion and accurately replay the movements via robotic manipulators. ARBot includes two capture models: stable wrist motion capture via a custom CV and IMU pipeline, and natural 6-DOF control via a mobile application. We design a proactively-safe QP controller to ensure smooth, jitter-free execution of the robotic manipulator, enabling it to function as a high-fidelity record and replay physical proxy. We open-source ARBot and release a benchmark dataset of 132 human and synthetic trajectories captured using ARBot to support controllable and scalable AR evaluation.",
    "url": "https://arxiv.org/abs/2602.06273",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research presents ARBot, a teleoperation platform that uses robotic manipulators to accurately capture and replay natural human motion for Augmented Reality (AR) evaluation. The platform includes stable wrist motion capture and 6-DOF control via a mobile application, with a QP controller ensuring smooth execution of movements. The open-sourcing of ARBot and release of a benchmark dataset of human and synthetic trajectories support controllable and scalable AR evaluation, offering a promising tool for validating AR tracking and interaction models."
  },
  {
    "title": "Chasing Tails: How Do People Respond to Wait Time Distributions?",
    "abstract": "We use a series of pre-registered, incentive-compatible online experiments to investigate how people evaluate and choose among different waiting time distributions. Our main findings are threefold. First, consistent with prior literature, people show an aversion to both longer expected waits and higher variance. Second, and more surprisingly, moment-based utility models fail to capture preferences when distributions have thick-right tails: indeed, decision-makers strongly prefer distributions with long-right tails (where probability mass is more evenly distributed over a larger support set) relative to tails that exhibit a spike near the maximum possible value, even when controlling for mean, variance, and higher moments. Conditional Value at Risk (CVaR) utility models commonly used in portfolio theory predict these choices well. Third, when given a choice, decision-makers overwhelmingly seek information about right-tail outcomes. These results have practical implications for service operations: (1) service designs that create a spike in long waiting times (such as priority or dedicated queue designs) may be particularly aversive; (2) when informativeness is the goal, providers should prioritize sharing right-tail probabilities or percentiles; and (3) to increase service uptake, providers can strategically disclose (or withhold) distributional information depending on right-tail shape.",
    "url": "https://arxiv.org/abs/2602.06263",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores how people respond to different waiting time distributions and finds that individuals have a preference for distributions with long-right tails over those with spikes near the maximum value, even when controlling for mean, variance, and higher moments. The study suggests that decision-makers prioritize information about right-tail outcomes and that service designs should avoid creating spikes in long waiting times. These findings have practical implications for service operations, suggesting that providers should prioritize sharing right-tail probabilities or percentiles to increase service uptake."
  },
  {
    "title": "A Dialogue-Based Human-Robot Interaction Protocol for Wheelchair and Robotic Arm Integrated Control",
    "abstract": "People with lower and upper body disabilities can benefit from wheelchairs and robotic arms to improve mobility and independence. Prior assistive interfaces, such as touchscreens and voice-driven predefined commands, often remain unintuitive and struggle to capture complex user intent. We propose a natural, dialogue based human robot interaction protocol that simulates an intelligent agent capable of communicating with users to understand intent and execute assistive actions. In a pilot study, five participants completed five assistive tasks (cleaning, drinking, feeding, drawer opening, and door opening) through dialogue-based interaction with a wheelchair and robotic arm. As a baseline, participants were required to open a door using the manual control (a wheelchair joystick and a game controller for the arm) and complete a questionnaire to gather their feedback. By analyzing the post-study questionnaires, we found that most participants enjoyed the dialogue-based interaction and assistive robot autonomy.",
    "url": "https://arxiv.org/abs/2602.06243",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores a dialogue-based human-robot interaction protocol for controlling wheelchairs and robotic arms, aiming to improve mobility and independence for individuals with disabilities. The study involved participants completing various tasks through dialogue-based interaction, with positive feedback indicating enjoyment and satisfaction with the assistive robot autonomy. This research highlights the potential for more intuitive and effective assistive interfaces that can better capture complex user intent and improve the overall user experience."
  },
  {
    "title": "From Human-Human Collaboration to Human-Agent Collaboration: A Vision, Design Philosophy, and an Empirical Framework for Achieving Successful Partnerships Between Humans and LLM Agents",
    "abstract": "The emergence of Large Language Model (LLM) agents enables us to build agent-based intelligent systems that move beyond the role of a \"tool\" to become genuine collaborators with humans, thereby realizing a novel human-agent collaboration paradigm. Our vision is that LLM agents should resemble remote human collaborators, which allows HCI researchers to ground the future exploration in decades of research on trust, awareness, and common ground in remote human collaboration, while also revealing the unique opportunities and challenges that emerge when one or more partners are AI agents. This workshop establishes a foundational research agenda for the new era by posing the question: How can the rich understanding of remote human collaboration inspire and inform the design and study of human-agent collaboration? We will bring together an interdisciplinary group from HCI, CSCW, and AI to explore this critical transition. The 180-minute workshop will be highly interactive, featuring a keynote speaker, a series of invited lightning talks, and an exploratory group design session where participants will storyboard novel paradigms of human-agent partnership. Our goal is to enlighten the research community by cultivating a shared vocabulary and producing a research agenda that charts the future of collaborative agents.",
    "url": "https://arxiv.org/abs/2602.05987",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research explores the transition from human-human collaboration to human-agent collaboration using Large Language Model (LLM) agents. The vision is for LLM agents to become genuine collaborators with humans, resembling remote human collaborators, drawing on decades of research on trust, awareness, and common ground. The workshop aims to establish a research agenda for this new era by exploring how the understanding of remote human collaboration can inform the design and study of human-agent collaboration, bringing together experts from HCI, CSCW, and AI to chart the future of collaborative agents."
  },
  {
    "title": "Prompting Destiny: Negotiating Socialization and Growth in an LLM-Mediated Speculative Gameworld",
    "abstract": "We present an LLM-mediated role-playing game that supports reflection on socialization, moral responsibility, and educational role positioning. Grounded in socialization theory, the game follows a four-season structure in which players guide a child prince through morally charged situations and compare the LLM-mediated NPC's differentiated responses across stages, helping them reason about how educational guidance shifts with socialization. To approximate real educational contexts and reduce score-chasing, the system hides real-time evaluative scores and provides delayed, end-of-stage growth feedback as reflective prompts. We conducted a user study (N=12) with gameplay logs and post-game interviews, analyzed via reflexive thematic analysis. Findings show how players negotiated responsibility and role positioning, and reveal an entry-load tension between open-ended expression and sustained engagement. We contribute design knowledge on translating sociological models of socialization into reflective AI-mediated game systems.",
    "url": "https://arxiv.org/abs/2602.05864",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research presents a role-playing game that explores socialization, moral responsibility, and educational role positioning through a four-season structure. The game uses LLM-mediated NPC responses to prompt players to reflect on how educational guidance changes with socialization. Findings from a user study show how players navigated responsibility and role positioning, highlighting the tension between open-ended expression and sustained engagement in the game. This research contributes to the design of AI-mediated game systems that incorporate sociological models of socialization for reflective gameplay experiences."
  },
  {
    "title": "\"It Talks Like a Patient, But Feels Different\": Co-Designing AI Standardized Patients with Medical Learners",
    "abstract": "Standardized patients (SPs) play a central role in clinical communication training but are costly, difficult to scale, and inconsistent. Large language model (LLM) based AI standardized patients (AI-SPs) promise flexible, on-demand practice, yet learners often report that they talk like a patient but feel different. We interviewed 12 clinical-year medical students and conducted three co-design workshops to examine how learners experience constraints of SP encounters and what they expect from AI-SPs. We identified six learner-centered needs, translated them into AI-SP design requirements, and synthesized a conceptual workflow. Our findings position AI-SPs as tools for deliberate practice and show that instructional usability, rather than conversational realism alone, drives learner trust, engagement, and educational value.",
    "url": "https://arxiv.org/abs/2602.05856",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the development of AI-based standardized patients (AI-SPs) for clinical communication training in medical education. The study found that while AI-SPs offer flexible and on-demand practice, learners still perceive them as different from human standardized patients. By identifying learner-centered needs and translating them into design requirements, the study highlights the importance of instructional usability in driving learner trust, engagement, and educational value with AI-SPs."
  },
  {
    "title": "DuoDrama: Supporting Screenplay Refinement Through LLM-Assisted Human Reflection",
    "abstract": "AI has been increasingly integrated into screenwriting practice. In refinement, screenwriters expect AI to provide feedback that supports reflection across the internal perspective of characters and the external perspective of the overall story. However, existing AI tools cannot sufficiently coordinate the two perspectives to meet screenwriters' needs. To address this gap, we present DuoDrama, an AI system that generates feedback to assist screenwriters' reflection in refinement. To enable DuoDrama, based on performance theories and a formative study with nine professional screenwriters, we design the Experience-Grounded Feedback Generation Workflow for Human Reflection (ExReflect). In ExReflect, an AI agent adopts an experience role to generate experience and then shifts to an evaluation role to generate feedback based on the experience. A study with fourteen professional screenwriters shows that DuoDrama improves feedback quality and alignment and enhances the effectiveness, depth, and richness of reflection. We conclude by discussing broader implications and future directions.",
    "url": "https://arxiv.org/abs/2602.05854",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces DuoDrama, an AI system designed to assist screenwriters in refining their screenplays by providing feedback that supports reflection on both the internal perspective of characters and the external perspective of the overall story. Through the Experience-Grounded Feedback Generation Workflow for Human Reflection (ExReflect), DuoDrama improves feedback quality and alignment, enhancing the effectiveness, depth, and richness of reflection for professional screenwriters. This research highlights the significance of integrating AI into the screenwriting process to improve the overall quality of scripts and support writers in their creative endeavors."
  },
  {
    "title": "Large Data Acquisition and Analytics at Synchrotron Radiation Facilities",
    "abstract": "Synchrotron facilities like the Cornell High Energy Synchrotron Source (CHESS) generate massive data volumes from complex beamline experiments, but face challenges such as limited access time, the need for on-site experiment monitoring, and managing terabytes of data per user group. We present the design, deployment, and evaluation of a framework that addresses CHESS's data acquisition and management issues. Deployed on a secure CHESS server, our system provides real time, web-based tools for remote experiment monitoring and data quality assessment, improving operational efficiency. Implemented across three beamlines (ID3A, ID3B, ID4B), the framework managed 50-100 TB of data and over 10 million files in late 2024. Testing with 43 research groups and 86 dashboards showed reduced overhead, improved accessibility, and streamlined data workflows. Our paper highlights the development, deployment, and evaluation of our framework and its transformative impact on synchrotron data acquisition.",
    "url": "https://arxiv.org/abs/2602.05837",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research focuses on addressing data acquisition and management challenges faced by synchrotron facilities like CHESS. The framework developed and deployed at CHESS provides real-time tools for remote monitoring and data quality assessment, leading to improved operational efficiency. The framework managed large volumes of data across multiple beamlines and showed significant improvements in overhead reduction, accessibility, and data workflow streamlining."
  },
  {
    "title": "Whispers of the Butterfly: A Research-through-Design Exploration of In-Situ Conversational AI Guidance in Large-Scale Outdoor MR Exhibitions",
    "abstract": "Large-scale outdoor mixed reality (MR) art exhibitions distribute curated virtual works across open public spaces, but interpretation rarely scales without turning exploration into a scripted tour. Through Research-through-Design, we created Dream-Butterfly, an in-situ conversational AI docent embodied as a small non-human companion that visitors summon for multilingual, exhibition-grounded explanations. We deployed Dream-Butterfly in a large-scale outdoor MR exhibition at a public university campus in southern China, and conducted an in-the-wild between-subject study (N=24) comparing a primarily human-led tour with an AI-led tour while keeping staff for safety in both conditions. Combining questionnaires and semi-structured interviews, we characterize how shifting the primary explanation channel reshapes explanation access, perceived responsiveness, immersion, and workload, and how visitors negotiate responsibility handoffs among staff, the AI guide, and themselves. We distill transferable design implications for configuring mixed human-AI guiding roles and embodying conversational agents in mobile, safety-constrained outdoor MR exhibitions.",
    "url": "https://arxiv.org/abs/2602.05826",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of conversational AI guidance in large-scale outdoor mixed reality art exhibitions to provide multilingual explanations to visitors. The study found that shifting the primary explanation channel to AI reshapes visitors' experience in terms of explanation access, perceived responsiveness, immersion, and workload. The findings provide design implications for incorporating AI-guided roles and conversational agents in mobile, safety-constrained outdoor MR exhibitions."
  },
  {
    "title": "ToMigo: Interpretable Design Concept Graphs for Aligning Generative AI with Creative Intent",
    "abstract": "Generative AI often produces results misaligned with user intentions, for example, resolving ambiguous prompts in unexpected ways. Despite existing approaches to clarify intent, a major challenge remains: understanding and influencing AI's interpretation of user intent through simple, direct inputs requiring no expertise or rigid procedures. We present ToMigo, representing intent as design concept graphs: nodes represent choices of purpose, content, or style, while edges link them with interpretable explanations. Applied to graphic design, ToMigo infers intent from reference images and text. We derived a schema of node types and edges from pre-study data, informing a multimodal large language model to generate graphs aligning nodes externally with user intent and internally toward a unified design goal. This structure enables users to explore AI reasoning and directly manipulate the design concept. In our user studies, ToMigo received high alignment ratings and captured most user intentions well. Users reported greater control and found interactive features-editable graphs, reflective chats, concept-design realignment-useful for evolving and realizing their design ideas.",
    "url": "https://arxiv.org/abs/2602.05825",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research introduces ToMigo, a method for aligning generative AI with user intent through design concept graphs that represent intent as nodes and edges. By inferring intent from reference images and text, ToMigo allows users to explore AI reasoning, manipulate design concepts, and achieve greater control over the creative process. User studies showed high alignment ratings and successful capture of user intentions, demonstrating the effectiveness of ToMigo in facilitating the evolution and realization of design ideas."
  },
  {
    "title": "Authorship Drift: How Self-Efficacy and Trust Evolve During LLM-Assisted Writing",
    "abstract": "Large language models (LLMs) are increasingly used as collaborative partners in writing. However, this raises a critical challenge of authorship, as users and models jointly shape text across interaction turns. Understanding authorship in this context requires examining users' evolving internal states during collaboration, particularly self-efficacy and trust. Yet, the dynamics of these states and their associations with users' prompting strategies and authorship outcomes remain underexplored. We examined these dynamics through a study of 302 participants in LLM-assisted writing, capturing interaction logs and turn-by-turn self-efficacy and trust ratings. Our analysis showed that collaboration generally decreased users' self-efficacy while increasing trust. Participants who lost self-efficacy were more likely to ask the LLM to edit their work directly, whereas those who recovered self-efficacy requested more review and feedback. Furthermore, participants with stable self-efficacy showed higher actual and perceived authorship of the final text. Based on these findings, we propose design implications for understanding and supporting authorship in human-LLM collaboration.",
    "url": "https://arxiv.org/abs/2602.05819",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores how self-efficacy and trust evolve during collaborative writing with large language models (LLMs). The study found that collaboration with LLMs generally decreased users' self-efficacy while increasing trust, with participants who lost self-efficacy more likely to ask the LLM to edit their work directly. Participants with stable self-efficacy showed higher actual and perceived authorship of the final text, suggesting design implications for supporting authorship in human-LLM collaboration."
  },
  {
    "title": "Exploring AI-Augmented Sensemaking of Patient-Generated Health Data: A Mixed-Method Study with Healthcare Professionals in Cardiac Risk Reduction",
    "abstract": "Individuals are increasingly generating substantial personal health and lifestyle data, e.g. through wearables and smartphones. While such data could transform preventative care, its integration into clinical practice is hindered by its scale, heterogeneity and the time pressure and data literacy of healthcare professionals (HCPs). We explore how large language models (LLMs) can support sensemaking of patient-generated health data (PGHD) with automated summaries and natural language data exploration. Using cardiovascular disease (CVD) risk reduction as a use case, 16 HCPs reviewed multimodal PGHD in a mixed-methods study with a prototype that integrated common charts, LLM-generated summaries, and a conversational interface. Findings show that AI summaries provided quick overviews that anchored exploration, while conversational interaction supported flexible analysis and bridged data-literacy gaps. However, HCPs raised concerns about transparency, privacy, and overreliance. We contribute empirical insights and sociotechnical design implications for integrating AI-driven summarization and conversation into clinical workflows to support PGHD sensemaking.",
    "url": "https://arxiv.org/abs/2602.05687",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores how large language models can assist healthcare professionals in making sense of patient-generated health data, specifically in the context of cardiac risk reduction. The study found that AI-generated summaries provided quick overviews and supported data exploration, while conversational interfaces helped bridge data-literacy gaps. However, concerns were raised about transparency, privacy, and overreliance on AI. The findings suggest potential benefits and challenges in integrating AI-driven tools into clinical workflows to enhance the sensemaking of patient-generated health data."
  },
  {
    "title": "(Computer) Vision in Action: Comparing Remote Sighted Assistance and a Multimodal Voice Agent in Inspection Sequences",
    "abstract": "Does human-AI assistance unfold in the same way as human-human assistance? This research explores what can be learned from the expertise of blind individuals and sighted volunteers to inform the design of multimodal voice agents and address the enduring challenge of proactivity. Drawing on granular analysis of two representative fragments from a larger corpus, we contrast the practices co-produced by an experienced human remote sighted assistant and a blind participant-as they collaborate to find a stain on a blanket over the phone-with those achieved when the same participant worked with a multimodal voice agent on the same task, a few moments earlier. This comparison enables us to specify precisely which fundamental proactive practices the agent did not enact in situ. We conclude that, so long as multimodal voice agents cannot produce environmentally occasioned vision-based actions, they will lack a key resource relied upon by human remote sighted assistants.",
    "url": "https://arxiv.org/abs/2602.05671",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research compares the effectiveness of human remote sighted assistance and a multimodal voice agent in assisting blind individuals with inspection tasks. The study found that the voice agent was unable to enact certain proactive practices that human assistants could, specifically related to vision-based actions. This highlights the importance of incorporating visually-based actions into AI systems to improve their effectiveness in assisting individuals with visual impairments."
  },
  {
    "title": "Making AI Agents Evaluate Misleading Charts without Nudging",
    "abstract": "AI agents are increasingly used as low-cost proxies for early visualization evaluation. In an initial study of deliberately flawed charts, we test whether agents spontaneously penalise chart junk and misleading encodings without being prompted to look for errors. Using established scales (BeauVis and PREVis), the agent evaluated visualizations containing decorative clutter, manipulated axes, and distorted proportional cues. The ratings of aesthetic appeal and perceived readability often remained relatively high even when graphical integrity was compromised. These results suggest that un-nudged AI agent evaluation may underweight integrity-related defects unless such checks are explicitly elicited.",
    "url": "https://arxiv.org/abs/2602.05662",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research examines whether AI agents can detect and penalize misleading charts without being prompted to do so. The study found that AI agents often rated visualizations containing errors, such as decorative clutter and manipulated axes, as aesthetically appealing and readable, even when graphical integrity was compromised. This suggests that un-nudged AI agent evaluation may not prioritize integrity-related defects unless specifically prompted to do so."
  },
  {
    "title": "AI chatbots versus human healthcare professionals: a systematic review and meta-analysis of empathy in patient care",
    "abstract": "Background: Empathy is widely recognized for improving patient outcomes, including reduced pain and anxiety and improved satisfaction, and its absence can cause harm. Meanwhile, use of artificial intelligence (AI)-based chatbots in healthcare is rapidly expanding, with one in five general practitioners using generative AI to assist with tasks such as writing letters. Some studies suggest AI chatbots can outperform human healthcare professionals (HCPs) in empathy, though findings are mixed and lack synthesis.\nSources of data: We searched multiple databases for studies comparing AI chatbots using large language models with human HCPs on empathy measures. We assessed risk of bias with ROBINS-I and synthesized findings using random-effects meta-analysis where feasible, whilst avoiding double counting.\nAreas of agreement: We identified 15 studies (2023-2024). Thirteen studies reported statistically significantly higher empathy ratings for AI, with only two studies situated in dermatology favouring human responses. Of the 15 studies, 13 provided extractable data and were suitable for pooling. Meta-analysis of those 13 studies, all utilising ChatGPT-3.5/4, showed a standardized mean difference of 0.87 (95% CI, 0.54-1.20) favouring AI (P < .00001), roughly equivalent to a two-point increase on a 10-point scale.\nAreas of controversy: Studies relied on text-based assessments that overlook non-verbal cues and evaluated empathy through proxy raters.\nGrowing points: Our findings indicate that, in text-only scenarios, AI chatbots are frequently perceived as more empathic than human HCPs.\nAreas timely for developing research: Future research should validate these findings with direct patient evaluations and assess whether emerging voice-enabled AI systems can deliver similar empathic advantages.",
    "url": "https://arxiv.org/abs/2602.05628",
    "journal": "arXiv cs.HC",
    "ai_summary": "This systematic review and meta-analysis compared the empathy of AI chatbots to human healthcare professionals in patient care. The study found that in text-only scenarios, AI chatbots, specifically ChatGPT-3.5/4, were perceived as more empathic than human HCPs, with a standardized mean difference of 0.87 favoring AI. The findings suggest the potential for AI chatbots to improve patient outcomes through enhanced empathy, but further research is needed to validate these results with direct patient evaluations and explore the impact of voice-enabled AI systems."
  },
  {
    "title": "Assessing Problem-Solving in HR Contexts: A Comparison Between Game-Based and Self-Report Measures",
    "abstract": "Game-based assessments (GBAs) are increasingly adopted in recruitment contexts as tools to assess transversal skills through observable behavior. However, empirical evidence directly comparing game-based behavioral indicators with traditional self-report measures remains limited. This study adopts a method-comparison approach to explore the convergence between self-perceived and behaviorally enacted problem-solving competence, comparing a game-based assessment with the Problem Solving Inventory (PSI-B).\nSeventy-eight participants completed both the PSI-B and a five-minute game-based problem-solving task, which classified performance into four behavioral proficiency levels. Results revealed no significant convergence between self-reported and behavior-based problem-solving scores, indicating a lack of convergence between the two measurement modalities.\nRather than indicating a lack of validity of the game-based assessment, these findings support the view that self-report and behavioral measures provide complementary information about problem-solving competence. The study highlights the risks of relying on a single assessment modality in personnel selection and underscores the value of integrating game-based tools within multi-method assessment frameworks.",
    "url": "https://arxiv.org/abs/2602.05525",
    "journal": "arXiv cs.HC",
    "ai_summary": "This study compared game-based assessments with traditional self-report measures in assessing problem-solving competence in HR contexts. The results showed a lack of convergence between self-reported and behavior-based problem-solving scores, suggesting that both assessment modalities provide complementary information. The findings emphasize the importance of integrating game-based tools within multi-method assessment frameworks to improve personnel selection processes."
  },
  {
    "title": "Relying on LLMs: Student Practices and Instructor Norms are Changing in Computer Science Education",
    "abstract": "Prior research has raised concerns about students' over-reliance on large language models (LLMs) in higher education. This paper examines how Computer Science students and instructors engage with LLMs across five scenarios: \"Writing\", \"Quiz\", \"Programming\", \"Project-based learning\", and \"Information retrieval\". Through user studies with 16 students and 6 instructors, we identify 7 key intents, including increasingly complex student practices. Findings reveal varying levels of conflict between student practices and instructor norms, ranging from clear conflict in \"Writing-generation\" and \"(Programming) quiz-solving\", through partial conflict in \"Programming project-implementation\" and \"Project-based learning\", to broad agreement in \"Writing-revision & ideation\", \"(Programming) quiz-correction\" and \"Info-query & summary\". We document instructors are shifting from prohibiting to recognizing students' use of LLMs for high-quality work, integrating usage records into assessment grading. Finally, we propose LLM design guidelines: deploying default guardrails with game-like and empathetic interaction to prevent students from \"deserting\" LLMs, especially for \"Writing-generation\", while utilizing comprehension checks in low-conflict intents to promote learning.",
    "url": "https://arxiv.org/abs/2602.05506",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research paper explores how Computer Science students and instructors are utilizing large language models (LLMs) in various educational scenarios. The study identifies different levels of conflict between student practices and instructor norms, with instructors increasingly recognizing and integrating LLM usage into assessments for high-quality work. The findings suggest the need for LLM design guidelines, including implementing default guardrails and comprehension checks to guide student usage and promote learning outcomes."
  },
  {
    "title": "DiLLS: Interactive Diagnosis of LLM-based Multi-agent Systems via Layered Summary of Agent Behaviors",
    "abstract": "Large language model (LLM)-based multi-agent systems have demonstrated impressive capabilities in handling complex tasks. However, the complexity of agentic behaviors makes these systems difficult to understand. When failures occur, developers often struggle to identify root causes and to determine actionable paths for improvement. Traditional methods that rely on inspecting raw log records are inefficient, given both the large volume and complexity of data. To address this challenge, we propose a framework and an interactive system, DiLLS, designed to reveal and structure the behaviors of multi-agent systems. The key idea is to organize information across three levels of query completion: activities, actions, and operations. By probing the multi-agent system through natural language, DiLLS derives and organizes information about planning and execution into a structured, multi-layered summary. Through a user study, we show that DiLLS significantly improves developers' effectiveness and efficiency in identifying, diagnosing, and understanding failures in LLM-based multi-agent systems.",
    "url": "https://arxiv.org/abs/2602.05446",
    "journal": "arXiv cs.HC",
    "ai_summary": "The research proposes a framework and interactive system, DiLLS, to help developers understand and diagnose failures in LLM-based multi-agent systems. By organizing information across three levels of query completion, DiLLS provides a structured, multi-layered summary of agent behaviors. A user study demonstrates that DiLLS significantly improves developers' effectiveness and efficiency in identifying and diagnosing issues in these complex systems."
  },
  {
    "title": "Co-Designing Collaborative Generative AI Tools for Freelancers",
    "abstract": "Most generative AI tools prioritize individual productivity and personalization, with limited support for collaboration. Designed for traditional workplaces, these tools do not fit freelancers' short-term teams or lack of shared institutional support, which can worsen their isolation and overlook freelancing platform dynamics. This mismatch means that, instead of empowering freelancers, current generative AI tools could reinforce existing precarity and make freelancer collaboration harder. To investigate how to design generative AI tools to support freelancer collaboration, we conducted co-design sessions with 27 freelancers. A key concern that emerged was the risk of AI systems compromising their creative agency and work identities when collaborating, especially when AI tools could reproduce content without attribution, threatening the authenticity and distinctiveness of their collaborative work. Freelancers proposed \"auxiliary AI\" systems, human-guided tools that support their creative agencies and identities, allowing for flexible freelancer-led collaborations that promote \"productive friction\". Drawing on Marcuse's concept of technological rationality, we argue that freelancers are resisting one-dimensional, efficiency-driven AI, and instead envisioning technologies that preserve their collective creative agencies. We conclude with design recommendations for collaborative generative AI tools for freelancers.",
    "url": "https://arxiv.org/abs/2602.05299",
    "journal": "arXiv cs.HC",
    "ai_summary": "Current generative AI tools do not support collaboration among freelancers, which can worsen their isolation and overlook platform dynamics. Freelancers are concerned about AI compromising their creative agency and work identities, and propose the use of \"auxiliary AI\" systems to support their creative processes. By resisting efficiency-driven AI and promoting \"productive friction\", freelancers are seeking technologies that preserve their collective creative agencies."
  },
  {
    "title": "Varifocal Displays Reduce the Impact of the Vergence-Accommodation Conflict on 3D Pointing Performance in Augmented Reality Systems",
    "abstract": "This paper investigates whether a custom varifocal display can improve 3D pointing performance in augmented reality (AR), where the vergence-accommodation conflict (VAC) is known to impair interaction. Varifocal displays have been hypothesized to alleviate the VAC by dynamically matching the focal distance to the user's gaze-defined target depth. Following prior work, we conducted a within-subject study with 24 participants performing an ISO 9241-411 pointing task under varifocal and fixed-focal viewing. Overall, varifocal viewing yielded significantly higher performance than the fixed-focal baseline across key interaction metrics, although the magnitude and even the direction of the benefit varied across individuals. In particular, participants' responses exhibited a baseline-dependent pattern, with smaller improvements (or occasional degradation) observed for those with better baseline performance. Our findings suggest that varifocal technology can improve AR pointing performance relative to fixed-focal viewing, while highlighting substantial individual differences that should be considered in design and evaluation.",
    "url": "https://arxiv.org/abs/2602.05129",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the use of varifocal displays to improve 3D pointing performance in augmented reality systems by reducing the vergence-accommodation conflict. The study found that varifocal viewing resulted in significantly higher performance compared to fixed-focal viewing, although the extent of improvement varied among individuals. These findings suggest that varifocal technology has the potential to enhance AR interaction, but individual differences in baseline performance should be taken into account in design and evaluation."
  },
  {
    "title": "Reporting and Reviewing LLM-Integrated Systems in HCI: Challenges and Considerations",
    "abstract": "What should HCI scholars consider when reporting and reviewing papers that involve LLM-integrated systems? We interview 18 authors of LLM-integrated system papers on their authoring and reviewing experiences. We find that norms of trust-building between authors and reviewers appear to be eroded by the uncertainty of LLM behavior and hyperbolic rhetoric surrounding AI. Authors perceive that reviewers apply uniquely skeptical and inconsistent standards towards papers that report LLM-integrated systems, and mitigate mistrust by adding technical evaluations, justifying usage, and de-emphasizing LLM presence. Authors' views challenge blanket directives to report all prompts and use open models, arguing that prompt reporting is context-dependent and justifying proprietary model usage despite ethical concerns. Finally, some tensions in peer review appear to stem from clashes between the norms and values of HCI and ML/NLP communities, particularly around what constitutes a contribution and an appropriate level of technical rigor. Based on our findings and additional feedback from six expert HCI researchers, we present a set of guidelines and considerations for authors, reviewers, and HCI communities around reporting and reviewing papers that involve LLM-integrated systems.",
    "url": "https://arxiv.org/abs/2602.05128",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the challenges and considerations for reporting and reviewing papers on LLM-integrated systems in the field of HCI. Authors of such papers perceive that reviewers apply skeptical and inconsistent standards, leading to mistrust and the need to justify the use of LLMs. The study highlights tensions between HCI and ML/NLP communities in terms of what constitutes a contribution and appropriate technical rigor, and offers guidelines for authors, reviewers, and HCI communities in reporting and reviewing LLM-integrated systems."
  },
  {
    "title": "Metacognitive Demands and Strategies While Using Off-The-Shelf AI Conversational Agents for Health Information",
    "abstract": "As Artificial Intelligence (AI) conversational agents become widespread, people are increasingly using them for health information seeking. The use of off-the-shelf conversational agents for health information seeking could place high metacognitive demands (the need for extensive monitoring and control of one's own thought process) on individuals, which could compromise their experience of seeking health information. However, currently, the specific demands that arise while using conversational agents for health information seeking, and the strategies people use to cope with those demands, remain unknown. To address these gaps, we conducted a think-aloud study with 15 participants as they sought health information using our off-the-shelf AI conversational agent. We identified the metacognitive demands such systems impose, the strategies people adopt in response, and propose considerations for designing beyond off-the-shelf interfaces to reduce these demands and support better user experiences and affordances in health information seeking.",
    "url": "https://arxiv.org/abs/2602.05111",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research study focused on the metacognitive demands and strategies individuals face when using off-the-shelf AI conversational agents for health information seeking. The study found that using these agents can place high metacognitive demands on users, impacting their experience. By identifying these demands and strategies, the research provides insights for designing interfaces that reduce these demands and enhance user experiences in health information seeking."
  },
  {
    "title": "VR Calm Plus: Coupling a Squeezable Tangible Interaction with Immersive VR for Stress Regulation",
    "abstract": "While Virtual Reality (VR) is increasingly employed for stress management, most applications rely heavily on audio-visual stimuli and overlook the therapeutic potential of squeezing engagement. To address this gap, we introduce VR Calm Plus, a multimodal system that integrates a pressure-sensitive plush toy into an interactive VR environment. This interface allows users to dynamically modulate the virtual atmosphere through physical squeezing actions, fostering a deeper sense of embodied relaxation. We evaluated the system with 40 participants using PANAS-X surveys, subjective questionnaires, physiological measures (heart rate, skin conductance, pulse rate variability), and semi-structured interviews. Results demonstrate that, compared to a visual-only baseline, squeeze-based interaction significantly enhances positive affect and perceived relaxation. Physiological data further revealed a state of \"active relaxation\", characterized by greater reductions in heart rate and preserved autonomic flexibility (PRV), alongside sustained emotional engagement (GSR). Our findings highlight the value of coupling tangible input with immersive environments to support emotional well-being and offer design insights for future VR-based mental health tools.",
    "url": "https://arxiv.org/abs/2602.05093",
    "journal": "arXiv cs.HC",
    "ai_summary": "The study introduces VR Calm Plus, a system that combines a pressure-sensitive plush toy with immersive VR to help regulate stress. The research found that the squeeze-based interaction significantly increased positive affect and perceived relaxation compared to visual-only stimuli. Physiological data also showed a state of \"active relaxation\" with reduced heart rate and preserved autonomic flexibility, suggesting the potential of coupling tangible input with VR for emotional well-being."
  },
  {
    "title": "A Design Space for Live Music Agents",
    "abstract": "Live music provides a uniquely rich setting for studying creativity and interaction due to its spontaneous nature. The pursuit of live music agents--intelligent systems supporting real-time music performance and interaction--has captivated researchers across HCI, AI, and computer music for decades, and recent advancements in AI suggest unprecedented opportunities to evolve their design. However, the interdisciplinary nature of music has led to fragmented development across research communities, hindering effective communication and collaborative progress. In this work, we bring together perspectives from these diverse fields to map the current landscape of live music agents. Based on our analysis of 184 systems across both academic literature and video, we develop a comprehensive design space that categorizes dimensions spanning usage contexts, interactions, technologies, and ecosystems. By highlighting trends and gaps in live music agents, our design space offers researchers, designers, and musicians a structured lens to understand existing systems and shape future directions in real-time human-AI music co-creation. We release our annotated systems as a living artifact at this https URL.",
    "url": "https://arxiv.org/abs/2602.05064",
    "journal": "arXiv cs.HC",
    "ai_summary": "This research explores the design space of live music agents, intelligent systems that support real-time music performance and interaction. By analyzing 184 systems from academic literature and video, the researchers develop a comprehensive design space that categorizes dimensions such as usage contexts, interactions, technologies, and ecosystems. This work provides a structured lens for researchers, designers, and musicians to understand existing systems and shape future directions in real-time human-AI music co-creation."
  }
]