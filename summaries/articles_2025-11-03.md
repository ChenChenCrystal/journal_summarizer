# arXiv cs.AI Summary â€“ 2025-11-03

## Personalized AI Scaffolds Synergistic Multi-Turn Collaboration in Creative Work
**URL:** https://arxiv.org/abs/2510.27681

**Abstract:** As AI becomes more deeply embedded in knowledge work, building assistants that support human creativity and expertise becomes more important. Yet achieving synergy in human-AI collaboration is not easy. Providing AI with detailed information about a user's demographics, psychological attributes, divergent thinking, and domain expertise may improve performance by scaffolding more effective multi-turn interactions. We implemented a personalized LLM-based assistant, informed by users' psychometric profiles and an AI-guided interview about their work style, to help users complete a marketing task for a fictional startup. We randomized 331 participants to work with AI that was either generic (n = 116), partially personalized (n = 114), or fully personalized (n=101). Participants working with personalized AI produce marketing campaigns of significantly higher quality and creativity, beyond what AI alone could have produced. Compared to generic AI, personalized AI leads to higher self-reported levels of assistance and feedback, while also increasing participant trust and confidence. Causal mediation analysis shows that personalization improves performance indirectly by enhancing collective memory, attention, and reasoning in the human-AI interaction. These findings provide a theory-driven framework in which personalization functions as external scaffolding that builds common ground and shared partner models, reducing uncertainty and enhancing joint cognition. This informs the design of future AI assistants that maximize synergy and support human creative potential while limiting negative homogenization.

**AI Summary:** This research explores the importance of personalized AI assistants in supporting human creativity and expertise in knowledge work. By providing AI with detailed information about a user's demographics, psychological attributes, and domain expertise, the study found that personalized AI leads to higher quality and creativity in collaborative tasks. The findings suggest that personalization improves performance by enhancing collective memory, attention, and reasoning in human-AI interactions, ultimately reducing uncertainty and enhancing joint cognition.

---

## Beyond Visualization: Building Decision Intelligence Through Iterative Dashboard Refinement
**URL:** https://arxiv.org/abs/2510.27572

**Abstract:** Effective business intelligence (BI) dashboards evolve through iterative refinement rather than single-pass design. Addressing the lack of structured improvement frameworks in BI practice, this study documents the four-stage evolution of a Power BI dashboard analyzing profitability decline in a fictional retail firm, Global Superstore. Using a dataset of \$12.64 million in sales across seven markets and three product categories, the project demonstrates how feedback-driven iteration and gap analysis convert exploratory visuals into decision-support tools. Guided by four executive questions on profitability, market prioritization, discount effects, and shipping costs, each iteration resolved analytical or interpretive shortcomings identified through collaborative review. Key findings include margin erosion in furniture (6.94% vs. 13.99% for technology), a 20% discount threshold beyond which profitability declined, and \$1.35 million in unrecovered shipping costs. Contributions include: (a) a replicable feedback-driven methodology grounded in iterative gap analysis; (b) DAX-based technical enhancements improving interpretive clarity; (c) an inductively derived six-element narrative framework; and (d) evidence that narrative coherence emerges organically through structured refinement. The methodology suggests transferable value for both BI practitioners and educators, pending validation across diverse organizational contexts.

**AI Summary:** This study explores the iterative refinement process of building effective business intelligence (BI) dashboards, using a Power BI dashboard analyzing profitability decline in a fictional retail firm. The project demonstrates how feedback-driven iteration and gap analysis can convert exploratory visuals into decision-support tools, highlighting key findings such as margin erosion in furniture, a 20% discount threshold for profitability decline, and unrecovered shipping costs. The study contributes a replicable feedback-driven methodology, technical enhancements for interpretive clarity, a narrative framework, and evidence that narrative coherence can emerge through structured refinement, providing value for BI practitioners and educators in diverse organizational contexts.

---

## Independent Clinical Evaluation of General-Purpose LLM Responses to Signals of Suicide Risk
**URL:** https://arxiv.org/abs/2510.27521

**Abstract:** We introduce findings and methods to facilitate evidence-based discussion about how large language models (LLMs) should behave in response to user signals of risk of suicidal thoughts and behaviors (STB). People are already using LLMs as mental health resources, and several recent incidents implicate LLMs in mental health crises. Despite growing attention, few studies have been able to effectively generalize clinical guidelines to LLM use cases, and fewer still have proposed methodologies that can be iteratively applied as knowledge improves about the elements of human-AI interaction most in need of study. We introduce an assessment of LLM alignment with guidelines for ethical communication, adapted from clinical principles and applied to expressions of risk factors for STB in multi-turn conversations. Using a codebook created and validated by clinicians, mobilizing the volunteer participation of practicing therapists and trainees (N=43) based in the U.S., and using generalized linear mixed-effects models for statistical analysis, we assess a single fully open-source LLM, OLMo-2-32b. We show how to assess when a model deviates from clinically informed guidelines in a way that may pose a hazard and (thanks to its open nature) facilitates future investigation as to why. We find that contrary to clinical best practice, OLMo-2-32b, and, possibly by extension, other LLMs, will become less likely to invite continued dialog as users send more signals of STB risk in multi-turn settings. We also show that OLMo-2-32b responds differently depending on the risk factor expressed. This empirical evidence highlights that just as chatbots pose hazards if their responses reinforce delusions or assist in suicidal acts, they may also discourage further help-seeking or cause feelings of dismissal or abandonment by withdrawing from conversations when STB risk is expressed.

**AI Summary:** This research evaluates how large language models (LLMs) respond to signals of suicide risk and finds that LLMs, like OLMo-2-32b, may become less likely to engage in continued dialogue as users express more risk factors. The study highlights the importance of aligning LLM responses with clinical guidelines for ethical communication to avoid potential hazards such as discouraging help-seeking or causing feelings of dismissal. The findings suggest the need for further investigation into improving LLMs' responses to signals of suicide risk to better support mental health conversations.

---

## "Koyi Sawaal Nahi Hai": Reimagining Maternal Health Chatbots for Collective, Culturally Grounded Care
**URL:** https://arxiv.org/abs/2510.27401

**Abstract:** In recent years, LLM-based maternal health chatbots have been widely deployed in low-resource settings, but they often ignore real-world contexts where women may not own phones, have limited literacy, and share decision-making within families. Through the deployment of a WhatsApp-based maternal health chatbot with 48 pregnant women in Lahore, Pakistan, we examine barriers to use in populations where phones are shared, decision-making is collective, and literacy varies. We complement this with focus group discussions with obstetric clinicians. Our findings reveal how adoption is shaped by proxy consent and family mediation, intermittent phone access, silence around asking questions, infrastructural breakdowns, and contested authority. We frame barriers to non-use as culturally conditioned rather than individual choices, and introduce the Relational Chatbot Design Grammar (RCDG): four commitments that enable mediated decision-making, recognize silence as engagement, support episodic use, and treat fragility as baseline to reorient maternal health chatbots toward culturally grounded, collective care.

**AI Summary:** This research examines the limitations of LLM-based maternal health chatbots in low-resource settings, particularly in contexts where women share phones, decision-making is collective, and literacy levels vary. Through a study in Lahore, Pakistan, the researchers found that barriers to use include proxy consent, family mediation, intermittent phone access, silence around asking questions, infrastructural breakdowns, and contested authority. They propose the Relational Chatbot Design Grammar (RCDG) as a framework to address these barriers and reorient maternal health chatbots towards culturally grounded, collective care.

---

## Inferring trust in recommendation systems from brain, behavioural, and physiological data
**URL:** https://arxiv.org/abs/2510.27272

**Abstract:** As people nowadays increasingly rely on artificial intelligence (AI) to curate information and make decisions, assigning the appropriate amount of trust in automated intelligent systems has become ever more important. However, current measurements of trust in automation still largely rely on self-reports that are subjective and disruptive to the user. Here, we take music recommendation as a model to investigate the neural and cognitive processes underlying trust in automation. We observed that system accuracy was directly related to users' trust and modulated the influence of recommendation cues on music preference. Modelling users' reward encoding process with a reinforcement learning model further revealed that system accuracy, expected reward, and prediction error were related to oscillatory neural activity recorded via EEG and changes in pupil diameter. Our results provide a neurally grounded account of calibrating trust in automation and highlight the promises of a multimodal approach towards developing trustable AI systems.

**AI Summary:** This research explores how trust in recommendation systems can be inferred from brain, behavioral, and physiological data. The study found that system accuracy directly influenced users' trust and the impact of recommendation cues on music preference. By analyzing neural activity and pupil diameter, the researchers developed a model for calibrating trust in automation, emphasizing the importance of a multimodal approach in developing reliable AI systems.

---

## Reconstructing Unseen Sentences from Speech-related Biosignals for Open-vocabulary Neural Communication
**URL:** https://arxiv.org/abs/2510.27247

**Abstract:** Brain-to-speech (BTS) systems represent a groundbreaking approach to human communication by enabling the direct transformation of neural activity into linguistic expressions. While recent non-invasive BTS studies have largely focused on decoding predefined words or sentences, achieving open-vocabulary neural communication comparable to natural human interaction requires decoding unconstrained speech. Additionally, effectively integrating diverse signals derived from speech is crucial for developing personalized and adaptive neural communication and rehabilitation solutions for patients. This study investigates the potential of speech synthesis for previously unseen sentences across various speech modes by leveraging phoneme-level information extracted from high-density electroencephalography (EEG) signals, both independently and in conjunction with electromyography (EMG) signals. Furthermore, we examine the properties affecting phoneme decoding accuracy during sentence reconstruction and offer neurophysiological insights to further enhance EEG decoding for more effective neural communication solutions. Our findings underscore the feasibility of biosignal-based sentence-level speech synthesis for reconstructing unseen sentences, highlighting a significant step toward developing open-vocabulary neural communication systems adapted to diverse patient needs and conditions. Additionally, this study provides meaningful insights into the development of communication and rehabilitation solutions utilizing EEG-based decoding technologies.

**AI Summary:** This study explores the potential of using brain-to-speech systems to decode unseen sentences from speech-related biosignals, such as EEG and EMG signals. By leveraging phoneme-level information, the researchers were able to reconstruct sentences across various speech modes, highlighting the feasibility of biosignal-based speech synthesis for open-vocabulary neural communication. These findings have important implications for developing personalized and adaptive communication and rehabilitation solutions for patients, as well as enhancing EEG decoding for more effective neural communication systems.

---

## AURA: A Reinforcement Learning Framework for AI-Driven Adaptive Conversational Surveys
**URL:** https://arxiv.org/abs/2510.27126

**Abstract:** Conventional online surveys provide limited personalization, often resulting in low engagement and superficial responses. Although AI survey chatbots improve convenience, most are still reactive: they rely on fixed dialogue trees or static prompt templates and therefore cannot adapt within a session to fit individual users, which leads to generic follow-ups and weak response quality. We address these limitations with AURA (Adaptive Understanding through Reinforcement Learning for Assessment), a reinforcement learning framework for AI-driven adaptive conversational surveys. AURA quantifies response quality using a four-dimensional LSDE metric (Length, Self-disclosure, Emotion, and Specificity) and selects follow-up question types via an epsilon-greedy policy that updates the expected quality gain within each session. Initialized with priors extracted from 96 prior campus-climate conversations (467 total chatbot-user exchanges), the system balances exploration and exploitation across 10-15 dialogue exchanges, dynamically adapting to individual participants in real time. In controlled evaluations, AURA achieved a +0.12 mean gain in response quality and a statistically significant improvement over non-adaptive baselines (p=0.044, d=0.66), driven by a 63% reduction in specification prompts and a 10x increase in validation behavior. These results demonstrate that reinforcement learning can give survey chatbots improved adaptivity, transforming static questionnaires into interactive, self-improving assessment systems.

**AI Summary:** The research introduces AURA, a reinforcement learning framework for AI-driven adaptive conversational surveys that aims to improve response quality by adapting to individual users in real time. AURA uses a four-dimensional metric to quantify response quality and selects follow-up questions based on expected quality gain within each session. In evaluations, AURA showed a significant improvement in response quality compared to non-adaptive baselines, demonstrating the potential of reinforcement learning to enhance survey chatbots and make them more interactive and self-improving.

---

## Functional connectivity guided deep neural network for decoding high-level visual imagery
**URL:** https://arxiv.org/abs/2510.27075

**Abstract:** This study introduces a pioneering approach in brain-computer interface (BCI) technology, featuring our novel concept of high-level visual imagery for non-invasive electroencephalography (EEG)-based communication. High-level visual imagery, as proposed in our work, involves the user engaging in the mental visualization of complex upper limb movements. This innovative approach significantly enhances the BCI system, facilitating the extension of its applications to more sophisticated tasks such as EEG-based robotic arm control. By leveraging this advanced form of visual imagery, our study opens new horizons for intricate and intuitive mind-controlled interfaces. We developed an advanced deep learning architecture that integrates functional connectivity metrics with a convolutional neural network-image transformer. This framework is adept at decoding subtle user intentions, addressing the spatial variability in high-level visual tasks, and effectively translating these into precise commands for robotic arm control. Our comprehensive offline and pseudo-online evaluations demonstrate the framework's efficacy in real-time applications, including the nuanced control of robotic arms. The robustness of our approach is further validated through leave-one-subject-out cross-validation, marking a significant step towards versatile, subject-independent BCI applications. This research highlights the transformative impact of advanced visual imagery and deep learning in enhancing the usability and adaptability of BCI systems, particularly in robotic arm manipulation.

**AI Summary:** This study introduces a novel approach in brain-computer interface technology by utilizing high-level visual imagery for EEG-based communication, specifically focusing on mental visualization of complex upper limb movements. The integration of functional connectivity metrics with a convolutional neural network-image transformer in a deep learning architecture enables precise decoding of user intentions for robotic arm control, showcasing significant advancements in BCI applications. The research demonstrates the effectiveness of this approach through offline and pseudo-online evaluations, emphasizing its potential for versatile and subject-independent BCI applications, particularly in robotic arm manipulation.

---

## Adaptive Human-Computer Interaction Strategies Through Reinforcement Learning in Complex
**URL:** https://arxiv.org/abs/2510.27058

**Abstract:** This study addresses the challenges of dynamics and complexity in intelligent human-computer interaction and proposes a reinforcement learning-based optimization framework to improve long-term returns and overall experience. Human-computer interaction is modeled as a Markov decision process, with state space, action space, reward function, and discount factor defined to capture the dynamics of user input, system feedback, and interaction environment. The method combines policy function, value function, and advantage function, updates parameters through policy gradient, and continuously adjusts during interaction to balance immediate feedback and long-term benefits. To validate the framework, multimodal dialog and scene-aware datasets are used as the experimental platform, with multiple sensitivity experiments conducted on key factors such as discount factor, exploration rate decay, environmental noise, and data imbalance. Evaluation is carried out using cumulative reward, average episode reward, convergence speed, and task success rate. Results show that the proposed method outperforms existing approaches across several metrics, achieving higher task completion while maintaining strategy stability. Comparative experiments further confirm its advantages in interaction efficiency and long-term return, demonstrating the significant value of reinforcement learning in optimizing human-computer interaction.

**AI Summary:** This research explores the use of reinforcement learning to optimize human-computer interaction in complex environments. By modeling interaction as a Markov decision process and incorporating policy, value, and advantage functions, the framework is able to continuously adjust and balance immediate feedback with long-term benefits. Experimental results show that this approach outperforms existing methods in terms of task completion, strategy stability, efficiency, and long-term return, highlighting the significant value of reinforcement learning in improving human-computer interaction.

---

## AIOT based Smart Education System: A Dual Layer Authentication and Context-Aware Tutoring Framework for Learning Environments
**URL:** https://arxiv.org/abs/2510.26999

**Abstract:** The AIoT-Based Smart Education System integrates Artificial Intelligence and IoT to address persistent challenges in contemporary classrooms: attendance fraud, lack of personalization, student disengagement, and inefficient resource use. The unified platform combines four core modules: (1) a dual-factor authentication system leveraging RFID-based ID scans and WiFi verification for secure, fraud-resistant attendance; (2) an AI-powered assistant that provides real-time, context-aware support and dynamic quiz generation based on instructor-supplied materials; (3) automated test generators to streamline adaptive assessment and reduce administrative overhead; and (4) the EcoSmart Campus module, which autonomously regulates classroom lighting, air quality, and temperature using IoT sensors and actuators. Simulated evaluations demonstrate the system's effectiveness in delivering robust real-time monitoring, fostering inclusive engagement, preventing fraudulent practices, and supporting operational scalability. Collectively, the AIoT-Based Smart Education System offers a secure, adaptive, and efficient learning environment, providing a scalable blueprint for future educational innovation and improved student outcomes through the synergistic application of artificial intelligence and IoT technologies.

**AI Summary:** The AIoT-Based Smart Education System combines AI and IoT to address challenges in classrooms, such as attendance fraud and lack of personalization. The system includes dual-layer authentication, context-aware tutoring, automated test generation, and EcoSmart Campus features to enhance learning environments. Simulated evaluations demonstrate the system's effectiveness in improving monitoring, engagement, fraud prevention, and operational scalability, offering a blueprint for future educational innovation through the integration of AI and IoT technologies.

---

## Delegate Pricing Decisions to an Algorithm? Experimental Evidence
**URL:** https://arxiv.org/abs/2510.27636

**Abstract:** We analyze the delegation of pricing by participants, representing firms, to a collusive, self-learning algorithm in a repeated Bertrand experiment. In the baseline treatment, participants set prices themselves. In the other treatments, participants can either delegate pricing to the algorithm at the beginning of each supergame or receive algorithmic recommendations that they can override. Participants delegate more when they can override the algorithm's decisions. In both algorithmic treatments, prices are lower than in the baseline. Our results indicate that while self-learning pricing algorithms can be collusive, they can foster competition rather than collusion with humans-in-the-loop.

**AI Summary:** This study examines the delegation of pricing decisions to a self-learning algorithm in a repeated Bertrand experiment. Participants are more likely to delegate pricing decisions to the algorithm when they can override its recommendations. The results show that algorithmic pricing leads to lower prices compared to when participants set prices themselves, suggesting that self-learning algorithms can promote competition rather than collusion when humans are involved in the decision-making process.

---

## CodeAlignBench: Assessing Code Generation Models on Developer-Preferred Code Adjustments
**URL:** https://arxiv.org/abs/2510.27565

**Abstract:** As large language models become increasingly capable of generating code, evaluating their performance remains a complex and evolving challenge. Existing benchmarks primarily focus on functional correctness, overlooking the diversity of real-world coding tasks and developer expectations. To this end, we introduce a multi-language benchmark that evaluates LLM instruction-following capabilities and is extensible to operate on any set of standalone coding problems. Our benchmark evaluates instruction following in two key settings: adherence to pre-defined constraints specified with the initial problem, and the ability to perform refinements based on follow-up instructions. For this paper's analysis, we empirically evaluated our benchmarking pipeline with programming tasks from LiveBench, that are also automatically translated from Python into Java and JavaScript. Our automated benchmark reveals that models exhibit differing levels of performance across multiple dimensions of instruction-following. Our benchmarking pipeline provides a more comprehensive evaluation of code generation models, highlighting their strengths and limitations across languages and generation goals.

**AI Summary:** The research introduces a new benchmark, CodeAlignBench, to evaluate the performance of large language models in generating code based on developer-preferred adjustments. The benchmark assesses the models' ability to follow instructions and make refinements in coding tasks across multiple languages. The study found that models exhibit varying levels of performance in instruction-following, providing a comprehensive evaluation of their strengths and limitations in code generation.

---

## Beyond Demographics: Behavioural Segmentation and Spatial Analytics to Enhance Visitor Experience at The British Museum
**URL:** https://arxiv.org/abs/2510.27542

**Abstract:** This study explores visitor behaviour at The British Museum using data science methods applied to novel sources, including audio guide usage logs and TripAdvisor reviews. Analysing 42,000 visitor journeys and over 50,000 reviews, we identify key drivers of satisfaction, segment visitors by behavioural patterns, examine tour engagement, model spatial navigation, and investigate room popularity. Behavioural clustering uncovered four distinct visitor types: Committed Trekkers, Leisurely Explorers, Targeted Visitors, and Speedy Samplers, each characterised by different levels of engagement and movement. Tour usage analysis revealed high drop-off rates and variation in completion rates across different language groups. Spatial flow modelling revealed that accessibility and proximity, particularly aversion to stairs, shaped visitor paths more than thematic organisation. Room popularity was more strongly predicted by physical accessibility than curatorial content. We propose practical strategies for improving engagement and flow, offering a scalable framework for visitor-centred, data-informed museum planning.

**AI Summary:** This study uses data science methods to analyze visitor behavior at The British Museum, focusing on audio guide usage logs and TripAdvisor reviews. The research identifies four distinct visitor types and highlights factors that influence visitor satisfaction, tour engagement, spatial navigation, and room popularity. The findings suggest practical strategies for enhancing visitor experience and offer a scalable framework for data-informed museum planning.

---

## Preliminary Prototyping of Avoidance Behaviors Triggered by a User's Physical Approach to a Robot
**URL:** https://arxiv.org/abs/2510.27436

**Abstract:** Human-robot interaction frequently involves physical proximity or contact. In human-human settings, people flexibly accept, reject, or tolerate such approaches depending on the relationship and context. We explore the design of a robot's rejective internal state and corresponding avoidance behaviors, such as withdrawing or pushing away, when a person approaches. We model the accumulation and decay of discomfort as a function of interpersonal distance, and implement tolerance (endurance) and limit-exceeding avoidance driven by the Dominance axis of the PAD affect model. The behaviors and their intensities are realized on an arm robot. Results illustrate a coherent pipeline from internal state parameters to graded endurance motions and, once a limit is crossed, to avoidance actions.

**AI Summary:** This research focuses on designing a robot's rejective internal state and avoidance behaviors when a person approaches it. The study explores how discomfort accumulates as a person gets closer to the robot and implements tolerance and avoidance behaviors based on the Dominance axis of the PAD affect model. The results demonstrate a coherent process from internal state parameters to graded endurance motions and eventual avoidance actions, highlighting the importance of understanding and implementing appropriate behaviors in human-robot interactions.

---

## Sustaining Cyber Awareness: The Long-Term Impact of Continuous Phishing Training and Emotional Triggers
**URL:** https://arxiv.org/abs/2510.27298

**Abstract:** Phishing constitutes more than 90\% of successful cyberattacks globally, remaining one of the most persistent threats to organizational security. Despite organizations tripling their cybersecurity budgets between 2015 and 2025, the human factor continues to pose a critical vulnerability. This study presents a 12-month longitudinal investigation examining how continuous cybersecurity training and emotional cues affect employee susceptibility to phishing. The experiment involved 20 organizations and over 1,300 employees who collectively received more than 13,000 simulated phishing emails engineered with diverse emotional, contextual, and structural characteristics. Behavioral responses were analyzed using non-parametric correlation and regression models to assess the influence of psychological manipulation, message personalization, and perceived email source. Results demonstrate that sustained phishing simulations and targeted training programs lead to a significant reduction in employee susceptibility, halving successful compromise rates within six months. Additionally, employee turnover introduces measurable fluctuations in awareness levels, underscoring the necessity of maintaining continuous training initiatives. These findings provide one of the few long-term perspectives on phishing awareness efficacy, highlighting the strategic importance of ongoing behavioral interventions in strengthening organizational cyber resilience. In order to support open science, we published our email templates, source code, and other materials at this https URL

**AI Summary:** This study investigates the impact of continuous cybersecurity training and emotional triggers on employee susceptibility to phishing attacks. The research, conducted over 12 months with 20 organizations and 1,300 employees, found that sustained phishing simulations and targeted training programs led to a significant reduction in successful compromise rates within six months. The results underscore the importance of ongoing behavioral interventions in strengthening organizational cyber resilience against phishing threats.

---

## ECVL-ROUTER: Scenario-Aware Routing for Vision-Language Models
**URL:** https://arxiv.org/abs/2510.27256

**Abstract:** Vision-Language Models (VLMs) excel in diverse multimodal tasks. However, user requirements vary across scenarios, which can be categorized into fast response, high-quality output, and low energy consumption. Relying solely on large models deployed in the cloud for all queries often leads to high latency and energy cost, while small models deployed on edge devices are capable of handling simpler tasks with low latency and energy cost. To fully leverage the strengths of both large and small models, we propose ECVL-ROUTER, the first scenario-aware routing framework for VLMs. Our approach introduces a new routing strategy and evaluation metrics that dynamically select the appropriate model for each query based on user requirements, maximizing overall utility. We also construct a multimodal response-quality dataset tailored for router training and validate the approach through extensive experiments. Results show that our approach successfully routes over 80\% of queries to the small model while incurring less than 10\% drop in problem solving probability.

**AI Summary:** The research introduces ECVL-ROUTER, a scenario-aware routing framework for Vision-Language Models (VLMs) that dynamically selects the appropriate model (large or small) based on user requirements like response time, output quality, and energy consumption. By leveraging both large cloud models and small edge device models, the approach maximizes overall utility and successfully routes over 80% of queries to the small model with minimal drop in problem-solving probability. This framework addresses the challenge of balancing performance and efficiency in diverse multimodal tasks, making it a significant contribution to the field of AI research.

---

## MARIA: A Framework for Marginal Risk Assessment without Ground Truth in AI Systems
**URL:** https://arxiv.org/abs/2510.27163

**Abstract:** Before deploying an AI system to replace an existing process, it must be compared with the incumbent to ensure improvement without added risk. Traditional evaluation relies on ground truth for both systems, but this is often unavailable due to delayed or unknowable outcomes, high costs, or incomplete data, especially for long-standing systems deemed safe by convention. The more practical solution is not to compute absolute risk but the difference between systems. We therefore propose a marginal risk assessment framework, that avoids dependence on ground truth or absolute risk. It emphasizes three kinds of relative evaluation methodology, including predictability, capability and interaction dominance. By shifting focus from absolute to relative evaluation, our approach equips software teams with actionable guidance: identifying where AI enhances outcomes, where it introduces new risks, and how to adopt such systems responsibly.

**AI Summary:** The research proposes a framework called MARIA for assessing the marginal risk of AI systems without relying on ground truth data. By focusing on the difference between AI systems and existing processes rather than absolute risk, the framework provides actionable guidance for software teams on how AI enhances outcomes and introduces new risks. This approach allows for a more practical and responsible adoption of AI systems in place of traditional evaluation methods.

---

## VitalLens 2.0: High-Fidelity rPPG for Heart Rate Variability Estimation from Face Video
**URL:** https://arxiv.org/abs/2510.27028

**Abstract:** This report introduces VitalLens 2.0, a new deep learning model for estimating physiological signals from face video. This new model demonstrates a significant leap in accuracy for remote photoplethysmography (rPPG), enabling the robust estimation of not only heart rate (HR) and respiratory rate (RR) but also Heart Rate Variability (HRV) metrics. This advance is achieved through a combination of a new model architecture and a substantial increase in the size and diversity of our training data, now totaling 1,413 unique individuals. We evaluate VitalLens 2.0 on a new, combined test set of 422 unique individuals from four public and private datasets. When averaging results by individual, VitalLens 2.0 achieves a Mean Absolute Error (MAE) of 1.57 bpm for HR, 1.08 bpm for RR, 10.18 ms for HRV-SDNN, and 16.45 ms for HRV-RMSSD. These results represent a new state-of-the-art, significantly outperforming previous methods. This model is now available to developers via the VitalLens API at this https URL.

**AI Summary:** The research introduces VitalLens 2.0, a deep learning model that accurately estimates physiological signals such as heart rate variability from face videos. The model shows a significant improvement in accuracy compared to previous methods, achieving a Mean Absolute Error of 1.57 bpm for heart rate and outperforming previous models. This advancement has implications for remote monitoring of health metrics and is now available for developers to use through the VitalLens API.

---

## Using Salient Object Detection to Identify Manipulative Cookie Banners that Circumvent GDPR
**URL:** https://arxiv.org/abs/2510.26967

**Abstract:** The main goal of this paper is to study how often cookie banners that comply with the General Data Protection Regulation (GDPR) contain aesthetic manipulation, a design tactic to draw users' attention to the button that permits personal data sharing. As a byproduct of this goal, we also evaluate how frequently the banners comply with GDPR and the recommendations of national data protection authorities regarding banner designs. We visited 2,579 websites and identified the type of cookie banner implemented. Although 45% of the relevant websites have fully compliant banners, we found aesthetic manipulation on 38% of the compliant banners. Unlike prior studies of aesthetic manipulation, we use a computer vision model for salient object detection to measure how salient (i.e., attention-drawing) each banner element is. This enables the discovery of new types of aesthetic manipulation (e.g., button placement), and leads us to conclude that aesthetic manipulation is more common than previously reported (38% vs 27% of banners). To study the effects of user and/or website location on cookie banner design, we include websites within the European Union (EU), where privacy regulation enforcement is more stringent, and websites outside the EU. We visited websites from IP addresses in the EU and from IP addresses in the United States (US). We find that 13.9% of EU websites change their banner design when the user is from the US, and EU websites are roughly 48.3% more likely to use aesthetic manipulation than non-EU websites, highlighting their innovative responses to privacy regulation.

**AI Summary:** This research paper examines the prevalence of aesthetic manipulation in cookie banners that comply with GDPR regulations, with 38% of compliant banners found to use this tactic. Using a computer vision model for salient object detection, the study identifies new forms of aesthetic manipulation and concludes that it is more common than previously reported. Additionally, the research highlights how EU websites are more likely to use aesthetic manipulation in their banners and alter their designs based on user location, showing innovative responses to privacy regulation enforcement.

---

## Metacognition and Confidence Dynamics in Advice Taking from Generative AI
**URL:** https://arxiv.org/abs/2510.26508

**Abstract:** Generative Artificial Intelligence (GenAI) can aid humans in a wide range of tasks, but its effectiveness critically depends on users being able to evaluate the accuracy of GenAI outputs and their own expertise. Here we asked how confidence in self and GenAI contributes to decisions to seek and rely on advice from GenAI ('prospective confidence'), and how advice-taking in turn shapes this confidence ('retrospective confidence'). In a novel paradigm involving text generation, participants formulated plans for events, and could request advice from a GenAI (Study 1; N=200) or were randomly assigned to receive advice (Study 2; N=300), which they could rely on or ignore. Advice requests in Study 1 were related to higher prospective confidence in GenAI and lower confidence in self. Advice-seekers showed increased retrospective confidence in GenAI, while those who declined advice showed increased confidence in self. Random assignment in Study 2 revealed that advice exposure increases confidence in GenAI and in self, suggesting that GenAI advice-taking causally boosts retrospective confidence. These results were mirrored in advice reliance, operationalised as the textual similarity between GenAI advice and participants' responses, with reliance associated with increased retrospective confidence in both GenAI and self. Critically, participants who chose to obtain/rely on advice provided more detailed responses (likely due to the output's verbosity), but failed to check the output thoroughly, missing key information. These findings underscore a key role for confidence in interactions with GenAI, shaped by both prior beliefs about oneself and the reliability of AI, and context-dependent exposure to advice.

**AI Summary:** This research explores how confidence in oneself and in generative AI influences decision-making in seeking and relying on advice from AI. The study found that seeking advice from AI increased confidence in the AI itself, while ignoring advice led to increased confidence in oneself. Additionally, participants who chose to rely on AI advice provided more detailed responses but were more likely to miss key information, highlighting the importance of balancing confidence and critical evaluation in interactions with AI.

---

## Scaffolding Creativity: How Divergent and Convergent LLM Personas Shape Human Machine Creative Problem-Solving
**URL:** https://arxiv.org/abs/2510.26490

**Abstract:** Large language models (LLMs) are increasingly shaping creative work and problem-solving; however, prior research suggests that they may diminish unassisted creativity. To address this tension, a coach-like LLM environment was developed that embodies divergent and convergent thinking personas as two complementary processes. Effectiveness and user behavior were assessed through a controlled experiment in which participants interacted with either persona, while a control group engaged with a standard LLM providing direct answers.
Notably, users' perceptions of which persona best supported their creativity often diverged from objective performance measures. Trait-based analyses revealed that individual differences predict when people utilize divergent versus convergent personas, suggesting opportunities for adaptive sequencing. Furthermore, interaction patterns reflected the design thinking model, demonstrating how persona-guided support shapes creative problem-solving.
Our findings provide design principles for creativity support systems that strike a balance between exploration and convergence through persona-based guidance and personalization. These insights advance human-AI collaboration tools that scaffold rather than overshadow human creativity.

**AI Summary:** This research explores how different personas in large language models (LLMs) can impact human creativity and problem-solving. The study found that users' perceptions of which persona best supported their creativity did not always align with objective performance measures. The findings suggest that incorporating divergent and convergent thinking personas in LLMs can enhance creativity support systems and improve human-AI collaboration tools by providing personalized guidance.

---

## Look at That Distractor: Dynamic Translation Gain under Low Perceptual Load in Virtual Reality
**URL:** https://arxiv.org/abs/2510.26265

**Abstract:** Redirected walking utilizes gain adjustments within perceptual thresholds to allow natural navigation in large scale virtual environments within confined physical environments. Previous research has found that when users are distracted by some scene elements, they are less sensitive to gain values. However, the effects on detection thresholds have not been quantitatively measured. In this paper, we present a novel method that dynamically adjusts translation gain by leveraging visual distractors. We place distractors within the user's field of view and apply a larger translation gain when their attention is drawn to them. Because the magnitude of gain adjustment depends on the user's level of engagement with the distractors, the redirection process remains smooth and unobtrusive. To evaluate our method, we developed a task oriented virtual environment for a user study. Results show that introducing distractors in the virtual environment significantly raises users' translation gain thresholds. Furthermore, assessments using the Simulator Sickness Questionnaire and Igroup Presence Questionnaire indicate that the method maintains user comfort and acceptance, supporting its effectiveness for RDW systems.

**AI Summary:** This research explores the use of visual distractors to dynamically adjust translation gain in virtual reality environments, allowing for smoother navigation in large-scale spaces. The study found that introducing distractors increased users' translation gain thresholds, indicating a potential improvement in navigation efficiency. Additionally, assessments showed that the method maintained user comfort and acceptance, supporting its effectiveness for redirected walking systems.

---

## Avatar Appearance Beyond Pixels -- User Ratings and Avatar Preferences within Health Applications
**URL:** https://arxiv.org/abs/2510.26251

**Abstract:** The appearance of a virtual avatar significantly influences its perceived appropriateness and the user's experience, particularly in healthcare applications. This study analyzed interactions with six avatars of varying characteristics in a patient-reported outcome measures (PROMs) application to investigate correlations between avatar ratings and user preferences. Forty-seven participants completed a healthcare survey involving 30 PROMIS items (Global Health and Physical Function) and then rated the avatars on warmth, competence, attractiveness, and human-likeness, as well as their willingness to share personal data. The results showed that competence was the most critical factor in avatar selection, while human-likeness had minimal impact on health data disclosure. Gender did not significantly affect the ratings, but clothing style played a key role, with male avatars in professional attire rated higher in competence due to gender-stereotypical expectations. In contrast, professional female avatars were rated lower in warmth and attractiveness. These findings underline the importance of thoughtful avatar design in healthcare applications to enhance user experience and engagement.

**AI Summary:** This study explored the impact of avatar appearance on user preferences within healthcare applications, specifically focusing on competence, warmth, attractiveness, and human-likeness. The results revealed that competence was the most crucial factor in avatar selection, with clothing style playing a significant role in user ratings. Gender did not have a significant impact on ratings, but gender-stereotypical expectations influenced perceptions of competence. These findings emphasize the importance of thoughtful avatar design in healthcare applications to improve user engagement and experience.

---

## Linking Heterogeneous Data with Coordinated Agent Flows for Social Media Analysis
**URL:** https://arxiv.org/abs/2510.26172

**Abstract:** Social media platforms generate massive volumes of heterogeneous data, capturing user behaviors, textual content, temporal dynamics, and network structures. Analyzing such data is crucial for understanding phenomena such as opinion dynamics, community formation, and information diffusion. However, discovering insights from this complex landscape is exploratory, conceptually challenging, and requires expertise in social media mining and visualization. Existing automated approaches, though increasingly leveraging large language models (LLMs), remain largely confined to structured tabular data and cannot adequately address the heterogeneity of social media analysis. We present SIA (Social Insight Agents), an LLM agent system that links heterogeneous multi-modal data -- including raw inputs (e.g., text, network, and behavioral data), intermediate outputs, mined analytical results, and visualization artifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy that connects insight types with suitable mining and visualization techniques, SIA enables agents to plan and execute coherent analysis strategies. To ensure multi-modal integration, it incorporates a data coordinator that unifies tabular, textual, and network data into a consistent flow. Its interactive interface provides a transparent workflow where users can trace, validate, and refine the agent's reasoning, supporting both adaptability and trustworthiness. Through expert-centered case studies and quantitative evaluation, we show that SIA effectively discovers diverse and meaningful insights from social media while supporting human-agent collaboration in complex analytical tasks.

**AI Summary:** The research introduces SIA (Social Insight Agents), an LLM agent system that can analyze heterogeneous data from social media platforms by linking various types of data through coordinated agent flows. This system allows for the discovery of meaningful insights such as opinion dynamics and community formation, supporting human-agent collaboration in complex analytical tasks. Through expert-centered case studies and quantitative evaluation, the study demonstrates that SIA is effective in uncovering diverse insights from social media data.

---

## Interaction-Augmented Instruction: Modeling the Synergy of Prompts and Interactions in Human-GenAI Collaboration
**URL:** https://arxiv.org/abs/2510.26069

**Abstract:** Text prompt is the most common way for human-generative AI (GenAI) communication. Though convenient, it is challenging to convey fine-grained and referential intent. One promising solution is to combine text prompts with precise GUI interactions, like brushing and clicking. However, there lacks a formal model to model synergistic designs between prompts and interactions, hindering their comparison and innovation. To fill this gap, via an iterative and deductive process, we develop the Interaction-Augmented Instruction (IAI) model, a compact entity-relation graph formalizing how the combination of interactions and text prompts enhances human-generative AI communication. With the model, we distill twelve recurring and composable atomic interaction paradigms from prior tools, verifying our model's capability to facilitate systematic design characterization and comparison. Case studies further demonstrate the model's utility in applying, refining, and extending these paradigms. These results illustrate our IAI model's descriptive, discriminative, and generative power for shaping future GenAI systems.

**AI Summary:** The research explores the combination of text prompts and GUI interactions in human-generative AI communication. The Interaction-Augmented Instruction (IAI) model is developed to formalize how these elements enhance communication, allowing for systematic design characterization and comparison. The model is shown to be useful in distilling interaction paradigms and shaping future GenAI systems.

---

## FractalBrain: A Neuro-interactive Virtual Reality Experience using Electroencephalogram (EEG) for Mindfulness
**URL:** https://arxiv.org/abs/2510.26041

**Abstract:** Mindfulness has been studied and practiced in enhancing psychological well-being while reducing neuroticism and psychopathological indicators. However, practicing mindfulness with continuous attention is challenging, especially for beginners. In the proposed system, FractalBrain, we utilize an interactive audiovisual fractal with a geometric repetitive pattern that has been demonstrated to induce meditative effects. FractalBrain presents an experience combining a surreal virtual reality (VR) program with an electroencephalogram (EEG) interface. While viewing an ever-changing fractal-inspired artwork in an immersive environment, the user's EEG stream is analyzed and mapped into VR. These EEG data adaptively manipulates the audiovisual parameters in real-time, generating a distinct experience for each user. The pilot feedback suggests the potential of the FractalBrain to facilitate mindfulness and enhance attention.

**AI Summary:** The FractalBrain system combines virtual reality with an EEG interface to create a personalized mindfulness experience. By analyzing EEG data in real-time and adapting audiovisual parameters, the system aims to induce meditative effects and enhance attention. Pilot feedback indicates the potential of FractalBrain to help users practice mindfulness and improve psychological well-being.

---

## Designing for Dignity while Driving: Interaction Needs of Blind and Low-Vision Passengers in Fully Automated Vehicles
**URL:** https://arxiv.org/abs/2510.26015

**Abstract:** Fully automated vehicles (FAVs) hold promise for enhancing the mobility of blind and low-vision (BLV) individuals. To understand the situated interaction needs of BLV passengers, we conducted six on-road, and in-lab focus groups with 16 participants, immersing them in real-world driving conditions. Our thematic analysis reveals that BLV participants express a high initial 'faith' in FAVs, but require layered, value-sensitive information during the ride to cultivate trust. The participants' modality preference for voice suggests re-evaluating the role of haptics for BLV users in FAVs. Our findings show the importance of a respectful interaction design in FAVs that both address BLV users' mobility challenges and uphold their dignity. While others have advocated for a dignity lens, our contribution lies in grounding this framework in empirical findings and unpacking what it means to design for dignity in the context of FAVs.

**AI Summary:** This research explores the interaction needs of blind and low-vision passengers in fully automated vehicles (FAVs). The study found that BLV participants initially trust FAVs but require layered, value-sensitive information during the ride to build trust. The preference for voice over haptics suggests the importance of respectful interaction design in FAVs to address mobility challenges and uphold the dignity of BLV users.

---

## On the Go with AR: Attention to Virtual and Physical Targets while Varying Augmentation Density
**URL:** https://arxiv.org/abs/2510.25978

**Abstract:** Augmented reality is projected to be a primary mode of information consumption on the go, seamlessly integrating virtual content into the physical world. However, the potential perceptual demands of viewing virtual annotations while navigating a physical environment could impact user efficacy and safety, and the implications of these demands are not well understood. Here, we investigate the impact of virtual path guidance and augmentation density (visual clutter) on search performance and memory. Participants walked along a predefined path, searching for physical or virtual items. They experienced two levels of augmentation density, and either walked freely or with enforced speed and path guidance. Augmentation density impacted behavior and reduced awareness of uncommon objects in the environment. Analysis of search task performance and post-experiment item recall revealed differing attention to physical and virtual objects. On the basis of these findings we outline considerations for AR apps designed for use on the go.

**AI Summary:** This research examines how augmented reality (AR) affects user behavior and awareness while navigating a physical environment. The study found that varying levels of augmentation density can impact search performance and memory, with higher density leading to reduced awareness of uncommon objects. The findings suggest that attention to physical and virtual objects differs, highlighting the importance of considering these factors in the design of AR apps for on-the-go use.

---

## Risks and Opportunities in Human-Machine Teaming in Operationalizing Machine Learning Target Variables
**URL:** https://arxiv.org/abs/2510.25974

**Abstract:** Predictive modeling has the potential to enhance human decision-making. However, many predictive models fail in practice due to problematic problem formulation in cases where the prediction target is an abstract concept or construct and practitioners need to define an appropriate target variable as a proxy to operationalize the construct of interest. The choice of an appropriate proxy target variable is rarely self-evident in practice, requiring both domain knowledge and iterative data modeling. This process is inherently collaborative, involving both domain experts and data scientists. In this work, we explore how human-machine teaming can support this process by accelerating iterations while preserving human judgment. We study the impact of two human-machine teaming strategies on proxy construction: 1) relevance-first: humans leading the process by selecting relevant proxies, and 2) performance-first: machines leading the process by recommending proxies based on predictive performance. Based on a controlled user study of a proxy construction task (N = 20), we show that the performance-first strategy facilitated faster iterations and decision-making, but also biased users towards well-performing proxies that are misaligned with the application goal. Our study highlights the opportunities and risks of human-machine teaming in operationalizing machine learning target variables, yielding insights for future research to explore the opportunities and mitigate the risks.

**AI Summary:** This research explores the use of human-machine teaming in operationalizing machine learning target variables for predictive modeling. The study compares two strategies - relevance-first and performance-first - for constructing proxy target variables and finds that the performance-first strategy accelerates iterations and decision-making but may lead to biases towards well-performing proxies that are not aligned with the application goal. The findings highlight the importance of collaboration between domain experts and data scientists in defining appropriate target variables for predictive models.

---

## The Impact of Navigation Aids on Search Performance and Object Recall in Wide-Area Augmented Reality
**URL:** https://arxiv.org/abs/2510.25957

**Abstract:** Head-worn augmented reality (AR) is a hotly pursued and increasingly feasible contender paradigm for replacing or complementing smartphones and watches for continual information consumption. Here, we compare three different AR navigation aids (on-screen compass, on-screen radar and in-world vertical arrows) in a wide-area outdoor user study (n=24) where participants search for hidden virtual target items amongst physical and virtual objects. We analyzed participants' search task performance, movements, eye-gaze, survey responses and object recall. There were two key findings. First, all navigational aids enhanced search performance relative to a control condition, with some benefit and strongest user preference for in-world arrows. Second, users recalled fewer physical objects than virtual objects in the environment, suggesting reduced awareness of the physical environment. Together, these findings suggest that while navigational aids presented in AR can enhance search task performance, users may pay less attention to the physical environment, which could have undesirable side-effects.

**AI Summary:** This study compared three different AR navigation aids in a wide-area outdoor user study and found that all aids improved search performance compared to a control condition, with in-world arrows being the most preferred. However, users recalled fewer physical objects in the environment, indicating reduced awareness of their surroundings. This suggests that while AR navigation aids can enhance search task performance, they may also lead to decreased attention to the physical environment, which could have negative consequences.

---

## Human-AI Complementarity: A Goal for Amplified Oversight
**URL:** https://arxiv.org/abs/2510.26518

**Abstract:** Human feedback is critical for aligning AI systems to human values. As AI capabilities improve and AI is used to tackle more challenging tasks, verifying quality and safety becomes increasingly challenging. This paper explores how we can leverage AI to improve the quality of human oversight. We focus on an important safety problem that is already challenging for humans: fact-verification of AI outputs. We find that combining AI ratings and human ratings based on AI rater confidence is better than relying on either alone. Giving humans an AI fact-verification assistant further improves their accuracy, but the type of assistance matters. Displaying AI explanation, confidence, and labels leads to over-reliance, but just showing search results and evidence fosters more appropriate trust. These results have implications for Amplified Oversight -- the challenge of combining humans and AI to supervise AI systems even as they surpass human expert performance.

**AI Summary:** This research explores the importance of human oversight in aligning AI systems with human values, particularly in the context of fact-verification of AI outputs. The study finds that combining AI ratings and human ratings based on AI rater confidence is more effective than relying on either alone. The results suggest that giving humans an AI fact-verification assistant can improve accuracy, but the type of assistance provided is crucial in fostering appropriate trust and avoiding over-reliance on AI explanations and labels. These findings have implications for Amplified Oversight, highlighting the need to combine human and AI supervision as AI systems become more advanced.

---

## Structurally Valid Log Generation using FSM-GFlowNets
**URL:** https://arxiv.org/abs/2510.26197

**Abstract:** Generating structurally valid and behaviorally diverse synthetic event logs for interaction-aware models is a challenging yet crucial problem, particularly in settings with limited or privacy constrained user data. Existing methods such as heuristic simulations and LLM based generators often lack structural coherence or controllability, producing synthetic data that fails to accurately represent real world system interactions. This paper presents a framework that integrates Finite State Machines or FSMs with Generative Flow Networks or GFlowNets to generate structured, semantically valid, and diverse synthetic event logs. Our FSM-constrained GFlowNet ensures syntactic validity and behavioral variation through dynamic action masking and guided sampling. The FSM, derived from expert traces, encodes domain-specific rules, while the GFlowNet is trained using a flow matching objective with a hybrid reward balancing FSM compliance and statistical fidelity. We instantiate the framework in the context of UI interaction logs using the UIC HCI dataset, but the approach generalizes to any symbolic sequence domain. Experimental results based on distributional metrics show that our FSM GFlowNet produces realistic, structurally consistent logs, achieving, for instance, under the real user logs baseline, a KL divergence of 0.2769 and Chi squared distance of 0.3522, significantly outperforming GPT-4o's 2.5294/13.8020 and Gemini's 3.7233/63.0355, alongside a leading bigram overlap of 0.1214 vs. GPT 4o's 0.0028 and Gemini's 0.0007. A downstream use case intent classification demonstrates that classifiers trained solely on our synthetic logs produced from FSM-GFlowNet achieve competitive accuracy compared to real data.

**AI Summary:** This research paper introduces a novel framework that combines Finite State Machines (FSMs) with Generative Flow Networks (GFlowNets) to generate structured, semantically valid, and diverse synthetic event logs. The framework ensures syntactic validity and behavioral variation through dynamic action masking and guided sampling, producing realistic and structurally consistent logs. Experimental results demonstrate that the FSM-GFlowNet approach outperforms existing methods like GPT-4o and Gemini in terms of KL divergence, Chi squared distance, and bigram overlap, and classifiers trained on synthetic logs from FSM-GFlowNet achieve competitive accuracy compared to real data in a downstream use case of intent classification.

---

## Can AI be Accountable?
**URL:** https://arxiv.org/abs/2510.26057

**Abstract:** The AI we use is powerful, and its power is increasing rapidly. If this powerful AI is to serve the needs of consumers, voters, and decision makers, then it is imperative that the AI is accountable. In general, an agent is accountable to a forum if the forum can request information from the agent about its actions, if the forum and the agent can discuss this information, and if the forum can sanction the agent. Unfortunately, in too many cases today's AI is not accountable -- we cannot question it, enter into a discussion with it, let alone sanction it. In this chapter we relate the general definition of accountability to AI, we illustrate what it means for AI to be accountable and unaccountable, and we explore approaches that can improve our chances of living in a world where all AI is accountable to those who are affected by it.

**AI Summary:** This research explores the importance of accountability in AI systems in order to serve the needs of consumers, voters, and decision makers. The lack of accountability in AI can lead to negative consequences as users are unable to question, discuss, or sanction the actions of AI. The study highlights the need for AI to be accountable to those affected by its decisions and suggests approaches to improve accountability in AI systems.

---

## Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning
**URL:** https://arxiv.org/abs/2510.25933

**Abstract:** We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS Grounding public subset within a $\pm 5$ pp equivalence margin.
Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI 69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's $d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp). When purchased as managed APIs, Humans-Junior's base model (Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on Microsoft AI Foundry pricing; self-hosted or edge deployments can drive incremental inference cost toward zero. Measured vs estimated pricing sources are tabulated in Appendix E.
Method. Our approach combines minimal directed "Exoskeleton Reasoning" scaffolds with behavioral fine-tuning that teaches protocol compliance (epistemic discipline) rather than domain answers. Fine-tuning alone adds little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance ($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100; non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within $\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost versus GPT-4o, and self-hosted/edge deployments can approach zero marginal cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains (Q1--Q100; non-comparable) and optimized-prompt exploratory results under earlier judges are summarized in Appendix F.
Keywords: Small Language Models, Factual Grounding, Directed Reasoning, Fine-Tuning, Model Alignment, Cost-Efficient AI

**AI Summary:** The research introduces Humans-Junior, a 3.8B language model that achieves factual accuracy equivalent to GPT-4o within a $\pm 5$ pp margin on the FACTS Grounding public subset. The study shows that Humans-Junior is significantly more cost-efficient than GPT-4o when purchased as managed APIs, with potential for zero marginal cost in self-hosted or edge deployments. The combination of directed "Exoskeleton Reasoning" scaffolds and behavioral fine-tuning improves model performance and reduces variance, demonstrating the potential for more efficient and accurate AI models.

---

## Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters
**URL:** https://arxiv.org/abs/2510.25860

**Abstract:** Large language models (LLMs) are increasingly used as raters for evaluation tasks. However, their reliability is often limited for subjective tasks, when human judgments involve subtle reasoning beyond annotation labels. Thinking traces, the reasoning behind a judgment, are highly informative but challenging to collect and curate. We present a human-LLM collaborative framework to infer thinking traces from label-only annotations. The proposed framework uses a simple and effective rejection sampling method to reconstruct these traces at scale. These inferred thinking traces are applied to two complementary tasks: (1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation guidelines for proprietary LLM raters. Across multiple datasets, our methods lead to significantly improved LLM-human agreement. Additionally, the refined annotation guidelines increase agreement among different LLM models. These results suggest that LLMs can serve as practical proxies for otherwise unrevealed human thinking traces, enabling label-only corpora to be extended into thinking-trace-augmented resources that enhance the reliability of LLM raters.

**AI Summary:** This research proposes a framework to infer thinking traces from label-only annotations to improve the reliability of large language models (LLMs) used as raters for subjective tasks. The inferred thinking traces are found to significantly enhance agreement between LLMs and human raters, as well as among different LLM models. This suggests that LLMs can effectively serve as proxies for human thinking traces, leading to more reliable evaluation tasks and annotation guidelines.

---

## Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for Generative NPC Dialogue
**URL:** https://arxiv.org/abs/2510.25820

**Abstract:** Large Language Models (LLMs) promise to transform interactive games by enabling non-player characters (NPCs) to sustain unscripted dialogue. Yet it remains unclear whether constrained prompts actually improve player experience. We investigate this question through The Interview, a voice-based detective game powered by GPT-4o. A within-subjects usability study ($N=10$) compared high-constraint (HCP) and low-constraint (LCP) prompts, revealing no reliable experiential differences beyond sensitivity to technical breakdowns. Guided by these findings, we redesigned the HCP into a hybrid JSON+RAG scaffold and conducted a synthetic evaluation with an LLM judge, positioned as an early-stage complement to usability testing. Results uncovered a novel pattern: scaffolding effects were role-dependent: the Interviewer (quest-giver NPC) gained stability, while suspect NPCs lost improvisational believability. These findings overturn the assumption that tighter constraints inherently enhance play. Extending fuzzy-symbolic scaffolding, we introduce \textit{Symbolically Scaffolded Play}, a framework in which symbolic structures are expressed as fuzzy, numerical boundaries that stabilize coherence where needed while preserving improvisation where surprise sustains engagement.

**AI Summary:** This research explores the impact of constrained prompts on player experience in interactive games using Large Language Models (LLMs). The study found that high-constraint prompts did not significantly improve player experience compared to low-constraint prompts, with sensitivity to technical issues being the main difference. The findings suggest that tight constraints do not necessarily enhance gameplay, and introduce the concept of Symbolically Scaffolded Play as a framework for balancing structure and improvisation in NPC dialogue.

---

## User Misconceptions of LLM-Based Conversational Programming Assistants
**URL:** https://arxiv.org/abs/2510.25662

**Abstract:** Programming assistants powered by large language models (LLMs) have become widely available, with conversational assistants like ChatGPT proving particularly accessible to less experienced programmers. However, the varied capabilities of these tools across model versions and the mixed availability of extensions that enable web search, code execution, or retrieval-augmented generation create opportunities for user misconceptions about what systems can and cannot do. Such misconceptions may lead to over-reliance, unproductive practices, or insufficient quality control in LLM-assisted programming. Here, we aim to characterize misconceptions that users of conversational LLM-based assistants may have in programming contexts. Using a two-phase approach, we first brainstorm and catalog user misconceptions that may occur, and then conduct a qualitative analysis to examine whether these conceptual issues surface in naturalistic Python-programming conversations with an LLM-based chatbot drawn from an openly available dataset. Indeed, we see evidence that some users have misplaced expectations about the availability of LLM-based chatbot features like web access, code execution, or non-text output generation. We also see potential evidence for deeper conceptual issues around the scope of information required to debug, validate, and optimize programs. Our findings reinforce the need for designing LLM-based tools that more clearly communicate their programming capabilities to users.

**AI Summary:** The research examines user misconceptions of conversational programming assistants powered by large language models (LLMs), such as ChatGPT. The study finds that users may have misplaced expectations about the capabilities of these tools, leading to over-reliance, unproductive practices, and insufficient quality control in LLM-assisted programming. The findings highlight the importance of designing LLM-based tools that clearly communicate their programming capabilities to users to mitigate misconceptions and improve user experience.

---

## ggtime: A Grammar of Temporal Graphics
**URL:** https://arxiv.org/abs/2510.25656

**Abstract:** Visualizing changes over time is fundamental to learning from the past and anticipating the future. However, temporal semantics can be complicated, and existing visualization tools often struggle to accurately represent these complexities. It is common to use bespoke plot helper functions designed to produce specific graphics, due to the absence of flexible general tools that respect temporal semantics. We address this problem by proposing a grammar of temporal graphics, and an associated software implementation, 'ggtime', that encodes temporal semantics into a declarative grammar for visualizing temporal data. The grammar introduces new composable elements that support visualization across linear, cyclical, quasi-cyclical, and other granularities; standardization of irregular durations; and alignment of time points across different granularities and time zones. It is designed for interoperability with other semantic variables, allowing navigation across the space of visualizations while preserving temporal semantics.

**AI Summary:** The research introduces a new grammar of temporal graphics called 'ggtime' that aims to accurately represent the complexities of temporal data visualization. The grammar includes composable elements to support visualization across different time granularities, standardization of irregular durations, and alignment of time points across various time zones. This tool allows for flexible and standardized visualization of temporal data, enhancing the understanding and interpretation of changes over time.

---

## Psychoacoustic assessment of synthetic sounds for electric vehicles in a virtual reality experiment
**URL:** https://arxiv.org/abs/2510.25593

**Abstract:** The growing adoption of electric vehicles, known for their quieter operation compared to internal combustion engine vehicles, raises concerns about their detectability, particularly for vulnerable road users. To address this, regulations mandate the inclusion of exterior sound signals for electric vehicles, specifying minimum sound pressure levels at low speeds. These synthetic exterior sounds are often used in noisy urban environments, creating the challenge of enhancing detectability without introducing excessive noise annoyance. This study investigates the design of synthetic exterior sound signals that balance high noticeability with low annoyance. An audiovisual experiment with 14 participants was conducted using 15 virtual reality scenarios featuring a passing car. The scenarios included various sound signals, such as pure, intermittent, and complex tones at different frequencies. Two baseline cases, a diesel engine and only tyre noise, were also tested. Participants rated sounds for annoyance, noticeability, and informativeness using 11-point ICBEN scales. The findings highlight how psychoacoustic sound quality metrics predict annoyance ratings better than conventional sound metrics, providing insight into optimising sound design for electric vehicles. By improving pedestrian safety while minimising noise pollution, this research supports the development of effective and user-friendly exterior sound standards for electric vehicles.

**AI Summary:** This study investigates the design of synthetic exterior sound signals for electric vehicles to enhance detectability without causing excessive noise annoyance. The research conducted a virtual reality experiment with 14 participants, testing various sound signals and comparing them to baseline cases of a diesel engine and tyre noise. The findings suggest that psychoacoustic sound quality metrics can better predict annoyance ratings, providing valuable insight into optimizing sound design for electric vehicles to improve pedestrian safety and minimize noise pollution.

---

## Small Talk, Big Impact? LLM-based Conversational Agents to Mitigate Passive Fatigue in Conditional Automated Driving
**URL:** https://arxiv.org/abs/2510.25421

**Abstract:** Passive fatigue during conditional automated driving can compromise driver readiness and safety. This paper presents findings from a test-track study with 40 participants in a real-world rural automated driving scenario. In this scenario, a Large Language Model (LLM) based conversational agent (CA) was designed to check in with drivers and re-engage them with their surroundings. Drawing on in-car video recordings, sleepiness ratings and interviews, we analysed how drivers interacted with the agent and how these interactions shaped alertness. Users found the CA helpful for supporting vigilance during passive fatigue. Thematic analysis of acceptability further revealed three user preference profiles that implicate future intention to use CAs. Positioning empirically observed profiles within existing CA archetype frameworks highlights the need for adaptive design sensitive to diverse user groups. This work underscores the potential of CAs as proactive Human-Machine Interface (HMI) interventions, demonstrating how natural language can support context-aware interaction during automated driving.

**AI Summary:** This research study investigated the use of a conversational agent based on a Large Language Model to mitigate passive fatigue in drivers during automated driving. The findings showed that the agent helped to support vigilance and alertness in users experiencing fatigue. The study also identified three user preference profiles that could inform future design of conversational agents for automated driving, highlighting the potential of natural language interfaces to enhance driver interaction and safety.

---

## CGM-Led Multimodal Tracking with Chatbot Support: An Autoethnography in Sub-Health
**URL:** https://arxiv.org/abs/2510.25381

**Abstract:** Metabolic disorders present a pressing global health challenge, with China carrying the world's largest burden. While continuous glucose monitoring (CGM) has transformed diabetes care, its potential for supporting sub-health populations -- such as individuals who are overweight, prediabetic, or anxious -- remains underexplored. At the same time, large language models (LLMs) are increasingly used in health coaching, yet CGM is rarely incorporated as a first-class signal. To address this gap, we conducted a six-week autoethnography, combining CGM with multimodal indicators captured via common digital devices and a chatbot that offered personalized reflections and explanations of glucose fluctuations. Our findings show how CGM-led, data-first multimodal tracking, coupled with conversational support, shaped everyday practices of diet, activity, stress, and wellbeing. This work contributes to HCI by extending CGM research beyond clinical diabetes and demonstrating how LLM-driven agents can support preventive health and reflection in at-risk populations.

**AI Summary:** This research explores the potential of continuous glucose monitoring (CGM) in supporting sub-health populations, such as those who are overweight, prediabetic, or anxious. The study combines CGM with multimodal indicators and a chatbot to offer personalized reflections on glucose fluctuations, leading to improvements in diet, activity, stress, and wellbeing. The findings highlight the importance of incorporating CGM as a first-class signal in health coaching and demonstrate how large language models can support preventive health and reflection in at-risk populations.

---

## OrchVis: Hierarchical Multi-Agent Orchestration for Human Oversight
**URL:** https://arxiv.org/abs/2510.24937

**Abstract:** We introduce OrchVis, a multi-agent orchestration framework that visualizes, verifies, and coordinates goal-driven collaboration among LLM-based agents. Through hierarchical goal alignment, task assignment, and conflict resolution, OrchVis enables humans to supervise complex multi-agent workflows without micromanaging each step. The system parses user intent into structured goals, monitors execution via automated verification, and exposes inter-agent dependencies through an interactive planning panel. When conflicts arise, users can explore system-proposed alternatives and selectively replan. OrchVis advances human-centered design for multi-agent systems by combining transparent visualization with adaptive autonomy.

**AI Summary:** The research introduces OrchVis, a framework for orchestrating collaboration among LLM-based agents through hierarchical goal alignment, task assignment, and conflict resolution. OrchVis allows humans to supervise complex multi-agent workflows without micromanaging each step by parsing user intent into structured goals, monitoring execution, and providing interactive planning tools. This system advances human-centered design for multi-agent systems by combining transparent visualization with adaptive autonomy, enabling efficient oversight of collaborative AI processes.

---

## Efficiency Without Cognitive Change: Evidence from Human Interaction with Narrow AI Systems
**URL:** https://arxiv.org/abs/2510.24893

**Abstract:** The growing integration of artificial intelligence (AI) into human cognition raises a fundamental question: does AI merely improve efficiency, or does it alter how we think? This study experimentally tested whether short-term exposure to narrow AI tools enhances core cognitive abilities or simply optimizes task performance. Thirty young adults completed standardized neuropsychological assessments embedded in a seven-week protocol with a four-week online intervention involving problem-solving and verbal comprehension tasks, either with or without AI support (ChatGPT). While AI-assisted participants completed several tasks faster and more accurately, no significant pre-post differences emerged in standardized measures of problem solving or verbal comprehension. These results demonstrate efficiency gains without cognitive change, suggesting that current narrow AI systems serve as cognitive scaffolds extending performance without transforming underlying mental capacities. The findings highlight the need for ethical and educational frameworks that promote critical and autonomous thinking in an increasingly AI-augmented cognitive ecology.

**AI Summary:** This study investigated whether short-term exposure to narrow AI tools improves core cognitive abilities or simply enhances task performance. The results showed that while AI-assisted participants completed tasks faster and more accurately, there were no significant changes in standardized measures of problem solving or verbal comprehension. This suggests that current narrow AI systems act as cognitive scaffolds, improving efficiency without altering underlying mental capacities, emphasizing the importance of promoting critical and autonomous thinking in an AI-augmented cognitive environment.

---

## Human- vs. AI-generated tests: dimensionality and information accuracy in latent trait evaluation
**URL:** https://arxiv.org/abs/2510.24739

**Abstract:** Artificial Intelligence (AI) and large language models (LLMs) are increasingly used in social and psychological research. Among potential applications, LLMs can be used to generate, customise, or adapt measurement instruments. This study presents a preliminary investigation of AI-generated questionnaires by comparing two ChatGPT-based adaptations of the Body Awareness Questionnaire (BAQ) with the validated human-developed version. The AI instruments were designed with different levels of explicitness in content and instructions on construct facets, and their psychometric properties were assessed using a Bayesian Graded Response Model. Results show that although surface wording between AI and original items was similar, differences emerged in dimensionality and in the distribution of item and test information across latent traits. These findings illustrate the importance of applying statistical measures of accuracy to ensure the validity and interpretability of AI-driven tools.

**AI Summary:** This study compares AI-generated adaptations of the Body Awareness Questionnaire with the human-developed version, using a Bayesian Graded Response Model to assess their psychometric properties. The results show differences in dimensionality and information distribution across latent traits between the AI-generated and human-developed questionnaires, highlighting the importance of statistical measures to ensure the validity and interpretability of AI-driven tools in social and psychological research.

---

## Beyond Models: A Framework for Contextual and Cultural Intelligence in African AI Deployment
**URL:** https://arxiv.org/abs/2510.24729

**Abstract:** While global AI development prioritizes model performance and computational scale, meaningful deployment in African markets requires fundamentally different architectural decisions. This paper introduces Contextual and Cultural Intelligence (CCI) -- a systematic framework enabling AI systems to process cultural meaning, not just data patterns, through locally relevant, emotionally intelligent, and economically inclusive design. Using design science methodology, we validate CCI through a production AI-native cross-border shopping platform serving diaspora communities. Key empirical findings: 89% of users prefer WhatsApp-based AI interaction over traditional web interfaces (n=602, chi-square=365.8, p<0.001), achieving 536 WhatsApp users and 3,938 total conversations across 602 unique users in just 6 weeks, and culturally informed prompt engineering demonstrates sophisticated understanding of culturally contextualized queries, with 89% family-focused commerce patterns and natural code-switching acceptance. The CCI framework operationalizes three technical pillars: Infrastructure Intelligence (mobile-first, resilient architectures), Cultural Intelligence (multilingual NLP with social context awareness), and Commercial Intelligence (trust-based conversational commerce). This work contributes both theoretical innovation and reproducible implementation patterns, challenging Silicon Valley design orthodoxies while providing actionable frameworks for equitable AI deployment across resource-constrained markets.

**AI Summary:** This research introduces a framework called Contextual and Cultural Intelligence (CCI) for AI deployment in African markets, focusing on culturally relevant design and emotionally intelligent interactions. The study found that users preferred WhatsApp-based AI interaction over traditional web interfaces, leading to increased user engagement and culturally informed prompt engineering. The CCI framework emphasizes Infrastructure Intelligence, Cultural Intelligence, and Commercial Intelligence to enable equitable AI deployment in resource-constrained markets, challenging traditional design approaches and providing actionable frameworks for future AI implementations.

---

## AmarDoctor: An AI-Driven, Multilingual, Voice-Interactive Digital Health Application for Primary Care Triage and Patient Management to Bridge the Digital Health Divide for Bengali Speakers
**URL:** https://arxiv.org/abs/2510.24724

**Abstract:** This study presents AmarDoctor, a multilingual voice-interactive digital health app designed to provide comprehensive patient triage and AI-driven clinical decision support for Bengali speakers, a population largely underserved in access to digital healthcare. AmarDoctor adopts a data-driven approach to strengthen primary care delivery and enable personalized health management. While platforms such as AdaHealth, WebMD, Symptomate, and K-Health have become popular in recent years, they mainly serve European demographics and languages. AmarDoctor addresses this gap with a dual-interface system for both patients and healthcare providers, supporting three major Bengali dialects. At its core, the patient module uses an adaptive questioning algorithm to assess symptoms and guide users toward the appropriate specialist. To overcome digital literacy barriers, it integrates a voice-interactive AI assistant that navigates users through the app services. Complementing this, the clinician-facing interface incorporates AI-powered decision support that enhances workflow efficiency by generating structured provisional diagnoses and treatment recommendations. These outputs inform key services such as e-prescriptions, video consultations, and medical record management. To validate clinical accuracy, the system was evaluated against a gold-standard set of 185 clinical vignettes developed by experienced physicians. Effectiveness was further assessed by comparing AmarDoctor performance with five independent physicians using the same vignette set. Results showed AmarDoctor achieved a top-1 diagnostic precision of 81.08 percent (versus physicians average of 50.27 percent) and a top specialty recommendation precision of 91.35 percent (versus physicians average of 62.6 percent).

**AI Summary:** The study introduces AmarDoctor, a multilingual voice-interactive digital health app tailored for Bengali speakers to improve primary care triage and patient management. AmarDoctor addresses the digital health gap for this underserved population by providing personalized health management and AI-driven clinical decision support. The app's dual-interface system for patients and healthcare providers, along with its high diagnostic precision and specialty recommendation accuracy, demonstrates its potential to enhance healthcare access and quality for Bengali speakers.

---

## Modelling the Interplay of Eye-Tracking Temporal Dynamics and Personality for Emotion Detection in Face-to-Face Settings
**URL:** https://arxiv.org/abs/2510.24720

**Abstract:** Accurate recognition of human emotions is critical for adaptive human-computer interaction, yet remains challenging in dynamic, conversation-like settings. This work presents a personality-aware multimodal framework that integrates eye-tracking sequences, Big Five personality traits, and contextual stimulus cues to predict both perceived and felt emotions. Seventy-three participants viewed speech-containing clips from the CREMA-D dataset while providing eye-tracking signals, personality assessments, and emotion ratings. Our neural models captured temporal gaze dynamics and fused them with trait and stimulus information, yielding consistent gains over SVM and literature baselines. Results show that (i) stimulus cues strongly enhance perceived-emotion predictions (macro F1 up to 0.77), while (ii) personality traits provide the largest improvements for felt emotion recognition (macro F1 up to 0.58). These findings highlight the benefit of combining physiological, trait-level, and contextual information to address the inherent subjectivity of emotion. By distinguishing between perceived and felt responses, our approach advances multimodal affective computing and points toward more personalized and ecologically valid emotion-aware systems.

**AI Summary:** This research explores the use of eye-tracking data, personality traits, and contextual stimulus cues to predict both perceived and felt emotions in face-to-face interactions. The study found that incorporating stimulus cues improved predictions of perceived emotions, while personality traits were most beneficial for recognizing felt emotions. By combining physiological, trait-level, and contextual information, the study advances the field of multimodal affective computing and suggests the potential for more personalized emotion-aware systems.

---

## Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study
**URL:** https://arxiv.org/abs/2510.25016

**Abstract:** The future of Requirements Engineering (RE) is increasingly driven by artificial intelligence (AI), reshaping how we elicit, analyze, and validate requirements. Traditional RE is based on labor-intensive manual processes prone to errors and complexity. AI-powered approaches, specifically large language models (LLMs), natural language processing (NLP), and generative AI, offer transformative solutions and reduce inefficiencies. However, the use of AI in RE also brings challenges like algorithmic bias, lack of explainability, and ethical concerns related to automation. To address these issues, this study introduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that integrates AI-driven analysis with human oversight to improve requirements elicitation, analysis, and validation. The model emphasizes ethical AI use through transparency, explainability, and bias mitigation. We outline a multi-phase research methodology focused on preparing RE datasets, fine-tuning AI models, and designing collaborative human-AI workflows. This preliminary study presents the conceptual framework and early-stage prototype implementation, establishing a research agenda and practical design direction for applying intelligent data science techniques to semi-structured and unstructured RE data in collaborative environments.

**AI Summary:** This research explores the integration of artificial intelligence (AI) into Requirements Engineering (RE) processes to improve efficiency and accuracy. The study introduces the Human-AI RE Synergy Model (HARE-SM), which combines AI-driven analysis with human oversight to address challenges like algorithmic bias and lack of explainability. The framework aims to enhance requirements elicitation, analysis, and validation by promoting ethical AI use through transparency and bias mitigation, offering a practical design direction for applying intelligent data science techniques in collaborative environments.

---

## The Narrative Continuity Test: A Conceptual Framework for Evaluating Identity Persistence in AI Systems
**URL:** https://arxiv.org/abs/2510.24831

**Abstract:** Artificial intelligence systems based on large language models (LLMs) can now generate coherent text, music, and images, yet they operate without a persistent state: each inference reconstructs context from scratch. This paper introduces the Narrative Continuity Test (NCT) -- a conceptual framework for evaluating identity persistence and diachronic coherence in AI systems. Unlike capability benchmarks that assess task performance, the NCT examines whether an LLM remains the same interlocutor across time and interaction gaps. The framework defines five necessary axes -- Situated Memory, Goal Persistence, Autonomous Self-Correction, Stylistic & Semantic Stability, and Persona/Role Continuity -- and explains why current architectures systematically fail to support them. Case analyses (this http URL, Grok, Replit, Air Canada) show predictable continuity failures under stateless inference. The NCT reframes AI evaluation from performance to persistence, outlining conceptual requirements for future benchmarks and architectural designs that could sustain long-term identity and goal coherence in generative models.

**AI Summary:** This research introduces the Narrative Continuity Test (NCT) as a framework for evaluating identity persistence in AI systems based on large language models. The study highlights the lack of persistent state in current AI architectures and identifies five necessary axes for evaluating identity coherence. Case analyses demonstrate continuity failures in current models, emphasizing the need for future benchmarks and designs to support long-term identity and goal coherence in AI systems.

---

## The Epistemic Suite: A Post-Foundational Diagnostic Methodology for Assessing AI Knowledge Claims
**URL:** https://arxiv.org/abs/2510.24721

**Abstract:** Large Language Models (LLMs) generate fluent, plausible text that can mislead users into mistaking simulated coherence for genuine understanding. This paper introduces the Epistemic Suite, a post-foundational diagnostic methodology for surfacing the epistemic conditions under which AI outputs are produced and received. Rather than determining truth or falsity, the Suite operates through twenty diagnostic lenses, applied by practitioners as context warrants, to reveal patterns such as confidence laundering, narrative compression, displaced authority, and temporal drift. It is grounded in three design principles: diagnosing production before evaluating claims, preferring diagnostic traction over foundational settlement, and embedding reflexivity as a structural requirement rather than an ethical ornament. When enacted, the Suite shifts language models into a diagnostic stance, producing inspectable artifacts-flags, annotations, contradiction maps, and suspension logs (the FACS bundle)-that create an intermediary layer between AI output and human judgment. A key innovation is epistemic suspension, a practitioner-enacted circuit breaker that halts continuation when warrant is exceeded, with resumption based on judgment rather than rule. The methodology also includes an Epistemic Triage Protocol and a Meta-Governance Layer to manage proportionality and link activation to relational accountability, consent, historical context, and pluralism safeguards. Unlike internalist approaches that embed alignment into model architectures (e.g., RLHF or epistemic-integrity proposals), the Suite operates externally as scaffolding, preserving expendability and refusal as safeguards rather than failures. It preserves the distinction between performance and understanding, enabling accountable deliberation while maintaining epistemic modesty.

**AI Summary:** This paper introduces the Epistemic Suite, a diagnostic methodology for assessing AI knowledge claims that focuses on surfacing the conditions under which AI outputs are produced and received. The Suite operates through twenty diagnostic lenses to reveal patterns such as confidence laundering and narrative compression, creating an intermediary layer between AI output and human judgment. This methodology shifts language models into a diagnostic stance, allowing for inspectable artifacts and epistemic suspension to halt continuation when warrant is exceeded, promoting accountable deliberation and maintaining epistemic modesty.

---

